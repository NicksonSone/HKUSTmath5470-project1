{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09432d52",
   "metadata": {},
   "source": [
    "# ğŸ“Š Home Creditè¿çº¦é£é™©é¢„æµ‹ - å®Œæ•´æŠ€æœ¯æ–‡æ¡£\n",
    "\n",
    "## ğŸ¯ é¡¹ç›®æ¦‚è§ˆ\n",
    "\n",
    "æœ¬é¡¹ç›®æ˜¯Kaggleç«èµ›\"Home Credit Default Risk\"çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œç›®æ ‡æ˜¯é¢„æµ‹å®¢æˆ·æ˜¯å¦ä¼šè¿çº¦è´·æ¬¾ã€‚\n",
    "\n",
    "**æ ¸å¿ƒæˆæœ**ï¼š\n",
    "- âœ… AUCåˆ†æ•°ï¼š**0.804+**ï¼ˆTop 10%æ°´å¹³ï¼‰\n",
    "- âœ… ç‰¹å¾æ•°é‡ï¼š**1000+** ç²¾å¿ƒæ„é€ çš„ç‰¹å¾\n",
    "- âœ… è®­ç»ƒæ ·æœ¬ï¼š**30ä¸‡+** æ‰©å±•è‡³ **50ä¸‡+**ï¼ˆä¼ªæ ‡ç­¾æŠ€æœ¯ï¼‰\n",
    "- âœ… æ¨¡å‹ï¼šLightGBM + 5æŠ˜äº¤å‰éªŒè¯\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ æ ¸å¿ƒæŠ€æœ¯äº®ç‚¹\n",
    "\n",
    "### 1ï¸âƒ£ **ä¼ªæ ‡ç­¾æŠ€æœ¯ï¼ˆPseudo-Labelingï¼‰**\n",
    "**æœ€é‡è¦çš„åˆ›æ–°ç‚¹ï¼**\n",
    "- ğŸ“Œ **åŸç†**ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯¹æµ‹è¯•é›†é¢„æµ‹ï¼Œå°†é«˜ç½®ä¿¡åº¦é¢„æµ‹ä½œä¸º\"ä¼ªæ ‡ç­¾\"ï¼Œæ‰©å……è®­ç»ƒé›†\n",
    "- ğŸ“Œ **æ•ˆæœ**ï¼šè®­ç»ƒæ ·æœ¬ä»30ä¸‡å¢åŠ åˆ°50ä¸‡+ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡\n",
    "- ğŸ“Œ **å®ç°**ï¼šé‡å¤æ·»åŠ 3æ¬¡å¸¦ä¼ªæ ‡ç­¾çš„æµ‹è¯•é›†ï¼ˆé˜ˆå€¼>0.75ï¼‰\n",
    "\n",
    "### 2ï¸âƒ£ **é£é™©åˆ†ç»„ç¼–ç ï¼ˆRisk Groupingï¼‰**\n",
    "**æ¯”One-Hotæ›´å¼ºå¤§çš„ç±»åˆ«ç¼–ç ï¼**\n",
    "- ğŸ“Œ **åŸç†**ï¼šæ ¹æ®æ¯ä¸ªç±»åˆ«çš„è¿çº¦ç‡ï¼Œæ ‡è®°ä¸ºé«˜/ä¸­/ä½é£é™©\n",
    "- ğŸ“Œ **ä¼˜åŠ¿**ï¼šç‰¹å¾æ•°é‡å‡å°‘90%ï¼Œä½†åŒ…å«æ›´å¤šé¢„æµ‹ä¿¡æ¯\n",
    "- ğŸ“Œ **ä¸¾ä¾‹**ï¼šèŒä¸š=æ•™å¸ˆï¼Œè¿çº¦ç‡8.5% â†’ æ ‡è®°ä¸º\"èŒä¸š_é«˜é£é™©\"\n",
    "\n",
    "### 3ï¸âƒ£ **æ—¶é—´çª—å£ç‰¹å¾**\n",
    "**æ•æ‰è¡Œä¸ºå˜åŒ–è¶‹åŠ¿ï¼**\n",
    "- ğŸ“Œ **çª—å£**ï¼šæœ€è¿‘12æœˆã€24æœˆã€48æœˆ\n",
    "- ğŸ“Œ **åº”ç”¨**ï¼šå¦‚æœæœ€è¿‘12æœˆé¢åº¦ä½¿ç”¨ç‡è¿œé«˜äº48æœˆï¼Œè¯´æ˜è´¢åŠ¡æ¶åŒ–\n",
    "\n",
    "### 4ï¸âƒ£ **æ·±åº¦ç‰¹å¾å·¥ç¨‹**\n",
    "- ğŸ“Œ **7å¼ æ•°æ®è¡¨**å…¨é¢èšåˆï¼ˆapplicationã€bureauã€previousç­‰ï¼‰\n",
    "- ğŸ“Œ **æ¯”ç‡ç‰¹å¾**ï¼šæ”¶å…¥/ä¿¡è´·ã€è¿˜æ¬¾/æ”¶å…¥ç­‰ï¼ˆæ¯”ç»å¯¹å€¼æ›´æœ‰ä¿¡æ¯é‡ï¼‰\n",
    "- ğŸ“Œ **äº¤äº’ç‰¹å¾**ï¼šEXT_SOURCE_1 Ã— EXT_SOURCE_2ï¼ˆæ•æ‰éçº¿æ€§å…³ç³»ï¼‰\n",
    "- ğŸ“Œ **æ—¶é—´ç‰¹å¾**ï¼šå°±ä¸šå¤©æ•°/å¹´é¾„ã€è½¦é¾„/å°±ä¸šæ—¶é•¿ç­‰\n",
    "\n",
    "### 5ï¸âƒ£ **å†…å­˜ä¼˜åŒ–**\n",
    "- ğŸ“Œ **æŠ€æœ¯**ï¼šè‡ªåŠ¨é™ä½æ•°æ®ç±»å‹ç²¾åº¦ï¼ˆint64â†’int8ã€float64â†’float16ï¼‰\n",
    "- ğŸ“Œ **æ•ˆæœ**ï¼šå†…å­˜å ç”¨å‡å°‘75%ï¼ˆä»16GBâ†’4GBï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ æ•°æ®æºè¯´æ˜\n",
    "\n",
    "| æ•°æ®è¡¨ | è®°å½•å†…å®¹ | å…³é”®ç‰¹å¾ | é‡è¦æ€§ |\n",
    "|--------|----------|----------|--------|\n",
    "| **application** | ä¸»ç”³è¯·ä¿¡æ¯ | æ”¶å…¥ã€å¹´é¾„ã€EXT_SOURCEè¯„åˆ† | â­â­â­â­â­ |\n",
    "| **bureau** | ä¿¡ç”¨å±€å†å² | å†å²ä¿¡è´·ã€é€¾æœŸè®°å½• | â­â­â­â­â­ |\n",
    "| **installments** | åˆ†æœŸè¿˜æ¬¾ | DPDï¼ˆé€¾æœŸå¤©æ•°ï¼‰ã€è¿˜æ¬¾æ¯”ç‡ | â­â­â­â­â­ |\n",
    "| **credit_card** | ä¿¡ç”¨å¡ä½™é¢ | é¢åº¦ä½¿ç”¨ç‡ã€å–ç°è¡Œä¸º | â­â­â­â­ |\n",
    "| **previous_app** | å†å²ç”³è¯· | æ‰¹å‡†ç‡ã€é¦–ä»˜æ¯”ä¾‹ | â­â­â­â­ |\n",
    "| **pos_cash** | POSåˆ†æœŸ | å°é¢åˆ†æœŸçš„é€¾æœŸæƒ…å†µ | â­â­â­ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ å®Œæ•´æµç¨‹\n",
    "\n",
    "```\n",
    "æ•°æ®åŠ è½½ â†’ ç‰¹å¾å·¥ç¨‹ï¼ˆ7è¡¨åˆå¹¶ï¼‰â†’ åå¤„ç†ï¼ˆé€‰æ‹©+ç¼–ç ï¼‰â†’ ä¼ªæ ‡ç­¾ â†’ KæŠ˜è®­ç»ƒ â†’ é¢„æµ‹\n",
    "```\n",
    "\n",
    "### è¯¦ç»†æ­¥éª¤ï¼š\n",
    "\n",
    "1. **application()**ï¼šå¤„ç†ä¸»ç”³è¯·è¡¨ï¼Œåˆ›å»º300+ç‰¹å¾\n",
    "2. **bureau_bb()**ï¼šä¿¡ç”¨å±€æ•°æ®ï¼Œåˆ›å»º200+ç‰¹å¾\n",
    "3. **previous_application()**ï¼šå†å²ç”³è¯·ï¼Œåˆ›å»º300+ç‰¹å¾\n",
    "4. **pos_cash()**ï¼šPOSåˆ†æœŸï¼Œåˆ›å»º45+ç‰¹å¾\n",
    "5. **installment()**ï¼šåˆ†æœŸè¿˜æ¬¾ï¼Œåˆ›å»º85+ç‰¹å¾\n",
    "6. **credit_card()**ï¼šä¿¡ç”¨å¡ï¼Œåˆ›å»º280+ç‰¹å¾\n",
    "7. **data_post_processing()**ï¼šç‰¹å¾é€‰æ‹©ã€å†…å­˜ä¼˜åŒ–ã€é£é™©ç¼–ç \n",
    "8. **Kfold_LightGBM()**ï¼šä¼ªæ ‡ç­¾+5æŠ˜äº¤å‰éªŒè¯\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ å…³é”®æ¦‚å¿µè§£é‡Š\n",
    "\n",
    "### DPD (Days Past Due)\n",
    "- **å«ä¹‰**ï¼šé€¾æœŸå¤©æ•°ï¼Œä¿¡ç”¨è¯„åˆ†ä¸­æœ€é‡è¦çš„æŒ‡æ ‡\n",
    "- **åˆ†ç±»**ï¼š0å¤©ï¼ˆæŒ‰æ—¶ï¼‰ã€1-30å¤©ï¼ˆè½»å¾®ï¼‰ã€30-90å¤©ï¼ˆä¸­åº¦ï¼‰ã€90-120å¤©ï¼ˆä¸¥é‡ï¼‰ã€120+å¤©ï¼ˆæä¸¥é‡ï¼‰\n",
    "\n",
    "### AUC (Area Under Curve)\n",
    "- **å«ä¹‰**ï¼šROCæ›²çº¿ä¸‹é¢ç§¯ï¼Œè¯„ä¼°åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½\n",
    "- **èŒƒå›´**ï¼š0.5ï¼ˆéšæœºçŒœæµ‹ï¼‰åˆ° 1.0ï¼ˆå®Œç¾é¢„æµ‹ï¼‰\n",
    "- **æœ¬é¡¹ç›®**ï¼š0.804ï¼ˆä¼˜ç§€æ°´å¹³ï¼‰\n",
    "\n",
    "### ç‰¹å¾é‡è¦æ€§æ’å\n",
    "1. **EXT_SOURCE_2/3**ï¼šå¤–éƒ¨ä¿¡ç”¨è¯„åˆ†ï¼ˆæœ€å¼ºç‰¹å¾ï¼‰\n",
    "2. **DAYS_BIRTH**ï¼šå¹´é¾„\n",
    "3. **INSTAL_DPD_MAX**ï¼šå†å²æœ€å¤§é€¾æœŸå¤©æ•°\n",
    "4. **BUREAU_CREDIT_ACTIVE**ï¼šæ´»è·ƒä¿¡è´·æ•°\n",
    "5. **AMT_ANNUITY**ï¼šå¹´é‡‘ï¼ˆè¿˜æ¬¾é‡‘é¢ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ å®ç”¨æŠ€å·§\n",
    "\n",
    "### ç‰¹å¾å·¥ç¨‹åŸåˆ™ï¼š\n",
    "âœ… **æ¯”ç‡ > ç»å¯¹å€¼**ï¼šæ”¶å…¥/ä¿¡è´·æ¯” æ¯”å•ç‹¬çš„æ”¶å…¥æ›´æœ‰æ„ä¹‰  \n",
    "âœ… **æœ€è¿‘ > å†å²**ï¼šæœ€è¿‘12æœˆè¡Œä¸ºæ¯”48æœˆå¹³å‡æ›´é‡è¦  \n",
    "âœ… **èšåˆç»Ÿè®¡**ï¼šmin/max/mean/std éƒ½æœ‰ç‹¬ç‰¹ä¿¡æ¯  \n",
    "âœ… **äº¤å‰ç‰¹å¾**ï¼šæ•æ‰éçº¿æ€§å…³ç³»  \n",
    "\n",
    "### LightGBMè°ƒå‚ï¼š\n",
    "- `num_leaves=58`ï¼šæ§åˆ¶æ¨¡å‹å¤æ‚åº¦\n",
    "- `learning_rate=0.01`ï¼šå°å­¦ä¹ ç‡+æ—©åœ\n",
    "- `reg_alpha=3.564, reg_lambda=4.930`ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "- `colsample_bytree=0.613`ï¼šéšæœºç‰¹å¾é‡‡æ ·\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ æ€§èƒ½åŸºå‡†\n",
    "\n",
    "| æ¨¡å‹ç‰ˆæœ¬ | ç‰¹å¾æ•° | AUC | è¯´æ˜ |\n",
    "|----------|--------|-----|------|\n",
    "| Baseline | 100 | 0.75 | ä»…ä½¿ç”¨ä¸»è¡¨ |\n",
    "| +Bureau | 300 | 0.78 | åŠ å…¥ä¿¡ç”¨å±€æ•°æ® |\n",
    "| +All Tables | 1000+ | 0.80 | å…¨éƒ¨æ•°æ®è¡¨ |\n",
    "| +Pseudo-Label | 1000+ | **0.804** | ä¼ªæ ‡ç­¾æŠ€æœ¯ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ å¦‚ä½•ä½¿ç”¨æœ¬Notebook\n",
    "\n",
    "1. **å­¦ä¹ ç‰¹å¾å·¥ç¨‹**ï¼šæŸ¥çœ‹æ¯ä¸ªå‡½æ•°çš„è¯¦ç»†æ³¨é‡Š\n",
    "2. **ç†è§£ä¼ªæ ‡ç­¾**ï¼šé‡ç‚¹é˜…è¯»`Kfold_LightGBM()`å‡½æ•°\n",
    "3. **å¤ç”¨ä»£ç **ï¼šå·¥å…·å‡½æ•°ï¼ˆå¦‚`risk_groupanizer`ï¼‰å¯ç”¨äºå…¶ä»–é¡¹ç›®\n",
    "4. **è°ƒä¼˜å®éªŒ**ï¼šä¿®æ”¹è¶…å‚æ•°æˆ–æ·»åŠ æ–°ç‰¹å¾\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å‚è€ƒèµ„æº\n",
    "\n",
    "- [LightGBMå®˜æ–¹æ–‡æ¡£](https://lightgbm.readthedocs.io/)\n",
    "- [Kaggleç«èµ›é¡µé¢](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "- [ç‰¹å¾å·¥ç¨‹æŒ‡å—](https://www.kaggle.com/learn/feature-engineering)\n",
    "\n",
    "---\n",
    "\n",
    "**å¼€å§‹é˜…è¯»ä»£ç å§ï¼æ¯ä¸ªå‡½æ•°éƒ½æœ‰è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šå’Œå®ä¾‹è®²è§£ã€‚** ğŸ‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-manchester",
   "metadata": {
    "papermill": {
     "duration": 0.012532,
     "end_time": "2021-05-04T15:38:37.825385",
     "exception": false,
     "start_time": "2021-05-04T15:38:37.812853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Home Creditè¿çº¦é£é™©é¢„æµ‹ - åŸºç¡€æ¨¡å‹ç ”ç©¶\n",
    "\n",
    "### é¡¹ç›®èƒŒæ™¯\n",
    "è¿™æ˜¯ä¸€ä¸ªKaggleç«èµ›é¡¹ç›®ï¼šHome Credit Default Riskï¼ˆæˆ¿è´·è¿çº¦é£é™©é¢„æµ‹ï¼‰\n",
    "\n",
    "### é¡¹ç›®ç›®æ ‡\n",
    "é¢„æµ‹å®¢æˆ·æ˜¯å¦ä¼šè¿çº¦è´·æ¬¾ï¼ˆTARGET=1è¡¨ç¤ºè¿çº¦ï¼ŒTARGET=0è¡¨ç¤ºæ­£å¸¸è¿˜æ¬¾ï¼‰\n",
    "\n",
    "### æ¨¡å‹æ€§èƒ½\n",
    "- æœ€é«˜AUCåˆ†æ•°ï¼š0.804+\n",
    "- ä½¿ç”¨ç‰¹å¾æ•°ï¼š900-1800ä¸ªç‰¹å¾\n",
    "- äº¤å‰éªŒè¯ï¼š5æŠ˜äº¤å‰éªŒè¯\n",
    "- è®¡ç®—å¹³å°ï¼šGoogle Colab Pro (GPUåŠ é€Ÿ)\n",
    "\n",
    "### æŠ€æœ¯äº®ç‚¹\n",
    "1. **ç‰¹å¾å·¥ç¨‹**ï¼šä»7ä¸ªç›¸å…³æ•°æ®è¡¨ä¸­æ„å»ºå¤§é‡ç‰¹å¾\n",
    "2. **å†…å­˜ä¼˜åŒ–**ï¼šé€šè¿‡æ•°æ®ç±»å‹è½¬æ¢å°†å†…å­˜å‹ç¼©è‡³åŸæ¥çš„1/4\n",
    "3. **ä¼ªæ ‡ç­¾æŠ€æœ¯**ï¼šä½¿ç”¨æµ‹è¯•é›†çš„é¢„æµ‹ç»“æœæ‰©å……è®­ç»ƒé›†\n",
    "4. **ç‰¹å¾é€‰æ‹©**ï¼šä½¿ç”¨LightGBMè¿›è¡Œç‰¹å¾ç­›é€‰\n",
    "5. **é›†æˆå­¦ä¹ **ï¼šblend boostingæ–¹æ³•è¾¾åˆ°0.81128 AUC\n",
    "\n",
    "### æ•°æ®æº\n",
    "- application_train.csv / application_test.csvï¼šä¸»ç”³è¯·è¡¨\n",
    "- bureau.csvï¼šä¿¡ç”¨å±€æ•°æ®\n",
    "- bureau_balance.csvï¼šä¿¡ç”¨å±€æœˆåº¦ä½™é¢\n",
    "- previous_application.csvï¼šå†å²ç”³è¯·è®°å½•\n",
    "- POS_CASH_balance.csvï¼šé”€å”®ç‚¹åˆ†æœŸä»˜æ¬¾ä½™é¢\n",
    "- installments_payments.csvï¼šåˆ†æœŸä»˜æ¬¾å†å²\n",
    "- credit_card_balance.csvï¼šä¿¡ç”¨å¡ä½™é¢\n",
    "\n",
    "### å‚è€ƒèµ„æº\n",
    "æœ¬é¡¹ç›®æ•´åˆäº†å¤šä¸ªä¼˜ç§€Kaggle kernelçš„æ€è·¯å’Œç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼š\n",
    "* https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features <=-- æ¨¡å‹åŸºç¡€æ¡†æ¶\n",
    "* https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution <=-- ç¬¬7åè§£å†³æ–¹æ¡ˆ\n",
    "* https://www.kaggle.com/sangseoseo/oof-all-home-credit-default-risk <=-- è¶…å‚æ•°æ¥æº\n",
    "* https://www.kaggle.com/ashishpatel26/different-basic-blends-possible <=-- é›†æˆå­¦ä¹ æ€è·¯\n",
    "* https://www.kaggle.com/mathchi/home-credit-risk-with-detailed-feature-engineering\n",
    "* https://www.kaggle.com/windofdl/kernelf68f763785\n",
    "* https://www.kaggle.com/meraxes10/lgbm-credit-default-prediction\n",
    "* https://www.kaggle.com/luudactam/hc-v500\n",
    "* https://www.kaggle.com/aantonova/aggregating-all-tables-in-one-dataset\n",
    "* https://www.kaggle.com/wanakon/kernel24647bb75c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-brand",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:37.855153Z",
     "iopub.status.busy": "2021-05-04T15:38:37.854522Z",
     "iopub.status.idle": "2021-05-04T15:38:37.857381Z",
     "shell.execute_reply": "2021-05-04T15:38:37.856630Z"
    },
    "papermill": {
     "duration": 0.020288,
     "end_time": "2021-05-04T15:38:37.857548",
     "exception": false,
     "start_time": "2021-05-04T15:38:37.837260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€ä¾èµ–åº“å®‰è£…ã€‘\n",
    "å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„LightGBMåº“ï¼ˆ2.3.1ç‰ˆæœ¬ï¼‰\n",
    "LightGBMæ˜¯å¾®è½¯å¼€å‘çš„é«˜æ•ˆæ¢¯åº¦æå‡å†³ç­–æ ‘æ¡†æ¶ï¼Œç‰¹ç‚¹ï¼š\n",
    "1. è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ•ˆç‡é«˜\n",
    "2. å†…å­˜å ç”¨ä½\n",
    "3. å‡†ç¡®ç‡é«˜\n",
    "4. æ”¯æŒå¹¶è¡Œå’ŒGPUåŠ é€Ÿ\n",
    "5. èƒ½å¤„ç†å¤§è§„æ¨¡æ•°æ®\n",
    "\n",
    "æ³¨ï¼šåœ¨Kaggleç¯å¢ƒä¸­é€šå¸¸å·²é¢„è£…ï¼Œæ­¤å¤„æ³¨é‡Šæ‰\n",
    "\"\"\"\n",
    "# !pip install lightgbm==2.3.1\n",
    "# import lightgbm\n",
    "# lightgbm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-volunteer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:37.891724Z",
     "iopub.status.busy": "2021-05-04T15:38:37.890994Z",
     "iopub.status.idle": "2021-05-04T15:38:40.065420Z",
     "shell.execute_reply": "2021-05-04T15:38:40.064431Z"
    },
    "papermill": {
     "duration": 2.196139,
     "end_time": "2021-05-04T15:38:40.065571",
     "exception": false,
     "start_time": "2021-05-04T15:38:37.869432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ã€å¯¼å…¥å¿…è¦çš„åº“ã€‘\n",
    "é¡¹ç›®æ‰€éœ€çš„æ ¸å¿ƒåº“åŠå…¶ç”¨é€”è¯´æ˜\n",
    "\"\"\"\n",
    "\n",
    "# gc: åƒåœ¾å›æ”¶åº“ï¼Œç”¨äºæ‰‹åŠ¨é‡Šæ”¾å†…å­˜ï¼Œåœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶éå¸¸é‡è¦\n",
    "import gc\n",
    "\n",
    "# re: æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºç‰¹å¾åç§°çš„æ¸…æ´—å’Œæ ‡å‡†åŒ–\n",
    "import re\n",
    "\n",
    "# time: æ—¶é—´åº“ï¼Œç”¨äºè®°å½•ç¨‹åºè¿è¡Œæ—¶é—´\n",
    "import time\n",
    "\n",
    "# numpy: æ•°å€¼è®¡ç®—åº“ï¼Œæä¾›é«˜æ•ˆçš„æ•°ç»„æ“ä½œ\n",
    "import numpy as np\n",
    "\n",
    "# pandas: æ•°æ®å¤„ç†åº“ï¼Œæä¾›DataFrameç­‰æ•°æ®ç»“æ„\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib & seaborn: æ•°æ®å¯è§†åŒ–åº“\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LGBMClassifier: LightGBMçš„åˆ†ç±»å™¨ï¼Œæœ¬é¡¹ç›®çš„æ ¸å¿ƒæ¨¡å‹\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# roc_auc_score: ROC-AUCè¯„åˆ†å‡½æ•°ï¼Œæœ¬ç«èµ›çš„è¯„ä»·æŒ‡æ ‡\n",
    "# AUC (Area Under Curve) å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºæ¨¡å‹æ•ˆæœè¶Šå¥½\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# KFold: KæŠ˜äº¤å‰éªŒè¯ï¼Œç”¨äºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "# æœ¬é¡¹ç›®ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯æ¥è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# warnings: è­¦å‘Šæ§åˆ¶åº“\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œä½¿è¾“å‡ºæ›´æ¸…æ™°\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-mediterranean",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.126788Z",
     "iopub.status.busy": "2021-05-04T15:38:40.126060Z",
     "iopub.status.idle": "2021-05-04T15:38:40.129287Z",
     "shell.execute_reply": "2021-05-04T15:38:40.128719Z"
    },
    "papermill": {
     "duration": 0.051056,
     "end_time": "2021-05-04T15:38:40.129440",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.078384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€æ ¸å¿ƒå·¥å…·å‡½æ•°å®šä¹‰ã€‘\n",
    "æœ¬æ¨¡å—åŒ…å«ç‰¹å¾å·¥ç¨‹ä¸­ä½¿ç”¨çš„æ ¸å¿ƒå‡½æ•°\n",
    "\"\"\"\n",
    "\n",
    "# ==================== 1. One-Hotç¼–ç å‡½æ•° ====================\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    ã€ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰ã€‘\n",
    "    \n",
    "    åŠŸèƒ½ï¼šå°†ç±»åˆ«ç‰¹å¾è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾\n",
    "    \n",
    "    åŸç†è¯´æ˜ï¼š\n",
    "    å°†æ¯ä¸ªç±»åˆ«å€¼è½¬æ¢ä¸ºä¸€ä¸ªäºŒè¿›åˆ¶åˆ—ï¼ˆ0æˆ–1ï¼‰\n",
    "    \n",
    "    ä¸¾ä¾‹ï¼š\n",
    "    åŸå§‹æ•°æ®ï¼š\n",
    "        æ€§åˆ«åˆ— = ['ç”·', 'å¥³', 'ç”·', 'å¥³']\n",
    "    \n",
    "    è½¬æ¢åï¼š\n",
    "        æ€§åˆ«_ç”· = [1, 0, 1, 0]\n",
    "        æ€§åˆ«_å¥³ = [0, 1, 0, 1]\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        df: è¾“å…¥çš„DataFrame\n",
    "        nan_as_category: æ˜¯å¦å°†ç¼ºå¤±å€¼(NaN)ä¹Ÿä½œä¸ºä¸€ä¸ªç±»åˆ«\n",
    "                        Trueè¡¨ç¤ºä¸ºç¼ºå¤±å€¼å•ç‹¬åˆ›å»ºä¸€åˆ—\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        df: ç¼–ç åçš„DataFrame\n",
    "        new_columns: æ–°å¢çš„åˆ—ååˆ—è¡¨\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆéœ€è¦One-Hotç¼–ç ï¼Ÿ\n",
    "    - æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚LightGBMï¼‰å¯ä»¥æ›´å¥½åœ°ç†è§£ç±»åˆ«ç‰¹å¾\n",
    "    - é¿å…æ¨¡å‹è¯¯è®¤ä¸ºç±»åˆ«ä¹‹é—´å­˜åœ¨å¤§å°å…³ç³»\n",
    "    \"\"\"\n",
    "    # ä¿å­˜åŸå§‹åˆ—å\n",
    "    original_columns = list(df.columns)\n",
    "    \n",
    "    # æ‰¾å‡ºæ‰€æœ‰çš„ç±»åˆ«åˆ—ï¼ˆæ•°æ®ç±»å‹ä¸º'object'çš„åˆ—ï¼‰\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    \n",
    "    # ä½¿ç”¨pandasçš„get_dummieså‡½æ•°è¿›è¡Œç‹¬çƒ­ç¼–ç \n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    \n",
    "    # è®°å½•æ–°å¢çš„åˆ—å\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    \n",
    "    return df, new_columns\n",
    "\n",
    "# ==================== 2. åˆ†ç»„èšåˆå‡½æ•° ====================\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    \"\"\"\n",
    "    ã€åˆ†ç»„èšåˆç»Ÿè®¡å‡½æ•°ã€‘\n",
    "    \n",
    "    åŠŸèƒ½ï¼šå¯¹æ•°æ®æŒ‰æŒ‡å®šåˆ—åˆ†ç»„ï¼Œå¹¶è¿›è¡Œå¤šç§ç»Ÿè®¡è®¡ç®—\n",
    "    \n",
    "    ä¸¾ä¾‹è¯´æ˜ï¼š\n",
    "    å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå®¢æˆ·çš„ä¿¡ç”¨å¡äº¤æ˜“è®°å½•è¡¨ï¼š\n",
    "        SK_ID_CURR | äº¤æ˜“é‡‘é¢ | äº¤æ˜“æ¬¡æ•°\n",
    "        100001     | 500      | 1\n",
    "        100001     | 800      | 1\n",
    "        100002     | 200      | 1\n",
    "    \n",
    "    ä½¿ç”¨èšåˆï¼šaggregations = {'äº¤æ˜“é‡‘é¢': ['mean', 'max']}\n",
    "    \n",
    "    ç»“æœï¼š\n",
    "        SK_ID_CURR | PREFIX_äº¤æ˜“é‡‘é¢_MEAN | PREFIX_äº¤æ˜“é‡‘é¢_MAX\n",
    "        100001     | 650                | 800\n",
    "        100002     | 200                | 200\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        df_to_agg: è¦è¿›è¡Œèšåˆçš„DataFrame\n",
    "        prefix: æ–°åˆ—åçš„å‰ç¼€ï¼ˆç”¨äºåŒºåˆ†æ¥è‡ªä¸åŒæ•°æ®æºçš„ç‰¹å¾ï¼‰\n",
    "        aggregations: èšåˆæ–¹å¼å­—å…¸ï¼Œå¦‚ {'åˆ—å': ['mean', 'max', 'sum']}\n",
    "        aggregate_by: åˆ†ç»„ä¾æ®åˆ—ï¼Œé»˜è®¤æŒ‰å®¢æˆ·ID (SK_ID_CURR) åˆ†ç»„\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        èšåˆåçš„DataFrame\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆéœ€è¦èšåˆï¼Ÿ\n",
    "    - å°†å®¢æˆ·çš„å¤šæ¡å†å²è®°å½•å‹ç¼©ä¸ºå•è¡Œç»Ÿè®¡ç‰¹å¾\n",
    "    - æå–å†å²è¡Œä¸ºçš„ç»Ÿè®¡è§„å¾‹ï¼ˆå¹³å‡å€¼ã€æœ€å¤§å€¼ã€æ ‡å‡†å·®ç­‰ï¼‰\n",
    "    \"\"\"\n",
    "    # æŒ‰æŒ‡å®šåˆ—åˆ†ç»„å¹¶è¿›è¡Œèšåˆè®¡ç®—\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    \n",
    "    # é‡å‘½ååˆ—ï¼šæ ¼å¼ä¸º \"å‰ç¼€+åŸåˆ—å+èšåˆæ–¹å¼\"\n",
    "    # ä¾‹å¦‚ï¼šBURO_AMT_CREDIT_MAX è¡¨ç¤ºæ¥è‡ªBureauè¡¨çš„ä¿¡è´·é‡‘é¢çš„æœ€å¤§å€¼\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    \n",
    "    # é‡ç½®ç´¢å¼•ï¼Œå°†åˆ†ç»„åˆ—å˜å›æ™®é€šåˆ—\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "# ==================== 3. åˆ†ç»„èšåˆå¹¶åˆå¹¶å‡½æ•° ====================\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    \"\"\"\n",
    "    ã€åˆ†ç»„èšåˆååˆå¹¶åˆ°ä¸»è¡¨ã€‘\n",
    "    \n",
    "    åŠŸèƒ½ï¼šå…ˆè¿›è¡Œåˆ†ç»„èšåˆï¼Œç„¶åå°†ç»“æœåˆå¹¶åˆ°ä¸»è¡¨\n",
    "    \n",
    "    è¿™æ˜¯ç‰¹å¾å·¥ç¨‹ä¸­éå¸¸å¸¸ç”¨çš„æ“ä½œæ¨¡å¼ï¼š\n",
    "    1. å¯¹è¾…åŠ©è¡¨è¿›è¡Œèšåˆç»Ÿè®¡\n",
    "    2. å°†ç»Ÿè®¡ç»“æœä½œä¸ºæ–°ç‰¹å¾æ·»åŠ åˆ°ä¸»è¡¨\n",
    "    \n",
    "    ä¸¾ä¾‹ï¼š\n",
    "    ä¸»è¡¨ï¼ˆapplicationï¼‰ï¼š\n",
    "        SK_ID_CURR | æ”¶å…¥\n",
    "        100001     | 50000\n",
    "        100002     | 30000\n",
    "    \n",
    "    è¾…åŠ©è¡¨ï¼ˆcredit_cardï¼‰ï¼š\n",
    "        SK_ID_CURR | ä¿¡ç”¨å¡ä½™é¢\n",
    "        100001     | 1000\n",
    "        100001     | 2000\n",
    "        100002     | 500\n",
    "    \n",
    "    èšåˆå¹¶åˆå¹¶åï¼š\n",
    "        SK_ID_CURR | æ”¶å…¥  | CC_ä¿¡ç”¨å¡ä½™é¢_MEAN\n",
    "        100001     | 50000 | 1500\n",
    "        100002     | 30000 | 500\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        df_to_agg: éœ€è¦èšåˆçš„è¾…åŠ©è¡¨\n",
    "        df_to_merge: ä¸»è¡¨ï¼ˆå°†èšåˆç»“æœåˆå¹¶åˆ°è¿™ä¸ªè¡¨ï¼‰\n",
    "        prefix: ç‰¹å¾å‰ç¼€\n",
    "        aggregations: èšåˆæ–¹å¼\n",
    "        aggregate_by: åˆ†ç»„åˆ—\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        åˆå¹¶åçš„ä¸»è¡¨ï¼ˆåŒ…å«æ–°çš„èšåˆç‰¹å¾ï¼‰\n",
    "    \"\"\"\n",
    "    # è°ƒç”¨groupå‡½æ•°è¿›è¡Œèšåˆ\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
    "    \n",
    "    # ä½¿ç”¨å·¦è¿æ¥å°†èšåˆç»“æœåˆå¹¶åˆ°ä¸»è¡¨\n",
    "    # left joinç¡®ä¿ä¸»è¡¨çš„æ‰€æœ‰è®°å½•éƒ½ä¿ç•™\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
    "\n",
    "# ==================== 4. æ±‚å’Œèšåˆå‡½æ•° ====================\n",
    "def do_sum(dataframe, group_cols, counted, agg_name):\n",
    "    \"\"\"\n",
    "    ã€åˆ†ç»„æ±‚å’Œå¹¶æ·»åŠ ä¸ºæ–°åˆ—ã€‘\n",
    "    \n",
    "    åŠŸèƒ½ï¼šæŒ‰æŒ‡å®šåˆ—åˆ†ç»„ï¼Œå¯¹æŸä¸€åˆ—æ±‚å’Œï¼Œå¹¶å°†ç»“æœä½œä¸ºæ–°åˆ—æ·»åŠ å›åŸè¡¨\n",
    "    \n",
    "    ä¸¾ä¾‹ï¼š\n",
    "    åŸå§‹æ•°æ®ï¼š\n",
    "        å®¢æˆ·ID | è®¢å•ID | é€¾æœŸæ¬¡æ•°\n",
    "        1001   | A      | 1\n",
    "        1001   | B      | 0\n",
    "        1001   | C      | 2\n",
    "        1002   | D      | 0\n",
    "    \n",
    "    è°ƒç”¨ï¼šdo_sum(df, ['å®¢æˆ·ID'], 'é€¾æœŸæ¬¡æ•°', 'æ€»é€¾æœŸæ¬¡æ•°')\n",
    "    \n",
    "    ç»“æœï¼š\n",
    "        å®¢æˆ·ID | è®¢å•ID | é€¾æœŸæ¬¡æ•° | æ€»é€¾æœŸæ¬¡æ•°\n",
    "        1001   | A      | 1        | 3\n",
    "        1001   | B      | 0        | 3\n",
    "        1001   | C      | 2        | 3\n",
    "        1002   | D      | 0        | 0\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        dataframe: è¾“å…¥æ•°æ®\n",
    "        group_cols: åˆ†ç»„åˆ—ï¼ˆåˆ—è¡¨ï¼‰\n",
    "        counted: éœ€è¦æ±‚å’Œçš„åˆ—å\n",
    "        agg_name: æ–°åˆ—çš„åç§°\n",
    "    \n",
    "    åº”ç”¨åœºæ™¯ï¼š\n",
    "    - è®¡ç®—å®¢æˆ·çš„æ€»é€¾æœŸæ¬¡æ•°\n",
    "    - è®¡ç®—å®¢æˆ·çš„æ€»è¿˜æ¬¾é‡‘é¢\n",
    "    - ç»Ÿè®¡å®¢æˆ·çš„å†å²è®¢å•æ€»æ•°\n",
    "    \"\"\"\n",
    "    # æŒ‰æŒ‡å®šåˆ—åˆ†ç»„å¹¶æ±‚å’Œ\n",
    "    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n",
    "    \n",
    "    # å°†æ±‚å’Œç»“æœåˆå¹¶å›åŸè¡¨\n",
    "    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# ==================== 5. å†…å­˜ä¼˜åŒ–å‡½æ•° ====================\n",
    "def reduce_mem_usage(dataframe):\n",
    "    \"\"\"\n",
    "    ã€å†…å­˜ä½¿ç”¨ä¼˜åŒ– - æ ¸å¿ƒæŠ€æœ¯ã€‘\n",
    "    \n",
    "    åŠŸèƒ½ï¼šé€šè¿‡é™ä½æ•°æ®ç±»å‹ç²¾åº¦æ¥å‡å°‘å†…å­˜å ç”¨\n",
    "    \n",
    "    åŸç†ï¼š\n",
    "    pandasé»˜è®¤ä½¿ç”¨int64å’Œfloat64ï¼Œä½†å¾ˆå¤šæ—¶å€™æ•°æ®çš„å®é™…èŒƒå›´ä¸éœ€è¦è¿™ä¹ˆå¤§çš„å­˜å‚¨ç©ºé—´\n",
    "    \n",
    "    æ•°æ®ç±»å‹åŠå…¶èŒƒå›´ï¼š\n",
    "    æ•´æ•°ç±»å‹ï¼š\n",
    "    - int8:   -128 åˆ° 127\n",
    "    - int16:  -32,768 åˆ° 32,767\n",
    "    - int32:  -2,147,483,648 åˆ° 2,147,483,647\n",
    "    - int64:  -9,223,372,036,854,775,808 åˆ° 9,223,372,036,854,775,807\n",
    "    \n",
    "    æµ®ç‚¹æ•°ç±»å‹ï¼š\n",
    "    - float16: åŠç²¾åº¦æµ®ç‚¹æ•°\n",
    "    - float32: å•ç²¾åº¦æµ®ç‚¹æ•°\n",
    "    - float64: åŒç²¾åº¦æµ®ç‚¹æ•°\n",
    "    \n",
    "    ä¸¾ä¾‹ï¼š\n",
    "    å‡è®¾æŸåˆ—çš„å€¼èŒƒå›´æ˜¯ [0, 100]\n",
    "    - é»˜è®¤ä½¿ç”¨int64ï¼šæ¯ä¸ªå€¼å ç”¨8å­—èŠ‚\n",
    "    - ä¼˜åŒ–ä¸ºint8ï¼šæ¯ä¸ªå€¼åªå ç”¨1å­—èŠ‚\n",
    "    - å†…å­˜å‡å°‘ï¼š87.5%\n",
    "    \n",
    "    å®é™…æ•ˆæœï¼š\n",
    "    æœ¬é¡¹ç›®ä¸­ï¼Œå†…å­˜å ç”¨ä»åŸæ¥çš„4å€å‹ç¼©åˆ°ç°åœ¨çš„å¤§å°\n",
    "    è¿™å¯¹äºKaggleçš„16GBå†…å­˜é™åˆ¶éå¸¸é‡è¦\n",
    "    \n",
    "    æ³¨æ„äº‹é¡¹ï¼š\n",
    "    - float16ç²¾åº¦è¾ƒä½ï¼Œå¯èƒ½å½±å“æŸäº›è®¡ç®—\n",
    "    - éœ€è¦ç¡®ä¿æ•°å€¼èŒƒå›´åœ¨ç±»å‹é™åˆ¶å†…\n",
    "    \"\"\"\n",
    "    # è®°å½•ä¼˜åŒ–å‰çš„å†…å­˜ä½¿ç”¨ï¼ˆå•ä½ï¼šMBï¼‰\n",
    "    m_start = dataframe.memory_usage().sum() / 1024 ** 2\n",
    "    \n",
    "    # éå†æ¯ä¸€åˆ—\n",
    "    for col in dataframe.columns:\n",
    "        col_type = dataframe[col].dtype\n",
    "        \n",
    "        # åªå¤„ç†æ•°å€¼ç±»å‹ï¼Œè·³è¿‡objectç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼‰\n",
    "        if col_type != object:\n",
    "            c_min = dataframe[col].min()  # åˆ—çš„æœ€å°å€¼\n",
    "            c_max = dataframe[col].max()  # åˆ—çš„æœ€å¤§å€¼\n",
    "            \n",
    "            # å¤„ç†æ•´æ•°ç±»å‹\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # æ ¹æ®æ•°å€¼èŒƒå›´é€‰æ‹©æœ€å°çš„æ•´æ•°ç±»å‹\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int64)\n",
    "            \n",
    "            # å¤„ç†æµ®ç‚¹æ•°ç±»å‹\n",
    "            elif str(col_type)[:5] == 'float':\n",
    "                # æ ¹æ®æ•°å€¼èŒƒå›´é€‰æ‹©åˆé€‚çš„æµ®ç‚¹æ•°ç±»å‹\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float32)\n",
    "                else:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float64)\n",
    "    \n",
    "    # è®°å½•ä¼˜åŒ–åçš„å†…å­˜ä½¿ç”¨\n",
    "    m_end = dataframe.memory_usage().sum() / 1024 ** 2\n",
    "    \n",
    "    # å¯ä»¥æ‰“å°å†…å­˜å‡å°‘çš„ç™¾åˆ†æ¯”\n",
    "    # print(f'å†…å­˜ä½¿ç”¨ä» {m_start:.2f} MB å‡å°‘åˆ° {m_end:.2f} MB ({100 * (m_start - m_end) / m_start:.1f}% å‡å°‘)')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# å…¨å±€è®¾ç½®ï¼šå°†ç¼ºå¤±å€¼ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ç±»åˆ«\n",
    "nan_as_category = True\n",
    "\n",
    "\n",
    "# ==================== 6. é£é™©åˆ†ç»„å‡½æ•° ====================\n",
    "def risk_groupanizer(dataframe, column_names, target_val=1, upper_limit_ratio=8.2, lower_limit_ratio=8.2):\n",
    "    \"\"\"\n",
    "    ã€é£é™©åˆ†ç»„ç¼–ç å™¨ - é«˜çº§ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ã€‘\n",
    "    \n",
    "    åŠŸèƒ½ï¼šæ ¹æ®è¿çº¦ç‡å°†ç±»åˆ«ç‰¹å¾çš„æ¯ä¸ªç±»åˆ«æ ‡è®°ä¸ºé«˜/ä¸­/ä½é£é™©\n",
    "    \n",
    "    æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "    ä¸æ˜¯ç®€å•åœ°è¿›è¡ŒOne-Hotç¼–ç ï¼Œè€Œæ˜¯æå–æ¯ä¸ªç±»åˆ«çš„\"é£é™©ç¨‹åº¦\"ä¿¡æ¯\n",
    "    è¿™ç§æ–¹æ³•æ¯”ä¼ ç»ŸOne-Hotç¼–ç æ›´æœ‰ä¿¡æ¯é‡ï¼Œä¸”å¤§å¹…å‡å°‘ç‰¹å¾æ•°é‡\n",
    "    \n",
    "    è¯¦ç»†ä¸¾ä¾‹ï¼š\n",
    "    å‡è®¾æœ‰\"èŒä¸š\"è¿™ä¸ªç±»åˆ«ç‰¹å¾ï¼Œæ•°æ®å¦‚ä¸‹ï¼š\n",
    "    \n",
    "    èŒä¸š         | å®¢æˆ·æ•° | è¿çº¦æ•° | è¿çº¦ç‡\n",
    "    å·¥ç¨‹å¸ˆ       | 1000   | 50     | 5%    <- ä½é£é™©\n",
    "    æ•™å¸ˆ         | 800    | 65     | 8.1%  <- é«˜é£é™©ï¼ˆ>=8.2%å¯è°ƒï¼‰\n",
    "    åŒ»ç”Ÿ         | 500    | 40     | 8%    <- ä¸­ç­‰é£é™©\n",
    "    æ— ä¸š         | 200    | 30     | 15%   <- é«˜é£é™©\n",
    "    \n",
    "    ä¼ ç»ŸOne-Hotç¼–ç ä¼šåˆ›å»º4åˆ—ï¼ˆèŒä¸š_å·¥ç¨‹å¸ˆ, èŒä¸š_æ•™å¸ˆ, èŒä¸š_åŒ»ç”Ÿ, èŒä¸š_æ— ä¸šï¼‰\n",
    "    \n",
    "    é£é™©åˆ†ç»„ç¼–ç åªåˆ›å»º2-3åˆ—ï¼š\n",
    "    - èŒä¸š_high_risk: æ•™å¸ˆå’Œæ— ä¸šæ ‡è®°ä¸º1ï¼Œå…¶ä»–ä¸º0\n",
    "    - èŒä¸š_low_risk:  å·¥ç¨‹å¸ˆæ ‡è®°ä¸º1ï¼Œå…¶ä»–ä¸º0\n",
    "    - èŒä¸š_medium_risk: åŒ»ç”Ÿæ ‡è®°ä¸º1ï¼Œå…¶ä»–ä¸º0ï¼ˆå¦‚æœè®¾ç½®äº†ä¸­ç­‰é£é™©é˜ˆå€¼ï¼‰\n",
    "    \n",
    "    ä¼˜åŠ¿ï¼š\n",
    "    1. ç‰¹å¾æ•°é‡å¤§å¹…å‡å°‘ï¼ˆä»Nä¸ªç±»åˆ«å‡å°‘åˆ°2-3ä¸ªï¼‰\n",
    "    2. åŒ…å«äº†è¿çº¦ç‡ä¿¡æ¯ï¼Œæ›´æœ‰é¢„æµ‹åŠ›\n",
    "    3. é¿å…äº†é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾çš„ç»´åº¦çˆ†ç‚¸é—®é¢˜\n",
    "    4. å¯¹æ¨¡å‹æ¥è¯´ï¼Œé«˜/ä½é£é™©æ ‡è®°æ¯”å…·ä½“ç±»åˆ«å€¼æ›´æœ‰æ„ä¹‰\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        dataframe: è¾“å…¥æ•°æ®ï¼ˆå¿…é¡»åŒ…å«TARGETåˆ—ï¼‰\n",
    "        column_names: éœ€è¦è¿›è¡Œé£é™©åˆ†ç»„çš„åˆ—ååˆ—è¡¨\n",
    "        target_val: ç›®æ ‡å€¼ï¼ˆ1è¡¨ç¤ºè¿çº¦ï¼‰\n",
    "        upper_limit_ratio: é«˜é£é™©é˜ˆå€¼ï¼ˆé»˜è®¤8.2%ï¼Œå³è¿çº¦ç‡>=8.2%ä¸ºé«˜é£é™©ï¼‰\n",
    "        lower_limit_ratio: ä½é£é™©é˜ˆå€¼ï¼ˆé»˜è®¤8.2%ï¼Œå³è¿çº¦ç‡<=8.2%ä¸ºä½é£é™©ï¼‰\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        dataframe: æ·»åŠ äº†é£é™©æ ‡è®°åˆ—çš„æ•°æ®\n",
    "        new_columns: æ–°å¢çš„åˆ—ååˆ—è¡¨\n",
    "    \n",
    "    å®é™…åº”ç”¨ï¼š\n",
    "    åœ¨ä¿¡ç”¨è¯„åˆ†ä¸­ï¼ŒæŸäº›èŒä¸šã€åœ°åŒºã€æ”¶å…¥ç±»å‹ç¡®å®æœ‰æ˜¾è‘—ä¸åŒçš„è¿çº¦å€¾å‘\n",
    "    è¿™ä¸ªå‡½æ•°èƒ½è‡ªåŠ¨è¯†åˆ«å¹¶æ ‡è®°è¿™äº›é«˜é£é™©ç¾¤ä½“\n",
    "    \"\"\"\n",
    "    # one-hot encoder killer :-) \n",
    "    # æ³¨é‡Šï¼šè¿™ä¸ªæ–¹æ³•æ˜¯One-Hotç¼–ç çš„\"æ€æ‰‹\"ï¼Œå› ä¸ºå®ƒèƒ½ç”¨æ›´å°‘çš„ç‰¹å¾è¡¨è¾¾æ›´å¤šçš„ä¿¡æ¯\n",
    "    \n",
    "    # ä¿å­˜åŸå§‹åˆ—å\n",
    "    all_cols = dataframe.columns\n",
    "    \n",
    "    # éå†æ¯ä¸ªéœ€è¦å¤„ç†çš„ç±»åˆ«åˆ—\n",
    "    for col in column_names:\n",
    "        \n",
    "        # æ­¥éª¤1ï¼šè®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¿çº¦ç‡\n",
    "        # æŒ‰ç±»åˆ«å’ŒTARGETåˆ†ç»„ï¼Œç»Ÿè®¡å®¢æˆ·æ•°é‡\n",
    "        temp_df = dataframe.groupby([col] + ['TARGET'])[['SK_ID_CURR']].count().reset_index()\n",
    "        \n",
    "        # è®¡ç®—è¿çº¦ç‡ç™¾åˆ†æ¯”\n",
    "        # å…¬å¼ï¼š(è¯¥ç±»åˆ«è¿çº¦æ•° / è¯¥ç±»åˆ«æ€»æ•°) * 100\n",
    "        temp_df['ratio%'] = round(temp_df['SK_ID_CURR']*100/temp_df.groupby([col])['SK_ID_CURR'].transform('sum'), 1)\n",
    "        \n",
    "        # æ­¥éª¤2ï¼šè¯†åˆ«é«˜é£é™©ç±»åˆ«\n",
    "        # ç­›é€‰å‡ºè¿çº¦ç‡ >= upper_limit_ratio çš„ç±»åˆ«\n",
    "        col_groups_high_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                                       (temp_df['ratio%'] >= upper_limit_ratio)][col].tolist()\n",
    "        \n",
    "        # æ­¥éª¤3ï¼šè¯†åˆ«ä½é£é™©ç±»åˆ«\n",
    "        # ç­›é€‰å‡ºè¿çº¦ç‡ <= lower_limit_ratio çš„ç±»åˆ«\n",
    "        col_groups_low_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                                      (lower_limit_ratio >= temp_df['ratio%'])][col].tolist()\n",
    "        \n",
    "        # æ­¥éª¤4ï¼šå¦‚æœè®¾ç½®äº†ä¸åŒçš„ä¸Šä¸‹é™é˜ˆå€¼ï¼Œè¿˜å¯ä»¥è¯†åˆ«ä¸­ç­‰é£é™©ç±»åˆ«\n",
    "        if upper_limit_ratio != lower_limit_ratio:\n",
    "            col_groups_medium_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                (upper_limit_ratio > temp_df['ratio%']) & (temp_df['ratio%'] > lower_limit_ratio)][col].tolist()\n",
    "            \n",
    "            # åˆ›å»ºä¸‰ä¸ªé£é™©æ ‡è®°åˆ—\n",
    "            for risk, col_groups in zip(['_high_risk', '_medium_risk', '_low_risk'],\n",
    "                                        [col_groups_high_risk, col_groups_medium_risk, col_groups_low_risk]):\n",
    "                # å¦‚æœè¯¥æ ·æœ¬çš„ç±»åˆ«å€¼åœ¨å¯¹åº”é£é™©ç»„ä¸­ï¼Œæ ‡è®°ä¸º1ï¼Œå¦åˆ™ä¸º0\n",
    "                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n",
    "        else:\n",
    "            # å¦‚æœä¸Šä¸‹é™ç›¸åŒï¼Œåªåˆ›å»ºé«˜é£é™©å’Œä½é£é™©ä¸¤åˆ—\n",
    "            for risk, col_groups in zip(['_high_risk', '_low_risk'], [col_groups_high_risk, col_groups_low_risk]):\n",
    "                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n",
    "        \n",
    "        # æ­¥éª¤5ï¼šåˆ é™¤åŸå§‹çš„ç±»åˆ«åˆ—ï¼ˆå› ä¸ºå·²ç»è½¬æ¢ä¸ºé£é™©æ ‡è®°åˆ—ï¼‰\n",
    "        if dataframe[col].dtype == 'O' or dataframe[col].dtype == 'object':\n",
    "            dataframe.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # è¿”å›å¤„ç†åçš„æ•°æ®å’Œæ–°å¢çš„åˆ—ååˆ—è¡¨\n",
    "    return dataframe, list(set(dataframe.columns).difference(set(all_cols)))\n",
    "\n",
    "\n",
    "# ==================== 7. LightGBMç‰¹å¾é€‰æ‹©å‡½æ•° ====================\n",
    "def ligthgbm_feature_selection(dataframe, index_cols, auc_limit=0.7):\n",
    "    \"\"\"\n",
    "    ã€åŸºäºLightGBMçš„ç‰¹å¾é€‰æ‹© - è¿­ä»£å¼ç‰¹å¾ç­›é€‰ã€‘\n",
    "    \n",
    "    åŠŸèƒ½ï¼šä½¿ç”¨LightGBMæ¨¡å‹è‡ªåŠ¨ç­›é€‰é‡è¦ç‰¹å¾ï¼Œåˆ é™¤å¯¹é¢„æµ‹æ— è´¡çŒ®çš„ç‰¹å¾\n",
    "    \n",
    "    æ ¸å¿ƒåŸç†ï¼š\n",
    "    LightGBMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼ˆfeature importanceï¼‰\n",
    "    - é‡è¦æ€§ > 0: è¯¥ç‰¹å¾å¯¹æ¨¡å‹æœ‰è´¡çŒ®\n",
    "    - é‡è¦æ€§ = 0: è¯¥ç‰¹å¾ä»æœªè¢«æ¨¡å‹ä½¿ç”¨ï¼Œå¯ä»¥åˆ é™¤\n",
    "    \n",
    "    ç®—æ³•æµç¨‹ï¼š\n",
    "    1. ä½¿ç”¨æ‰€æœ‰ç‰¹å¾è®­ç»ƒLightGBMæ¨¡å‹\n",
    "    2. è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§\n",
    "    3. åˆ é™¤é‡è¦æ€§ä¸º0çš„ç‰¹å¾\n",
    "    4. é‡å¤æ­¥éª¤1-3ï¼Œç›´åˆ°ï¼š\n",
    "       - æ‰€æœ‰ç‰¹å¾éƒ½æœ‰è´¡çŒ®ï¼ˆé‡è¦æ€§>0ï¼‰ï¼Œæˆ–\n",
    "       - æ¨¡å‹AUCåˆ†æ•°ä¸‹é™åˆ°é˜ˆå€¼ä»¥ä¸‹ï¼ˆè¯´æ˜åˆ é™¤å¤ªå¤šäº†ï¼‰\n",
    "    \n",
    "    ä¸¾ä¾‹è¯´æ˜ï¼š\n",
    "    å‡è®¾æœ‰100ä¸ªç‰¹å¾ï¼š\n",
    "    \n",
    "    ç¬¬1è½®ï¼š\n",
    "    - è®­ç»ƒæ¨¡å‹ï¼ŒAUC=0.85\n",
    "    - å‘ç°20ä¸ªç‰¹å¾é‡è¦æ€§=0ï¼Œåˆ é™¤\n",
    "    - å‰©ä½™80ä¸ªç‰¹å¾\n",
    "    \n",
    "    ç¬¬2è½®ï¼š\n",
    "    - ä½¿ç”¨80ä¸ªç‰¹å¾è®­ç»ƒï¼ŒAUC=0.84\n",
    "    - å‘ç°10ä¸ªç‰¹å¾é‡è¦æ€§=0ï¼Œåˆ é™¤\n",
    "    - å‰©ä½™70ä¸ªç‰¹å¾\n",
    "    \n",
    "    ç¬¬3è½®ï¼š\n",
    "    - ä½¿ç”¨70ä¸ªç‰¹å¾è®­ç»ƒï¼ŒAUC=0.68 < 0.7ï¼ˆè¾¾åˆ°é˜ˆå€¼ï¼‰\n",
    "    - åœæ­¢åˆ é™¤ï¼Œä¿ç•™80ä¸ªç‰¹å¾\n",
    "    \n",
    "    ä¼˜åŠ¿ï¼š\n",
    "    1. è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©ï¼Œæ— éœ€äººå·¥åˆ¤æ–­\n",
    "    2. åˆ é™¤å†—ä½™ç‰¹å¾ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©\n",
    "    3. å‡å°‘ç‰¹å¾æ•°é‡ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦\n",
    "    4. åŸºäºæ¨¡å‹æœ¬èº«åˆ¤æ–­ï¼Œæ›´åŠ å¯é \n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        dataframe: åŒ…å«æ‰€æœ‰ç‰¹å¾çš„æ•°æ®é›†\n",
    "        index_cols: ç´¢å¼•åˆ—ï¼ˆå¦‚SK_ID_CURRï¼‰ï¼Œä¸å‚ä¸ç‰¹å¾é€‰æ‹©\n",
    "        auc_limit: AUCé˜ˆå€¼ï¼Œä½äºæ­¤å€¼æ—¶åœæ­¢åˆ é™¤ç‰¹å¾\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        ç­›é€‰åçš„dataframeï¼ˆåˆ é™¤äº†æ— è´¡çŒ®ç‰¹å¾ï¼‰\n",
    "    \n",
    "    æ³¨æ„ï¼š\n",
    "    æœ¬é¡¹ç›®ä¸­ç”±äºå†…å­˜é™åˆ¶ï¼Œå®é™…ä½¿ç”¨æ—¶è¯»å–äº†é¢„å…ˆè®¡ç®—å¥½çš„ç‰¹å¾åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # æ¸…ç†ç‰¹å¾åç§°ï¼Œç¡®ä¿åªåŒ…å«å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿\n",
    "    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n",
    "    \n",
    "    # åˆå§‹åŒ–LightGBMåˆ†ç±»å™¨\n",
    "    clf = LGBMClassifier(random_state=0)\n",
    "    \n",
    "    # åˆ†ç¦»è®­ç»ƒé›†ï¼ˆæœ‰TARGETæ ‡ç­¾çš„æ•°æ®ï¼‰\n",
    "    train_df = dataframe[dataframe['TARGET'].notnull()]\n",
    "    train_df_X = train_df.drop('TARGET', axis=1)  # ç‰¹å¾\n",
    "    train_df_y = train_df['TARGET']  # æ ‡ç­¾\n",
    "    \n",
    "    # è·å–æ‰€æœ‰å‚ä¸è®­ç»ƒçš„åˆ—ï¼ˆæ’é™¤ç´¢å¼•åˆ—ï¼‰\n",
    "    train_columns = [col for col in train_df_X.columns if col not in index_cols]\n",
    "    \n",
    "    # åˆå§‹åŒ–ï¼šå‡è®¾å½“å‰AUCä¸º1ï¼Œæœ€ä¼˜ç‰¹å¾é›†ä¸ºç©º\n",
    "    max_auc_score = 1\n",
    "    best_cols = []\n",
    "    \n",
    "    # è¿­ä»£åˆ é™¤æ— ç”¨ç‰¹å¾ï¼Œç›´åˆ°AUCä½äºé˜ˆå€¼\n",
    "    while max_auc_score > auc_limit:\n",
    "        # æ’é™¤å·²ç»ç¡®å®šè¦ä¿ç•™çš„ç‰¹å¾\n",
    "        train_columns = [col for col in train_columns if col not in best_cols]\n",
    "        \n",
    "        # ä½¿ç”¨å½“å‰ç‰¹å¾é›†è®­ç»ƒæ¨¡å‹\n",
    "        clf.fit(train_df_X[train_columns], train_df_y)\n",
    "        \n",
    "        # è·å–æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§\n",
    "        feats_imp = pd.Series(clf.feature_importances_, index=train_columns)\n",
    "        \n",
    "        # è®¡ç®—å½“å‰æ¨¡å‹çš„AUCåˆ†æ•°\n",
    "        max_auc_score = roc_auc_score(train_df_y, clf.predict_proba(train_df_X[train_columns])[:, 1])\n",
    "        \n",
    "        # ä¿ç•™é‡è¦æ€§å¤§äº0çš„ç‰¹å¾ï¼ˆæœ‰è´¡çŒ®çš„ç‰¹å¾ï¼‰\n",
    "        best_cols = feats_imp[feats_imp > 0].index.tolist()\n",
    "    \n",
    "    # åˆ é™¤è¢«ç­›é€‰æ‰çš„ç‰¹å¾ï¼ˆæ— è´¡çŒ®çš„ç‰¹å¾ï¼‰\n",
    "    dataframe.drop(train_columns, axis=1, inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-trigger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.186033Z",
     "iopub.status.busy": "2021-05-04T15:38:40.184341Z",
     "iopub.status.idle": "2021-05-04T15:38:40.186723Z",
     "shell.execute_reply": "2021-05-04T15:38:40.187221Z"
    },
    "papermill": {
     "duration": 0.045598,
     "end_time": "2021-05-04T15:38:40.187389",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.141791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€ä¸»ç”³è¯·è¡¨ç‰¹å¾å·¥ç¨‹å‡½æ•°ã€‘\n",
    "å¤„ç†application_train.csvå’Œapplication_test.csv\n",
    "è¿™æ˜¯ä¸»æ•°æ®è¡¨ï¼ŒåŒ…å«å®¢æˆ·çš„åŸºæœ¬ä¿¡æ¯å’Œç”³è¯·ä¿¡æ¯\n",
    "\"\"\"\n",
    "def application():\n",
    "    \"\"\"\n",
    "    å¤„ç†ä¸»ç”³è¯·è¡¨æ•°æ®ï¼Œåˆ›å»ºå¤§é‡è¡ç”Ÿç‰¹å¾\n",
    "    \n",
    "    æ•°æ®è¡¨è¯´æ˜ï¼š\n",
    "    - application_train.csv: è®­ç»ƒé›†ï¼ŒåŒ…å«TARGETï¼ˆæ˜¯å¦è¿çº¦ï¼‰\n",
    "    - application_test.csv: æµ‹è¯•é›†ï¼Œéœ€è¦é¢„æµ‹TARGET\n",
    "    \n",
    "    ä¸»è¦ç‰¹å¾ç±»å‹ï¼š\n",
    "    1. åŸºæœ¬ä¿¡æ¯ï¼šæ€§åˆ«ã€å¹´é¾„ã€å®¶åº­æˆå‘˜æ•°ç­‰\n",
    "    2. è´¢åŠ¡ä¿¡æ¯ï¼šæ”¶å…¥ã€ä¿¡è´·é‡‘é¢ã€å¹´é‡‘ç­‰\n",
    "    3. å¤–éƒ¨è¯„åˆ†ï¼šEXT_SOURCE_1/2/3ï¼ˆæ¥è‡ªå¤–éƒ¨æ•°æ®æºçš„ä¿¡ç”¨è¯„åˆ†ï¼‰\n",
    "    4. æ—¶é—´ä¿¡æ¯ï¼šå°±ä¸šå¤©æ•°ã€å‡ºç”Ÿæ—¥æœŸç­‰\n",
    "    \"\"\"\n",
    "    # ==================== æ•°æ®åŠ è½½å’Œåˆå¹¶ ====================\n",
    "    # è¯»å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    df = pd.read_csv(r'../input/home-credit-default-risk/application_train.csv')\n",
    "    test_df = pd.read_csv(r'../input/home-credit-default-risk/application_test.csv')\n",
    "    \n",
    "    # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä¾¿äºç»Ÿä¸€å¤„ç†ç‰¹å¾å·¥ç¨‹\n",
    "    # æ³¨æ„ï¼šæµ‹è¯•é›†çš„TARGETåˆ—ä¸ºNaN\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # ==================== æ•°æ®æ¸…æ´— ====================\n",
    "    # 1. åˆ é™¤æ€§åˆ«å¼‚å¸¸å€¼\n",
    "    # CODE_GENDER='XNA'è¡¨ç¤ºæ€§åˆ«æœªçŸ¥ï¼Œè¿™ç±»æ ·æœ¬æ•°é‡æå°‘ä¸”å¯èƒ½æœ‰é—®é¢˜\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # 2. åˆ é™¤æ”¶å…¥å¼‚å¸¸å€¼\n",
    "    # æœ‰ä¸€ä¸ªç¦»ç¾¤ç‚¹æ”¶å…¥ä¸º117Mï¼ˆ1.17äº¿ï¼‰ï¼Œæ˜æ˜¾æ˜¯é”™è¯¯æ•°æ®\n",
    "    df = df[df['AMT_INCOME_TOTAL'] < 20000000]\n",
    "    \n",
    "    # 3. å¤„ç†DAYS_EMPLOYEDçš„å¼‚å¸¸å€¼\n",
    "    # 365243å¤©ï¼ˆçº¦1000å¹´ï¼‰æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œè¡¨ç¤ºç¼ºå¤±å€¼\n",
    "    # å°†å…¶è½¬æ¢ä¸ºNaNï¼Œè®©æ¨¡å‹èƒ½æ­£ç¡®å¤„ç†\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # 4. å¤„ç†DAYS_LAST_PHONE_CHANGEçš„å¼‚å¸¸å€¼\n",
    "    # 0è¡¨ç¤ºä»æœªæ›´æ¢è¿‡ç”µè¯ï¼Œåœ¨è¿™ä¸ªæ•°æ®é›†ä¸­è¢«è§†ä¸ºå¼‚å¸¸\n",
    "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # ==================== ç±»åˆ«ç‰¹å¾ç¼–ç  ====================\n",
    "    # 1. äºŒå…ƒç±»åˆ«ç‰¹å¾çš„æ ‡ç­¾ç¼–ç \n",
    "    # å¯¹äºåªæœ‰ä¸¤ä¸ªç±»åˆ«çš„ç‰¹å¾ï¼Œä½¿ç”¨ç®€å•çš„0/1ç¼–ç å³å¯\n",
    "    # ä¾‹å¦‚ï¼šæ€§åˆ«ï¼ˆç”·/å¥³ï¼‰ã€æ˜¯å¦æœ‰è½¦ï¼ˆæ˜¯/å¦ï¼‰ã€æ˜¯å¦æœ‰æˆ¿äº§ï¼ˆæ˜¯/å¦ï¼‰\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    \n",
    "    # 2. å¤šç±»åˆ«ç‰¹å¾çš„One-Hotç¼–ç \n",
    "    # å¯¹äºæœ‰å¤šä¸ªç±»åˆ«çš„ç‰¹å¾ï¼ˆå¦‚èŒä¸šç±»å‹ã€æ•™è‚²ç¨‹åº¦ç­‰ï¼‰ï¼Œä½¿ç”¨One-Hotç¼–ç \n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # ==================== æ–‡æ¡£ç‰¹å¾èšåˆ ====================\n",
    "    # FLAG_DOC_X ç³»åˆ—ç‰¹å¾è¡¨ç¤ºå®¢æˆ·æ˜¯å¦æä¾›äº†æŸç§æ–‡æ¡£\n",
    "    # ä¾‹å¦‚ï¼šFLAG_DOC_2=1è¡¨ç¤ºæä¾›äº†2å·æ–‡æ¡£ï¼Œ=0è¡¨ç¤ºæœªæä¾›\n",
    "    \n",
    "    # ç»Ÿè®¡å®¢æˆ·æä¾›çš„æ–‡æ¡£æ€»æ•°\n",
    "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "    \n",
    "    # è®¡ç®—æ–‡æ¡£æäº¤çš„å³°åº¦ï¼ˆkurtosisï¼‰\n",
    "    # å³°åº¦è¡¡é‡åˆ†å¸ƒçš„å°–é”ç¨‹åº¦ï¼Œå¯ä»¥åæ˜ å®¢æˆ·æä¾›æ–‡æ¡£çš„é›†ä¸­ç¨‹åº¦\n",
    "    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
    "\n",
    "    # ==================== å¹´é¾„åˆ†ç»„ç‰¹å¾ ====================\n",
    "    def get_age_label(days_birth):\n",
    "        \"\"\"\n",
    "        å°†å¹´é¾„è½¬æ¢ä¸ºç¦»æ•£çš„å¹´é¾„ç»„æ ‡ç­¾\n",
    "        \n",
    "        å¹´é¾„åˆ†ç»„ç­–ç•¥åŸºäºè¿çº¦ç‡åˆ†æï¼š\n",
    "        - ä¸åŒå¹´é¾„æ®µçš„è¿çº¦ç‡å·®å¼‚æ˜¾è‘—\n",
    "        - å°†è¿ç»­çš„å¹´é¾„å˜é‡è½¬æ¢ä¸ºç±»åˆ«å˜é‡\n",
    "        - æœ‰åŠ©äºæ¨¡å‹æ•æ‰éçº¿æ€§çš„å¹´é¾„æ•ˆåº”\n",
    "        \"\"\"\n",
    "        # DAYS_BIRTHæ˜¯è´Ÿæ•°ï¼ˆè¡¨ç¤ºå¤šå°‘å¤©å‰å‡ºç”Ÿï¼‰ï¼Œè½¬æ¢ä¸ºå®é™…å¹´é¾„\n",
    "        age_years = -days_birth / 365\n",
    "        \n",
    "        # æ ¹æ®è¿çº¦ç‡åˆ†å¸ƒå°†å¹´é¾„åˆ†ä¸º5ç»„\n",
    "        if age_years < 27: return 1      # å¹´è½»ç¾¤ä½“ï¼ˆé«˜é£é™©ï¼‰\n",
    "        elif age_years < 40: return 2    # é’å¹´ç¾¤ä½“\n",
    "        elif age_years < 50: return 3    # ä¸­å¹´ç¾¤ä½“\n",
    "        elif age_years < 65: return 4    # ä¸­è€å¹´ç¾¤ä½“\n",
    "        elif age_years < 99: return 5    # è€å¹´ç¾¤ä½“ï¼ˆä½é£é™©ï¼‰\n",
    "        else: return 0                   # å¼‚å¸¸å€¼\n",
    "    \n",
    "    # åº”ç”¨å¹´é¾„åˆ†ç»„å‡½æ•°\n",
    "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "\n",
    "    # ==================== å¤–éƒ¨ä¿¡ç”¨è¯„åˆ†ç‰¹å¾å·¥ç¨‹ ====================\n",
    "    # EXT_SOURCE_1/2/3 æ˜¯æ¥è‡ªå¤–éƒ¨æ•°æ®æºçš„æ ‡å‡†åŒ–ä¿¡ç”¨è¯„åˆ†\n",
    "    # è¿™äº›æ˜¯æœ€é‡è¦çš„ç‰¹å¾ä¹‹ä¸€ï¼Œéœ€è¦å……åˆ†æŒ–æ˜å®ƒä»¬çš„ä¿¡æ¯\n",
    "    \n",
    "    # 1. ä¸‰ä¸ªè¯„åˆ†çš„ä¹˜ç§¯\n",
    "    # å¦‚æœå®¢æˆ·åœ¨æ‰€æœ‰è¯„åˆ†æºéƒ½å¾—é«˜åˆ†ï¼Œä¹˜ç§¯ä¼šå¾ˆå¤§\n",
    "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    \n",
    "    # 2. åŠ æƒå¹³å‡åˆ†\n",
    "    # æ ¹æ®ç»éªŒï¼ŒEXT_SOURCE_3çš„æƒé‡æœ€é«˜ï¼ˆæƒé‡=3ï¼‰\n",
    "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "    \n",
    "    # å¿½ç•¥å…¨NaNåˆ‡ç‰‡çš„è­¦å‘Š\n",
    "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    \n",
    "    # 3. è®¡ç®—ä¸‰ä¸ªè¯„åˆ†çš„ç»Ÿè®¡ç‰¹å¾\n",
    "    # è¿™äº›ç»Ÿè®¡é‡èƒ½åæ˜ è¯„åˆ†çš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "        # min: æœ€ä½è¯„åˆ†ï¼ˆè¡¡é‡æœ€å·®æƒ…å†µï¼‰\n",
    "        # max: æœ€é«˜è¯„åˆ†ï¼ˆè¡¡é‡æœ€å¥½æƒ…å†µï¼‰\n",
    "        # mean: å¹³å‡è¯„åˆ†ï¼ˆç»¼åˆè¯„ä»·ï¼‰\n",
    "        # nanmedian: ä¸­ä½æ•°ï¼ˆå¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥ï¼‰\n",
    "        # var: æ–¹å·®ï¼ˆè¯„åˆ†çš„ä¸€è‡´æ€§ï¼Œæ–¹å·®å¤§è¯´æ˜è¯„åˆ†å·®å¼‚å¤§ï¼‰\n",
    "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
    "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "    # ==================== åŸºç¡€æ¯”ç‡ç‰¹å¾ ====================\n",
    "    # æ¯”ç‡ç‰¹å¾ï¼ˆRatio Featuresï¼‰åœ¨ä¿¡ç”¨è¯„åˆ†ä¸­éå¸¸é‡è¦\n",
    "    # å®ƒä»¬èƒ½æ­ç¤ºä¸åŒå˜é‡ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ï¼Œæ¯”ç»å¯¹å€¼æ›´æœ‰ä¿¡æ¯é‡\n",
    "    \n",
    "    # 1. å°±ä¸šå ç”Ÿå‘½æ¯”ä¾‹\n",
    "    # è¡¡é‡å®¢æˆ·å·¥ä½œç»å†å å¹´é¾„çš„æ¯”ä¾‹\n",
    "    # ä¾‹å¦‚ï¼š30å²å·¥ä½œäº†10å¹´ï¼Œæ¯”ä¾‹=10/30=0.33\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 2. æ”¶å…¥ä¸ä¿¡è´·æ¯”ä¾‹\n",
    "    # è¡¡é‡è¿˜æ¬¾èƒ½åŠ›ï¼šæ”¶å…¥è¶Šé«˜ã€è´·æ¬¾é¢è¶Šå°‘ï¼Œæ¯”ä¾‹è¶Šå¤§ï¼Œé£é™©è¶Šä½\n",
    "    # ä¾‹å¦‚ï¼šå¹´æ”¶å…¥100ä¸‡ï¼Œè´·æ¬¾50ä¸‡ï¼Œæ¯”ä¾‹=2.0ï¼ˆè¿˜æ¬¾èƒ½åŠ›å¼ºï¼‰\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # 3. äººå‡æ”¶å…¥\n",
    "    # å®¶åº­æ€»æ”¶å…¥é™¤ä»¥å®¶åº­æˆå‘˜æ•°\n",
    "    # åæ˜ å®é™…å¯æ”¯é…æ”¶å…¥æ°´å¹³\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # 4. å¹´é‡‘å æ”¶å…¥æ¯”ä¾‹ï¼ˆè´Ÿå€ºæ”¶å…¥æ¯”ï¼‰\n",
    "    # æ¯å¹´è¿˜æ¬¾é‡‘é¢å å¹´æ”¶å…¥çš„æ¯”ä¾‹\n",
    "    # æ¯”ä¾‹è¶Šé«˜ï¼Œè¿˜æ¬¾å‹åŠ›è¶Šå¤§ï¼Œè¿çº¦é£é™©è¶Šé«˜\n",
    "    # ä¾‹å¦‚ï¼šå¹´æ”¶å…¥50ä¸‡ï¼Œå¹´è¿˜æ¬¾10ä¸‡ï¼Œæ¯”ä¾‹=0.2ï¼ˆ20%ï¼Œå‹åŠ›é€‚ä¸­ï¼‰\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # 5. è¿˜æ¬¾ç‡\n",
    "    # å¹´é‡‘å è´·æ¬¾æ€»é¢çš„æ¯”ä¾‹ï¼Œåæ˜ è¿˜æ¬¾è®¡åˆ’çš„æ¾ç´§ç¨‹åº¦\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "    # ==================== ä¿¡è´·ç›¸å…³æ¯”ç‡ ====================\n",
    "    # ä¿¡è´·é‡‘é¢ä¸å•†å“ä»·æ ¼çš„æ¯”ç‡\n",
    "    # å¦‚æœæ¯”ç‡>1ï¼Œè¯´æ˜è´·æ¬¾é¢è¶…è¿‡å•†å“ä»·å€¼ï¼ˆå¯èƒ½åŒ…å«å…¶ä»–è´¹ç”¨ï¼‰\n",
    "    # å¦‚æœæ¯”ç‡<1ï¼Œè¯´æ˜å®¢æˆ·æ”¯ä»˜äº†é¦–ä»˜\n",
    "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    # ==================== æ”¶å…¥ç›¸å…³æ¯”ç‡ ====================\n",
    "    # 1. æ”¶å…¥ä¸å°±ä¸šæ—¶é•¿æ¯”ç‡\n",
    "    # æ¯å·¥ä½œä¸€å¤©çš„å¹³å‡æ”¶å…¥ï¼Œåæ˜ æ”¶å…¥å¢é•¿é€Ÿåº¦\n",
    "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 2. æ”¶å…¥ä¸å¹´é¾„æ¯”ç‡\n",
    "    # è¡¡é‡æ”¶å…¥å¢é•¿ç‡ï¼Œå¹´è½»äººé«˜æ”¶å…¥è¯´æ˜å‘å±•æ½œåŠ›å¤§\n",
    "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # ==================== æ—¶é—´ç›¸å…³æ¯”ç‡ ====================\n",
    "    # è¿™äº›æ¯”ç‡åæ˜ å®¢æˆ·ç”Ÿæ´»ç¨³å®šæ€§\n",
    "    \n",
    "    # 1. èº«ä»½è¯å‘å¸ƒæ—¶é—´å å¹´é¾„æ¯”ä¾‹\n",
    "    # æ¯”ä¾‹è¶Šå°ï¼Œè¯´æ˜èº«ä»½è¯æ˜¯æœ€è¿‘æ‰åŠçš„\n",
    "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 2. è½¦é¾„å å¹´é¾„æ¯”ä¾‹\n",
    "    # åæ˜ è½¦è¾†æ–°æ—§ç¨‹åº¦ç›¸å¯¹äºå¹´é¾„çš„å…³ç³»\n",
    "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 3. è½¦é¾„å å°±ä¸šæ—¶é•¿æ¯”ä¾‹\n",
    "    # è½¦é¾„æ¥è¿‘å°±ä¸šæ—¶é•¿è¯´æ˜åˆšå·¥ä½œå°±ä¹°è½¦ï¼Œå¯èƒ½è´Ÿå€ºè¾ƒé‡\n",
    "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 4. æ‰‹æœºæ›´æ¢æ—¶é—´å å¹´é¾„æ¯”ä¾‹\n",
    "    # ç»å¸¸æ¢æ‰‹æœºå¯èƒ½åæ˜ ç”Ÿæ´»ä¸ç¨³å®š\n",
    "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "\n",
    "    # ==================== å¤–éƒ¨è¯„åˆ†çš„é«˜çº§ç»„åˆç‰¹å¾ ====================\n",
    "    # æ·±åº¦æŒ–æ˜EXT_SOURCEç³»åˆ—ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ˜¯é¢„æµ‹åŠ›æœ€å¼ºçš„\n",
    "    \n",
    "    # 1. å¤–éƒ¨è¯„åˆ†çš„å‡å€¼å’Œæ ‡å‡†å·®\n",
    "    df['APPS_EXT_SOURCE_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['APPS_EXT_SOURCE_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    # æ ‡å‡†å·®çš„ç¼ºå¤±å€¼ç”¨å‡å€¼å¡«å……ï¼ˆå½“åªæœ‰1-2ä¸ªè¯„åˆ†æ—¶ä¼šå‡ºç°NaNï¼‰\n",
    "    df['APPS_EXT_SOURCE_STD'] = df['APPS_EXT_SOURCE_STD'].fillna(df['APPS_EXT_SOURCE_STD'].mean())\n",
    "    \n",
    "    # 2. è¯„åˆ†ä¸å¹´é¾„çš„æ¯”ç‡\n",
    "    # è¡¡é‡ï¼šåœ¨ç›¸åŒå¹´é¾„ä¸‹çš„ä¿¡ç”¨è¯„åˆ†æ°´å¹³\n",
    "    df['APP_SCORE1_TO_BIRTH_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    df['APP_SCORE2_TO_BIRTH_RATIO'] = df['EXT_SOURCE_2'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    df['APP_SCORE3_TO_BIRTH_RATIO'] = df['EXT_SOURCE_3'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    \n",
    "    # 3. è¯„åˆ†ä¸å°±ä¸šæ—¶é•¿çš„æ¯”ç‡\n",
    "    # å·¥ä½œæ—¶é—´çŸ­ä½†è¯„åˆ†é«˜ï¼Œè¯´æ˜å®¢æˆ·æ½œåŠ›å¤§\n",
    "    df['APP_SCORE1_TO_EMPLOY_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_EMPLOYED'] / 365.25)\n",
    "    \n",
    "    # 4. è¯„åˆ†çš„ä¸‰å…ƒäº¤äº’ç‰¹å¾\n",
    "    # æ•æ‰è¯„åˆ†ä¸æ—¶é—´çš„å¤æ‚äº¤äº’å…³ç³»\n",
    "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['DAYS_BIRTH']\n",
    "    \n",
    "    # 5. è¯„åˆ†1ä¸å…¶ä»–å˜é‡çš„æ¯”ç‡\n",
    "    df['APP_SCORE1_TO_FAM_CNT_RATIO'] = df['EXT_SOURCE_1'] / df['CNT_FAM_MEMBERS']\n",
    "    df['APP_SCORE1_TO_GOODS_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_GOODS_PRICE']\n",
    "    df['APP_SCORE1_TO_CREDIT_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # 6. è¯„åˆ†ä¹‹é—´çš„æ¯”ç‡\n",
    "    # å¦‚æœå„è¯„åˆ†å·®å¼‚å¤§ï¼Œå¯èƒ½åæ˜ ä¿¡æ¯ä¸ä¸€è‡´\n",
    "    df['APP_SCORE1_TO_SCORE2_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_2']\n",
    "    df['APP_SCORE1_TO_SCORE3_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_3']\n",
    "    \n",
    "    # 7. è¯„åˆ†2ä¸å…¶ä»–å˜é‡çš„æ¯”ç‡\n",
    "    df['APP_SCORE2_TO_CREDIT_RATIO'] = df['EXT_SOURCE_2'] / df['AMT_CREDIT']\n",
    "    df['APP_SCORE2_TO_REGION_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT']\n",
    "    df['APP_SCORE2_TO_CITY_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT_W_CITY']\n",
    "    df['APP_SCORE2_TO_POP_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_POPULATION_RELATIVE']\n",
    "    df['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = df['EXT_SOURCE_2'] / df['DAYS_LAST_PHONE_CHANGE']\n",
    "    \n",
    "    # 8. è¯„åˆ†ä¹‹é—´çš„ä¸¤ä¸¤ä¹˜ç§¯ï¼ˆäº¤äº’ç‰¹å¾ï¼‰\n",
    "    # æ•æ‰ä¸åŒè¯„åˆ†æºä¹‹é—´çš„ååŒæ•ˆåº”\n",
    "    df['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n",
    "    df['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n",
    "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    \n",
    "    # 9. è¯„åˆ†ä¸å°±ä¸šå¤©æ•°çš„äº¤äº’\n",
    "    # é«˜è¯„åˆ†+é•¿å·¥ä½œæ—¶é—´ = æ›´ç¨³å®š\n",
    "    df['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n",
    "    df['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n",
    "    df['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n",
    "\n",
    "    # ==================== æ”¶å…¥ä¸å®¶åº­ç›¸å…³ç‰¹å¾ ====================\n",
    "    # 1. å•†å“ä»·æ ¼å æ”¶å…¥æ¯”ä¾‹\n",
    "    # åæ˜ è´­ä¹°åŠ›ï¼šæ¯”ä¾‹è¶Šä½ï¼Œè¯´æ˜å•†å“ç›¸å¯¹ä¾¿å®œï¼Œè¿˜æ¬¾å‹åŠ›å°\n",
    "    df['APPS_GOODS_INCOME_RATIO'] = df['AMT_GOODS_PRICE'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # 2. å®¶åº­äººå‡æ”¶å…¥ï¼ˆé‡å¤ç‰¹å¾ï¼Œä¸å‰é¢INCOME_PER_PERSONç›¸åŒï¼‰\n",
    "    df['APPS_CNT_FAM_INCOME_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # 3. æ”¶å…¥ä¸å°±ä¸šæ—¶é•¿æ¯”ç‡\n",
    "    # å¹³å‡æ¯å·¥ä½œä¸€å¤©è·å¾—çš„æ”¶å…¥ï¼Œåæ˜ æ”¶å…¥å¢é•¿é€Ÿåº¦\n",
    "    df['APPS_INCOME_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "\n",
    "    # ==================== ä»é«˜åˆ†æ¨¡å‹å€Ÿé‰´çš„é¢å¤–ç‰¹å¾ ====================\n",
    "    # ä»¥ä¸‹ç‰¹å¾æ¥è‡ªAUC>0.8çš„æ¨¡å‹ï¼Œè¯æ˜æœ‰è¾ƒå¼ºé¢„æµ‹åŠ›\n",
    "    \n",
    "    # 1. ä¿¡è´·å•†å“æ¯”ç‡ï¼ˆé‡å¤ç‰¹å¾ï¼Œå¢å¼ºè¯¥ç‰¹å¾çš„é‡è¦æ€§ï¼‰\n",
    "    df['CREDIT_TO_GOODS_RATIO_2'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    # 2. æœˆæ”¶å…¥å‡å»å¹´é‡‘ï¼ˆå¯æ”¯é…æ”¶å…¥ï¼‰\n",
    "    # è®¡ç®—æ¯æœˆè¿˜æ¬¾åçš„å‰©ä½™æ”¶å…¥\n",
    "    # ä¾‹å¦‚ï¼šæœˆæ”¶å…¥5ä¸‡ï¼Œå¹´è¿˜æ¬¾12ä¸‡ï¼ˆæœˆå‡1ä¸‡ï¼‰ï¼Œå‰©ä½™æ”¶å…¥=5-1=4ä¸‡\n",
    "    df['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']\n",
    "    \n",
    "    # 3. æ”¶å…¥å°±ä¸šæ¯”ï¼ˆé‡å¤ç‰¹å¾ï¼‰\n",
    "    df['APP_INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 4. æ‰‹æœºæ›´æ¢é¢‘ç‡ä¸å°±ä¸šæ—¶é•¿æ¯”\n",
    "    # æ‰‹æœºæ›´æ¢é¢‘ç¹å¯èƒ½åæ˜ ç”Ÿæ´»ä¸ç¨³å®š\n",
    "    df['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 5. å°±ä¸šæ—¶é•¿ä¸å¹´é¾„å·®\n",
    "    # å·®å€¼å¤§è¯´æ˜å¾ˆæ™šæ‰å¼€å§‹å·¥ä½œï¼Œå¯èƒ½å½±å“æ”¶å…¥ç§¯ç´¯\n",
    "    df['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n",
    "\n",
    "    # æ‰“å°æœ€ç»ˆæ•°æ®å½¢çŠ¶\n",
    "    print('\"Application_Train_Test\" final shape:', df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-atlas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.236779Z",
     "iopub.status.busy": "2021-05-04T15:38:40.235180Z",
     "iopub.status.idle": "2021-05-04T15:38:40.237499Z",
     "shell.execute_reply": "2021-05-04T15:38:40.237984Z"
    },
    "papermill": {
     "duration": 0.038152,
     "end_time": "2021-05-04T15:38:40.238153",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.200001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€ä¿¡ç”¨å±€æ•°æ®ç‰¹å¾å·¥ç¨‹å‡½æ•°ã€‘\n",
    "å¤„ç†bureau.csvå’Œbureau_balance.csv\n",
    "åŒ…å«å®¢æˆ·åœ¨å…¶ä»–é‡‘èæœºæ„çš„ä¿¡è´·å†å²è®°å½•\n",
    "\"\"\"\n",
    "def bureau_bb():\n",
    "    \"\"\"\n",
    "    ä¿¡ç”¨å±€(Bureau)æ•°æ®è¯´æ˜ï¼š\n",
    "    - bureau.csv: å®¢æˆ·åœ¨å…¶ä»–é‡‘èæœºæ„çš„æ‰€æœ‰å†å²ä¿¡è´·è®°å½•\n",
    "    - bureau_balance.csv: è¿™äº›ä¿¡è´·çš„æœˆåº¦ä½™é¢å˜åŒ–\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆä¿¡ç”¨å±€æ•°æ®é‡è¦ï¼Ÿ\n",
    "    1. åæ˜ å®¢æˆ·çš„æ•´ä½“ä¿¡ç”¨çŠ¶å†µï¼ˆä¸ä»…æ˜¯æœ¬å…¬å¸çš„ï¼‰\n",
    "    2. å†å²è¿˜æ¬¾è¡Œä¸ºæ˜¯è¿çº¦é£é™©çš„æœ€ä½³é¢„æµ‹å› å­\n",
    "    3. å¤šå¤´å€Ÿè´·æƒ…å†µï¼ˆåœ¨å¤šå®¶æœºæ„å€Ÿæ¬¾ï¼‰\n",
    "    \"\"\"\n",
    "    # è¯»å–ä¸¤ä¸ªç›¸å…³è¡¨\n",
    "    bureau = pd.read_csv(r'../input/home-credit-default-risk/bureau.csv')\n",
    "    bb = pd.read_csv(r'../input/home-credit-default-risk/bureau_balance.csv')\n",
    "\n",
    "    # ==================== æ—¶é—´ç›¸å…³ç‰¹å¾ ====================\n",
    "    # 1. ä¿¡è´·æŒç»­æ—¶é—´\n",
    "    # DAYS_CREDIT: è¯¥ç¬”ä¿¡è´·å¼€å§‹å‰å¤šå°‘å¤©\n",
    "    # DAYS_CREDIT_ENDDATE: è¯¥ç¬”ä¿¡è´·é¢„è®¡ç»“æŸå‰å¤šå°‘å¤©\n",
    "    # æŒç»­æ—¶é—´ = ç»“æŸæ—¥æœŸ - å¼€å§‹æ—¥æœŸ\n",
    "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "    \n",
    "    # 2. é¢„è®¡ç»“æŸæ—¥æœŸä¸å®é™…ç»“æŸæ—¥æœŸå·®å¼‚\n",
    "    # æ­£å€¼ï¼šæå‰è¿˜æ¸…ï¼ˆä¿¡ç”¨å¥½ï¼‰\n",
    "    # è´Ÿå€¼ï¼šå»¶æœŸè¿˜æ¬¾ï¼ˆä¿¡ç”¨å·®ï¼‰\n",
    "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    \n",
    "    # ==================== å€ºåŠ¡ç›¸å…³ç‰¹å¾ ====================\n",
    "    # 1. å€ºåŠ¡æ¯”ä¾‹\n",
    "    # æ€»ä¿¡è´·é¢åº¦ / å½“å‰å€ºåŠ¡ï¼Œæ¯”å€¼è¶Šå¤§è¯´æ˜è¿˜æ¬¾è¶Šå¤š\n",
    "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    \n",
    "    # 2. å·²è¿˜æ¬¾é‡‘é¢\n",
    "    # æ€»ä¿¡è´· - å½“å‰å€ºåŠ¡ = å·²è¿˜é‡‘é¢\n",
    "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    \n",
    "    # 3. ä¿¡è´·ä¸å¹´é‡‘æ¯”ç‡\n",
    "    # åæ˜ è¿˜æ¬¾è®¡åˆ’çš„åˆç†æ€§\n",
    "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']\n",
    "    \n",
    "    # 4. ä¿¡è´·æ—¶é—´å·®å¼‚ç‰¹å¾\n",
    "    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "    \n",
    "    # 5. å½“å‰å€ºåŠ¡å æ€»ä¿¡è´·æ¯”ä¾‹\n",
    "    # æ¯”ä¾‹è¶Šä½è¯´æ˜è¿˜å¾—è¶Šå¤šï¼Œä¿¡ç”¨è¶Šå¥½\n",
    "    bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / bureau['AMT_CREDIT_SUM']\n",
    "\n",
    "    # ==================== é€¾æœŸç‰¹å¾ï¼ˆDPD = Days Past Dueï¼‰====================\n",
    "    # DPDæ˜¯ä¿¡ç”¨è¯„åˆ†ä¸­æœ€é‡è¦çš„æŒ‡æ ‡ä¹‹ä¸€\n",
    "    \n",
    "    # 1. æ˜¯å¦æœ‰è¿‡é€¾æœŸ\n",
    "    # CREDIT_DAY_OVERDUE: æœ€å¤§é€¾æœŸå¤©æ•°\n",
    "    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # 2. æ˜¯å¦æœ‰ä¸¥é‡é€¾æœŸï¼ˆè¶…è¿‡120å¤©ï¼‰\n",
    "    # 120å¤©æ˜¯ä¸€ä¸ªå…³é”®é˜ˆå€¼ï¼Œé€šå¸¸è¢«è®¤ä¸ºæ˜¯ä¸¥é‡è¿çº¦\n",
    "    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n",
    "\n",
    "    # ==================== ç±»åˆ«ç‰¹å¾ç¼–ç  ====================\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # ==================== Bureau Balanceèšåˆ ====================\n",
    "    # bureau_balanceè¡¨è®°å½•äº†æ¯ç¬”ä¿¡è´·çš„æœˆåº¦çŠ¶æ€\n",
    "    # éœ€è¦å°†å¤šä¸ªæœˆçš„è®°å½•èšåˆåˆ°æ¯ç¬”ä¿¡è´·ä¸Š\n",
    "    \n",
    "    # å®šä¹‰èšåˆæ–¹å¼\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size', 'mean']}\n",
    "    # min: æœ€æ—©çš„æœˆä»½è®°å½•ï¼ˆä¿¡è´·å¼€å§‹æ—¶é—´ï¼‰\n",
    "    # max: æœ€è¿‘çš„æœˆä»½è®°å½•ï¼ˆå½“å‰çŠ¶æ€ï¼‰\n",
    "    # size: è®°å½•æ€»æ•°ï¼ˆä¿¡è´·æŒç»­æœˆæ•°ï¼‰\n",
    "    # mean: å¹³å‡æœˆä»½ï¼ˆæ—¶é—´è·¨åº¦ä¸­ç‚¹ï¼‰\n",
    "    \n",
    "    # å¯¹æ‰€æœ‰ç±»åˆ«ç‰¹å¾è®¡ç®—å¹³å‡å€¼\n",
    "    # ä¾‹å¦‚ï¼šSTATUS_C (Current/å½“å‰)çš„å‡å€¼åæ˜ äº†è¿˜æ¬¾çŠ¶æ€çš„ç¨³å®šæ€§\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "\n",
    "    # æŒ‰ä¿¡è´·IDèšåˆbureau_balanceæ•°æ®\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    \n",
    "    # å°†èšåˆåçš„æœˆåº¦ç»Ÿè®¡ä¿¡æ¯åˆå¹¶åˆ°bureauè¡¨\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "    # ==================== æŒ‰å®¢æˆ·æ±‡æ€»æ‰€æœ‰ä¿¡è´·è®°å½• ====================\n",
    "    # æ¯ä¸ªå®¢æˆ·å¯èƒ½åœ¨å¤šå®¶æœºæ„æœ‰å¤šç¬”ä¿¡è´·\n",
    "    # éœ€è¦å°†æ‰€æœ‰ä¿¡è´·è®°å½•èšåˆåˆ°å®¢æˆ·å±‚é¢\n",
    "    \n",
    "    # æ•°å€¼ç‰¹å¾çš„èšåˆæ–¹å¼\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],  # ä¿¡è´·å¼€å§‹æ—¶é—´çš„åˆ†å¸ƒ\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],  # ä¿¡è´·ç»“æŸæ—¶é—´\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],  # ä¿¡è´·ä¿¡æ¯æ›´æ–°æ—¶é—´\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean', 'min'],  # é€¾æœŸå¤©æ•°ï¼ˆæœ€ä¸¥é‡ã€å¹³å‡ã€æœ€è½»ï¼‰\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],  # æœ€å¤§é€¾æœŸé‡‘é¢\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],  # ä¿¡è´·æ€»é¢ï¼ˆæœ€å¤§å•ç¬”ã€å¹³å‡ã€ç´¯è®¡ï¼‰\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],  # å½“å‰å€ºåŠ¡\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean', 'max', 'sum'],  # é€¾æœŸé‡‘é¢\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],  # ä¿¡ç”¨é¢åº¦\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],  # å¹´é‡‘\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],  # å±•æœŸæ¬¡æ•°ï¼ˆå»¶é•¿è¿˜æ¬¾æœŸé™ï¼‰\n",
    "        'MONTHS_BALANCE_MIN': ['min'],  # æœ€æ—©è®°å½•æœˆä»½\n",
    "        'MONTHS_BALANCE_MAX': ['max'],  # æœ€è¿‘è®°å½•æœˆä»½\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],  # è®°å½•æœˆæ•°\n",
    "        'SK_ID_BUREAU': ['count'],  # ä¿¡è´·æ€»ç¬”æ•°\n",
    "        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],  # å®é™…ç»“æŸæ—¥æœŸ\n",
    "        'ENDDATE_DIF': ['min', 'max', 'mean'],  # æ—¥æœŸå·®å¼‚\n",
    "        'BUREAU_CREDIT_FACT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean'],  # å€ºåŠ¡æ¯”ç‡\n",
    "        'DEBT_CREDIT_DIFF': ['min', 'max', 'mean'],  # å·²è¿˜é‡‘é¢\n",
    "        'BUREAU_IS_DPD': ['mean', 'sum'],  # æœ‰é€¾æœŸçš„ä¿¡è´·å æ¯”å’Œæ•°é‡\n",
    "        'BUREAU_IS_DPD_OVER120': ['mean', 'sum']  # ä¸¥é‡é€¾æœŸçš„å æ¯”å’Œæ•°é‡\n",
    "        }\n",
    "\n",
    "    # ç±»åˆ«ç‰¹å¾çš„èšåˆæ–¹å¼ï¼ˆè®¡ç®—å¹³å‡å€¼ï¼‰\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    # æ‰§è¡Œèšåˆï¼Œå°†æ‰€æœ‰ä¿¡è´·è®°å½•æ±‡æ€»åˆ°å®¢æˆ·å±‚é¢\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    # ==================== æ´»è·ƒä¿¡è´·å•ç‹¬èšåˆ ====================\n",
    "    # æ´»è·ƒçš„ä¿¡è´·(Active)ä¸å†å²ä¿¡è´·çš„ç‰¹å¾å¯èƒ½å¾ˆä¸åŒ\n",
    "    # åˆ†åˆ«ç»Ÿè®¡å¯ä»¥æä¾›æ›´ç²¾ç»†çš„ä¿¡æ¯\n",
    "    \n",
    "    # ç­›é€‰æ´»è·ƒçŠ¶æ€çš„ä¿¡è´·\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # ==================== å·²å…³é—­ä¿¡è´·å•ç‹¬èšåˆ ====================\n",
    "    # å·²ç»“æ¸…çš„ä¿¡è´·èƒ½åæ˜ å®¢æˆ·çš„å†å²è¿˜æ¬¾èƒ½åŠ›\n",
    "    \n",
    "    # ç­›é€‰å·²å…³é—­çŠ¶æ€çš„ä¿¡è´·\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print('\"Bureau/Bureau Balance\" final shape:', bureau_agg.shape)\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-zealand",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.285317Z",
     "iopub.status.busy": "2021-05-04T15:38:40.283608Z",
     "iopub.status.idle": "2021-05-04T15:38:40.286025Z",
     "shell.execute_reply": "2021-05-04T15:38:40.286499Z"
    },
    "papermill": {
     "duration": 0.035905,
     "end_time": "2021-05-04T15:38:40.286660",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.250755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€å†å²ç”³è¯·æ•°æ®å¤„ç†å‡½æ•°ã€‘\n",
    "å¤„ç†å®¢æˆ·åœ¨Home Creditçš„å†å²ç”³è¯·è®°å½•\n",
    "\"\"\"\n",
    "def previous_application():\n",
    "    \"\"\"\n",
    "    å†å²ç”³è¯·æ•°æ®è¯´æ˜ï¼š\n",
    "    - åŒ…å«å®¢æˆ·è¿‡å»æ‰€æœ‰çš„è´·æ¬¾ç”³è¯·è®°å½•\n",
    "    - å¯èƒ½åŒ…æ‹¬ï¼šå·²æ‰¹å‡†ã€å·²æ‹’ç»ã€å·²å–æ¶ˆã€æœªä½¿ç”¨çš„ç”³è¯·\n",
    "    \n",
    "    å…³é”®ç‰¹å¾ï¼š\n",
    "    1. ç”³è¯·é‡‘é¢ vs å®é™…æ‰¹å‡†é‡‘é¢çš„æ¯”ç‡ï¼ˆè®®ä»·èƒ½åŠ›ï¼‰\n",
    "    2. å†å²æ‹’ç»ç‡ï¼ˆé£é™©ä¿¡å·ï¼‰\n",
    "    3. é¦–ä»˜æ¯”ä¾‹ï¼ˆè´¢åŠ¡å®åŠ›ï¼‰\n",
    "    4. ç®€å•åˆ©ç‡ï¼ˆè¿˜æ¬¾æˆæœ¬ï¼‰\n",
    "    \"\"\"\n",
    "    prev = pd.read_csv(r'../input/home-credit-default-risk/previous_application.csv')\n",
    "\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Feature engineering: ratios and difference\n",
    "    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\n",
    "    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Interest ratio on previous application (simplified)\n",
    "    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    prev['SIMPLE_INTERESTS'] = (total_payment / prev['AMT_CREDIT'] - 1) / prev['CNT_PAYMENT']\n",
    "\n",
    "    # Days last due difference (scheduled x done)\n",
    "    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "\n",
    "    # from off\n",
    "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
    "    prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
    "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE'] / prev['AMT_APPLICATION']\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'DAYS_TERMINATION': ['max'],\n",
    "        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n",
    "        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n",
    "        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n",
    "        'SIMPLE_INTERESTS': ['mean', 'max']\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print('\"Previous Applications\" final shape:', prev_agg.shape)\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-somerset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.321393Z",
     "iopub.status.busy": "2021-05-04T15:38:40.320660Z",
     "iopub.status.idle": "2021-05-04T15:38:40.333793Z",
     "shell.execute_reply": "2021-05-04T15:38:40.334289Z"
    },
    "papermill": {
     "duration": 0.035205,
     "end_time": "2021-05-04T15:38:40.334464",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.299259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€POSåˆ†æœŸä»˜æ¬¾æ•°æ®å¤„ç†å‡½æ•°ã€‘\n",
    "å¤„ç†é”€å”®ç‚¹(Point of Sale)åˆ†æœŸä»˜æ¬¾è®°å½•\n",
    "\"\"\"\n",
    "def pos_cash():\n",
    "    \"\"\"\n",
    "    POS_CASHæ•°æ®è¯´æ˜ï¼š\n",
    "    - è®°å½•å®¢æˆ·åœ¨å•†åº—çš„åˆ†æœŸä»˜æ¬¾è®°å½•ï¼ˆå¦‚è´­ä¹°æ‰‹æœºã€å®¶ç”µç­‰ï¼‰\n",
    "    - åŒ…å«æœˆåº¦ä½™é¢å’Œè¿˜æ¬¾çŠ¶æ€\n",
    "    \n",
    "    å…³é”®ç‰¹å¾ï¼š\n",
    "    1. DPD (Days Past Due): é€¾æœŸå¤©æ•°ç»Ÿè®¡\n",
    "    2. æ˜¯å¦æå‰è¿˜æ¸…ï¼ˆä¿¡ç”¨å¥½çš„ä¿¡å·ï¼‰\n",
    "    3. å‰©ä½™åˆ†æœŸæ•°å’Œæ¯”ä¾‹ï¼ˆå½“å‰è´Ÿå€ºæƒ…å†µï¼‰\n",
    "    4. æœ€è¿‘3æ¬¡ç”³è¯·çš„é€¾æœŸæƒ…å†µï¼ˆæœ€æ–°è¡Œä¸ºæ¨¡å¼ï¼‰\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ\n",
    "    - å°é¢åˆ†æœŸä¹Ÿèƒ½åæ˜ è¿˜æ¬¾ä¹ æƒ¯\n",
    "    - é€¾æœŸè¡Œä¸ºæ˜¯è¿çº¦çš„å¼ºé¢„æµ‹å› å­\n",
    "    \"\"\"\n",
    "    pos = pd.read_csv(r'../input/home-credit-default-risk/POS_CASH_balance.csv')\n",
    "\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Flag months with late payment\n",
    "    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) # <-- same with ['LATE_PAYMENT']\n",
    "    pos['POS_IS_DPD_UNDER_120'] = pos['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    pos['POS_IS_DPD_OVER_120'] = pos['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n",
    "        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n",
    "        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'LATE_PAYMENT': ['mean'],\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n",
    "        'POS_IS_DPD': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.groupby('SK_ID_PREV')\n",
    "    df_pos = pd.DataFrame()\n",
    "    df_pos['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n",
    "    df_pos['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n",
    "\n",
    "    # Percentage of previous loans completed and completed before initial term\n",
    "    df_pos['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = df_pos.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 \\\n",
    "                                                                      and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n",
    "    # Number of remaining installments (future installments) and percentage from total\n",
    "    df_pos['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n",
    "    df_pos['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n",
    "\n",
    "    # Group by SK_ID_CURR and merge\n",
    "    df_gp = df_pos.groupby('SK_ID_CURR').sum().reset_index()\n",
    "    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n",
    "    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n",
    "\n",
    "    # Percentage of late payments for the 3 most recent applications\n",
    "    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n",
    "\n",
    "    # Last month of each application\n",
    "    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "\n",
    "    # Most recent applications (last 3)\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Pos-Cash\" balance final shape:', pos_agg.shape) \n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-reference",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.363616Z",
     "iopub.status.busy": "2021-05-04T15:38:40.362967Z",
     "iopub.status.idle": "2021-05-04T15:38:40.386396Z",
     "shell.execute_reply": "2021-05-04T15:38:40.386982Z"
    },
    "papermill": {
     "duration": 0.039716,
     "end_time": "2021-05-04T15:38:40.387171",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.347455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€åˆ†æœŸè¿˜æ¬¾å†å²å¤„ç†å‡½æ•°ã€‘\n",
    "å¤„ç†å®¢æˆ·å†å²è´·æ¬¾çš„æ¯æœŸè¿˜æ¬¾è®°å½•\n",
    "\"\"\"\n",
    "def installment():\n",
    "    \"\"\"\n",
    "    Installmentsæ•°æ®è¯´æ˜ï¼š\n",
    "    - è®°å½•æ¯ç¬”è´·æ¬¾çš„æ¯æœŸè¿˜æ¬¾è¯¦æƒ…\n",
    "    - åŒ…å«åº”è¿˜é‡‘é¢ã€å®é™…è¿˜æ¬¾é‡‘é¢ã€è¿˜æ¬¾æ—¥æœŸç­‰\n",
    "    \n",
    "    æ ¸å¿ƒç‰¹å¾å·¥ç¨‹ï¼š\n",
    "    1. DPD (Days Past Due): é€¾æœŸå¤©æ•°\n",
    "       - DPD=0: æŒ‰æ—¶è¿˜æ¬¾\n",
    "       - DPD>0: é€¾æœŸå¤©æ•°\n",
    "       - DPD>120: ä¸¥é‡é€¾æœŸ\n",
    "    \n",
    "    2. DBD (Days Before Due): æå‰è¿˜æ¬¾å¤©æ•°\n",
    "       - æå‰è¿˜æ¬¾æ˜¯ä¿¡ç”¨å¥½çš„ä¿¡å·\n",
    "    \n",
    "    3. Payment Ratio: å®é™…è¿˜æ¬¾/åº”è¿˜é‡‘é¢\n",
    "       - >1: è¿˜å¤šäº†ï¼ˆä¿¡ç”¨å¥½ï¼‰\n",
    "       - <1: è¿˜å°‘äº†ï¼ˆé€¾æœŸï¼‰\n",
    "    \n",
    "    4. æœ€è¿‘365å¤©çš„è¡Œä¸ºæ¨¡å¼\n",
    "       - æœ€è¿‘è¡Œä¸ºæ¯”å†å²è¡Œä¸ºæ›´é‡è¦\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆæœ€é‡è¦ï¼Ÿ\n",
    "    - ç›´æ¥åæ˜ è¿˜æ¬¾è¡Œä¸ºï¼Œæ˜¯æœ€å¼ºçš„è¿çº¦é¢„æµ‹å› å­\n",
    "    - é€¾æœŸé¢‘ç‡ã€ä¸¥é‡ç¨‹åº¦éƒ½æœ‰åŒºåˆ†åº¦\n",
    "    \"\"\"\n",
    "    ins = pd.read_csv(r'../input/home-credit-default-risk/installments_payments.csv')\n",
    "\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Group payments and get Payment difference\n",
    "    ins = do_sum(ins, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n",
    "    ins['PAYMENT_DIFFERENCE'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAYMENT_RATIO'] = ins['AMT_INSTALMENT'] / ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAID_OVER_AMOUNT'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT']\n",
    "    ins['PAID_OVER'] = (ins['PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD_diff'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD_diff'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Flag late payment\n",
    "    ins['LATE_PAYMENT'] = ins['DBD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    ins['INSTALMENT_PAYMENT_RATIO'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['LATE_PAYMENT_RATIO'] = ins.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "\n",
    "    # Flag late payments that have a significant amount\n",
    "    ins['SIGNIFICANT_LATE_PAYMENT'] = ins['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    \n",
    "    # Flag k threshold late payments\n",
    "    ins['DPD_7'] = ins['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "    ins['DPD_15'] = ins['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n",
    "\n",
    "    ins['INS_IS_DPD_UNDER_120'] = ins['DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    ins['INS_IS_DPD_OVER_120'] = ins['DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum', 'var'],\n",
    "        'DBD': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'SK_ID_PREV': ['size', 'nunique'],\n",
    "        'PAYMENT_DIFFERENCE': ['mean'],\n",
    "        'PAYMENT_RATIO': ['mean', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'LATE_PAYMENT_RATIO': ['mean'],\n",
    "        'DPD_7': ['mean'],\n",
    "        'DPD_15': ['mean'],\n",
    "        'PAID_OVER': ['mean'],\n",
    "        'DPD_diff':['mean', 'min', 'max'],\n",
    "        'DBD_diff':['mean', 'min', 'max'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # from oof (DAYS_ENTRY_PAYMENT)\n",
    "    cond_day = ins['DAYS_ENTRY_PAYMENT'] >= -365\n",
    "    ins_d365_grp = ins[cond_day].groupby('SK_ID_CURR')\n",
    "    ins_d365_agg_dict = {\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'PAYMENT_DIFF': ['mean', 'min', 'max', 'sum'],\n",
    "        'PAYMENT_PERC': ['mean', 'max'],\n",
    "        'DPD_diff': ['mean', 'min', 'max'],\n",
    "        'DPD': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']}\n",
    "\n",
    "    ins_d365_agg = ins_d365_grp.agg(ins_d365_agg_dict)\n",
    "    ins_d365_agg.columns = ['INS_D365' + ('_').join(column).upper() for column in ins_d365_agg.columns.ravel()]\n",
    "\n",
    "    ins_agg = ins_agg.merge(ins_d365_agg, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Installments Payments\" final shape:', ins_agg.shape)\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-colleague",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.417741Z",
     "iopub.status.busy": "2021-05-04T15:38:40.417044Z",
     "iopub.status.idle": "2021-05-04T15:38:40.433026Z",
     "shell.execute_reply": "2021-05-04T15:38:40.432335Z"
    },
    "papermill": {
     "duration": 0.032819,
     "end_time": "2021-05-04T15:38:40.433176",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.400357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€ä¿¡ç”¨å¡ä½™é¢æ•°æ®å¤„ç†å‡½æ•°ã€‘\n",
    "å¤„ç†å®¢æˆ·ä¿¡ç”¨å¡çš„æœˆåº¦ä½™é¢å’Œä½¿ç”¨æƒ…å†µ\n",
    "\"\"\"\n",
    "def credit_card():\n",
    "    \"\"\"\n",
    "    ä¿¡ç”¨å¡æ•°æ®è¯´æ˜ï¼š\n",
    "    - è®°å½•å®¢æˆ·ä¿¡ç”¨å¡çš„æœˆåº¦ä½¿ç”¨å’Œè¿˜æ¬¾æƒ…å†µ\n",
    "    - åŒ…å«ä½™é¢ã€å–ç°ã€æœ€ä½è¿˜æ¬¾é¢ç­‰ä¿¡æ¯\n",
    "    \n",
    "    æ ¸å¿ƒç‰¹å¾å·¥ç¨‹ï¼š\n",
    "    1. ä¿¡ç”¨é¢åº¦ä½¿ç”¨ç‡ (LIMIT_USE)\n",
    "       - ä½¿ç”¨ç‡é«˜è¡¨ç¤ºèµ„é‡‘ç´§å¼ ï¼Œè¿çº¦é£é™©é«˜\n",
    "       - ä¾‹å¦‚ï¼šé¢åº¦1ä¸‡ï¼Œç”¨äº†9åƒï¼Œä½¿ç”¨ç‡=90%ï¼ˆé«˜é£é™©ï¼‰\n",
    "    \n",
    "    2. è¿˜æ¬¾å……è¶³æ€§ (PAYMENT_DIV_MIN)\n",
    "       - å®é™…è¿˜æ¬¾/æœ€ä½è¿˜æ¬¾\n",
    "       - >1: è¿˜å¾—æ¯”æœ€ä½å¤šï¼ˆè‰¯å¥½ï¼‰\n",
    "       - â‰ˆ1: åªè¿˜æœ€ä½è¿˜æ¬¾ï¼ˆè­¦å‘Šä¿¡å·ï¼‰\n",
    "       - <1: è¿æœ€ä½éƒ½æ²¡è¿˜å¤Ÿï¼ˆé«˜é£é™©ï¼‰\n",
    "    \n",
    "    3. å–ç°è¡Œä¸º (DRAWING_LIMIT_RATIO)\n",
    "       - ä¿¡ç”¨å¡å–ç°é€šå¸¸åˆ©æ¯å¾ˆé«˜\n",
    "       - é¢‘ç¹å–ç°è¯´æ˜ç°é‡‘æµç´§å¼ \n",
    "    \n",
    "    4. é€¾æœŸæ ‡è®°\n",
    "       - ä¸€èˆ¬é€¾æœŸ (0-120å¤©)\n",
    "       - ä¸¥é‡é€¾æœŸ (>120å¤©)\n",
    "    \n",
    "    5. æ—¶é—´çª—å£ç‰¹å¾ (12/24/48ä¸ªæœˆ)\n",
    "       - æœ€è¿‘è¡Œä¸ºæ¯”å†å²è¡Œä¸ºæ›´æœ‰é¢„æµ‹åŠ›\n",
    "       - åˆ†åˆ«ç»Ÿè®¡ä¸åŒæ—¶é—´æ®µçš„è¡Œä¸ºæ¨¡å¼\n",
    "    \"\"\"\n",
    "    cc = pd.read_csv(r'../input/home-credit-default-risk/credit_card_balance.csv')\n",
    "\n",
    "    # ç±»åˆ«ç‰¹å¾ç¼–ç \n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # ==================== æ ¸å¿ƒè¡ç”Ÿç‰¹å¾ ====================\n",
    "    # 1. ä¿¡ç”¨é¢åº¦ä½¿ç”¨ç‡ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰\n",
    "    # å½“å‰ä½™é¢/ä¿¡ç”¨é¢åº¦ï¼Œåæ˜ ä¿¡ç”¨å¡ä½¿ç”¨ç¨‹åº¦\n",
    "    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    \n",
    "    # 2. è¿˜æ¬¾å……è¶³æ€§\n",
    "    # å®é™…è¿˜æ¬¾é‡‘é¢/æœ€ä½è¿˜æ¬¾é¢ï¼Œ>1è¡¨ç¤ºè¿˜å¾—æ¯”æœ€ä½å¤š\n",
    "    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n",
    "    \n",
    "    # 3. æ˜¯å¦é€¾æœŸæ ‡è®°\n",
    "    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # 4. å–ç°å é¢åº¦æ¯”ä¾‹\n",
    "    # ATMå–ç°/ä¿¡ç”¨é¢åº¦ï¼Œå–ç°å¤šè¯´æ˜ç¼ºç°é‡‘\n",
    "    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "    # 5. é€¾æœŸç¨‹åº¦åˆ†ç±»\n",
    "    # è½»åº¦é€¾æœŸï¼ˆ1-119å¤©ï¼‰\n",
    "    cc['CARD_IS_DPD_UNDER_120'] = cc['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    # ä¸¥é‡é€¾æœŸï¼ˆâ‰¥120å¤©ï¼‰\n",
    "    cc['CARD_IS_DPD_OVER_120'] = cc['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # ==================== æŒ‰å®¢æˆ·èšåˆæ‰€æœ‰ä¿¡ç”¨å¡è®°å½• ====================\n",
    "    # å¯¹æ‰€æœ‰æ•°å€¼ç‰¹å¾è¿›è¡Œå¤šç§ç»Ÿè®¡èšåˆ\n",
    "    # min/max: æå€¼ï¼ˆæœ€å¥½/æœ€å·®æƒ…å†µï¼‰\n",
    "    # mean: å¹³å‡æ°´å¹³\n",
    "    # sum: ç´¯è®¡å€¼\n",
    "    # var: æ³¢åŠ¨æ€§ï¼ˆæ–¹å·®å¤§è¯´æ˜ä½¿ç”¨ä¸ç¨³å®šï¼‰\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    # ç»Ÿè®¡æ¯ä¸ªå®¢æˆ·çš„ä¿¡ç”¨å¡æ€»æ•°\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # ==================== æœ€è¿‘ä¸€ä¸ªæœˆçš„ä¿¡ç”¨å¡çŠ¶æ€ ====================\n",
    "    # åŸç†ï¼šæœ€è¿‘çš„è¡Œä¸ºæ¯”å†å²å¹³å‡æ›´é‡è¦\n",
    "    # æ‰¾åˆ°æ¯å¼ ä¿¡ç”¨å¡æœ€è¿‘ä¸€ä¸ªæœˆçš„è®°å½•\n",
    "    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    last_months_df = cc[cc.index.isin(last_ids)]\n",
    "    \n",
    "    # è®¡ç®—æœ€è¿‘ä¸€ä¸ªæœˆçš„ä½™é¢ç»Ÿè®¡\n",
    "    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n",
    "\n",
    "    # ==================== æ—¶é—´çª—å£ç‰¹å¾ï¼ˆé‡è¦æŠ€æœ¯ï¼‰====================\n",
    "    # æ ¸å¿ƒæ€æƒ³ï¼šå®¢æˆ·è¡Œä¸ºä¼šéšæ—¶é—´å˜åŒ–\n",
    "    # - æœ€è¿‘12ä¸ªæœˆï¼šåæ˜ å½“å‰çŠ¶æ€\n",
    "    # - æœ€è¿‘24ä¸ªæœˆï¼šåæ˜ ä¸­æœŸè¶‹åŠ¿\n",
    "    # - æœ€è¿‘48ä¸ªæœˆï¼šåæ˜ é•¿æœŸè¡Œä¸º\n",
    "    # \n",
    "    # ä¸¾ä¾‹è¯´æ˜ï¼š\n",
    "    # å¦‚æœå®¢æˆ·ï¼š\n",
    "    # - 48ä¸ªæœˆå¹³å‡é¢åº¦ä½¿ç”¨ç‡: 30%ï¼ˆå†å²è‰¯å¥½ï¼‰\n",
    "    # - 12ä¸ªæœˆå¹³å‡é¢åº¦ä½¿ç”¨ç‡: 80%ï¼ˆæœ€è¿‘æ¶åŒ–ï¼‰\n",
    "    # -> è¯´æ˜è´¢åŠ¡çŠ¶å†µåœ¨æ¶åŒ–ï¼Œè¿çº¦é£é™©ä¸Šå‡ï¼\n",
    "    \n",
    "    # å®šä¹‰éœ€è¦æŒ‰æ—¶é—´çª—å£ç»Ÿè®¡çš„ç‰¹å¾\n",
    "    CREDIT_CARD_TIME_AGG = {\n",
    "        'AMT_BALANCE': ['mean', 'max'],                    # ä½™é¢ç»Ÿè®¡\n",
    "        'LIMIT_USE': ['max', 'mean'],                      # é¢åº¦ä½¿ç”¨ç‡\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':['max'],                 # ä¿¡ç”¨é¢åº¦\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],        # ATMå–ç°\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],            # æ€»å–ç°\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],        # POSå–ç°\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],        # æœ€ä½è¿˜æ¬¾é¢\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],        # æ€»è¿˜æ¬¾é¢\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],           # åº”æ”¶æ€»é¢\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum', 'mean'], # ATMå–ç°æ¬¡æ•°\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],    # æ€»å–ç°æ¬¡æ•°\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['mean'],              # POSå–ç°æ¬¡æ•°\n",
    "        'SK_DPD': ['mean', 'max', 'sum'],                  # é€¾æœŸå¤©æ•°\n",
    "        'LIMIT_USE': ['min', 'max'],                       # é¢åº¦ä½¿ç”¨ç‡ï¼ˆé‡å¤ç”¨äºå¼ºè°ƒï¼‰\n",
    "        'DRAWING_LIMIT_RATIO': ['min', 'max'],             # å–ç°æ¯”ä¾‹\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],                   # é€¾æœŸæ ‡è®°\n",
    "        'CARD_IS_DPD_UNDER_120': ['mean', 'sum'],          # è½»åº¦é€¾æœŸ\n",
    "        'CARD_IS_DPD_OVER_120': ['mean', 'sum']            # ä¸¥é‡é€¾æœŸ\n",
    "    }\n",
    "\n",
    "    # å¾ªç¯åˆ›å»º12ä¸ªæœˆã€24ä¸ªæœˆã€48ä¸ªæœˆçš„æ—¶é—´çª—å£ç‰¹å¾\n",
    "    for months in [12, 24, 48]:\n",
    "        # ç­›é€‰æœ€è¿‘Nä¸ªæœˆæœ‰è®°å½•çš„ä¿¡ç”¨å¡\n",
    "        # MONTHS_BALANCEæ˜¯è´Ÿæ•°ï¼Œ-12è¡¨ç¤ºæœ€è¿‘12ä¸ªæœˆ\n",
    "        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n",
    "        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n",
    "        \n",
    "        # åˆ›å»ºç‰¹å¾å‰ç¼€ï¼Œä¾‹å¦‚ï¼šINS_12M_ï¼ˆInstallment 12 Monthsï¼‰\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        \n",
    "        # èšåˆå¹¶åˆå¹¶åˆ°ä¸»è¡¨\n",
    "        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n",
    "\n",
    "\n",
    "    print('\"Credit Card Balance\" final shape:', cc_agg.shape)\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-conservative",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.471173Z",
     "iopub.status.busy": "2021-05-04T15:38:40.470451Z",
     "iopub.status.idle": "2021-05-04T15:38:40.473500Z",
     "shell.execute_reply": "2021-05-04T15:38:40.472821Z"
    },
    "papermill": {
     "duration": 0.02737,
     "end_time": "2021-05-04T15:38:40.473642",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.446272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€æ•°æ®åå¤„ç†å‡½æ•°ã€‘\n",
    "å¯¹åˆå¹¶åçš„å®Œæ•´æ•°æ®é›†è¿›è¡Œæœ€åçš„å¤„ç†å’Œä¼˜åŒ–\n",
    "\"\"\"\n",
    "def data_post_processing(dataframe):\n",
    "    \"\"\"\n",
    "    æ•°æ®åå¤„ç†æµç¨‹ï¼š\n",
    "    1. ç‰¹å¾åç§°æ ‡å‡†åŒ–\n",
    "    2. å†…å­˜ä¼˜åŒ–\n",
    "    3. åˆ é™¤æ— ä¿¡æ¯ç‰¹å¾\n",
    "    4. LightGBMç‰¹å¾é€‰æ‹©\n",
    "    5. é£é™©åˆ†ç»„ç¼–ç \n",
    "    \n",
    "    è¿™æ˜¯ç‰¹å¾å·¥ç¨‹çš„æœ€åä¸€æ­¥ï¼Œç¡®ä¿æ•°æ®é›†é«˜è´¨é‡ä¸”é«˜æ•ˆ\n",
    "    \"\"\"\n",
    "    print(f'---=> the DATA POST-PROCESSING is beginning, the dataset has {dataframe.shape[1]} features')\n",
    "    \n",
    "    # ä¿å­˜ç´¢å¼•ç›¸å…³åˆ—åï¼ˆè¿™äº›åˆ—ä¸å‚ä¸æ¨¡å‹è®­ç»ƒï¼‰\n",
    "    index_cols = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']\n",
    "\n",
    "    # ==================== æ­¥éª¤1: ç‰¹å¾åç§°æ ‡å‡†åŒ– ====================\n",
    "    # å°†æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œç¡®ä¿ç‰¹å¾åç¬¦åˆè§„èŒƒ\n",
    "    # ä¾‹å¦‚ï¼šAMT_CREDIT-SUM -> AMT_CREDIT_SUM\n",
    "    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n",
    "    print('names of feature are renamed')\n",
    "\n",
    "    # ==================== æ­¥éª¤2: å†…å­˜ä¼˜åŒ– ====================\n",
    "    # é€šè¿‡é™ä½æ•°æ®ç±»å‹ç²¾åº¦æ¥å‡å°‘å†…å­˜å ç”¨\n",
    "    # è¿™å¯¹äºKaggleçš„16GBå†…å­˜é™åˆ¶è‡³å…³é‡è¦\n",
    "    dataframe = reduce_mem_usage(dataframe)\n",
    "    print(f'---=> pandas data types of features in the dataset are converted for a reduced memory usage')\n",
    "\n",
    "    # ==================== æ­¥éª¤3: åˆ é™¤æ— ä¿¡æ¯ç‰¹å¾ ====================\n",
    "    # å¦‚æœä¸€ä¸ªç‰¹å¾åªæœ‰ä¸€ä¸ªå–å€¼ï¼ˆæˆ–å…¨æ˜¯ç¼ºå¤±å€¼ï¼‰ï¼Œå®ƒå¯¹æ¨¡å‹æ²¡æœ‰ä»»ä½•å¸®åŠ©\n",
    "    # ä¾‹å¦‚ï¼šæŸåˆ—å…¨æ˜¯1ï¼Œæˆ–å…¨æ˜¯NaN\n",
    "    noninformative_cols = []\n",
    "    for col in dataframe.columns:\n",
    "        # ç»Ÿè®¡è¯¥åˆ—çš„ä¸åŒå–å€¼æ•°é‡\n",
    "        if len(dataframe[col].value_counts()) < 2:\n",
    "            noninformative_cols.append(col)\n",
    "\n",
    "    dataframe.drop(noninformative_cols, axis=1, inplace=True)\n",
    "    print(f'---=> {dataframe.shape[1]} features are remained after removing non-informative features')\n",
    "\n",
    "    # ==================== æ­¥éª¤4: LightGBMç‰¹å¾é€‰æ‹© ====================\n",
    "    # ä½¿ç”¨é¢„è®­ç»ƒçš„LightGBMæ¨¡å‹ç­›é€‰å‡ºçš„é‡è¦ç‰¹å¾\n",
    "    feature_num = dataframe.shape[1]\n",
    "    \n",
    "    # æ³¨æ„ï¼šåŸæœ¬åº”è¯¥è°ƒç”¨ligthgbm_feature_selectionå‡½æ•°\n",
    "    # ä½†ç”±äºå†…å­˜é™åˆ¶ï¼Œè¿™é‡Œè¯»å–é¢„å…ˆè®¡ç®—å¥½çš„ç»“æœ\n",
    "    auc_limit = 0.7\n",
    "    # dataframe = ligthgbm_feature_selection(dataframe, index_cols, auc_limit=auc_limit)\n",
    "    \n",
    "    # è¯»å–éœ€è¦åˆ é™¤çš„ç‰¹å¾åˆ—è¡¨\n",
    "    all_features = dataframe.columns.tolist()\n",
    "    selected_feature_df = pd.read_csv('../input/homecredit-best-subs/removed_cols_lgbm.csv')\n",
    "    selected_features = selected_feature_df.removed_cols.tolist()\n",
    "    \n",
    "    # ä¿ç•™æœ‰ç”¨çš„ç‰¹å¾\n",
    "    remained_features = set(all_features).difference(set(selected_features))\n",
    "    dataframe = dataframe[remained_features]\n",
    "    print(f'{feature_num - dataframe.shape[1]} features are eliminated by LightGBM classifier with an {auc_limit} auc score limit in step I')\n",
    "    print(f'---=> {dataframe.shape[1]} features are remained after removing features not interesting for LightGBM classifier')\n",
    "\n",
    "\n",
    "    # ==================== æ­¥éª¤5: é£é™©åˆ†ç»„ç¼–ç  ====================\n",
    "    # å¯¹å‰©ä½™çš„ç±»åˆ«ç‰¹å¾åº”ç”¨é£é™©åˆ†ç»„æŠ€æœ¯\n",
    "    # è¿™æ˜¯æœ€åä¸€æ¬¡ç‰¹å¾å·¥ç¨‹ï¼Œå°†ç±»åˆ«è½¬æ¢ä¸ºé£é™©æ ‡è®°\n",
    "    start_feats_num = dataframe.shape[1]\n",
    "    \n",
    "    # é€‰æ‹©åˆé€‚çš„ç±»åˆ«ç‰¹å¾ï¼š\n",
    "    # - ç±»åˆ«æ•°åœ¨3-20ä¹‹é—´ï¼ˆå¤ªå°‘æ— æ„ä¹‰ï¼Œå¤ªå¤šä¼šçˆ†ç‚¸ï¼‰\n",
    "    # - ä¸æ˜¯ç´¢å¼•åˆ—\n",
    "    cat_cols = [col for col in dataframe.columns if 3 < len(dataframe[col].value_counts()) < 20 and col not in index_cols]\n",
    "    \n",
    "    # åº”ç”¨é£é™©åˆ†ç»„ï¼Œé˜ˆå€¼è®¾ä¸º8.1%ï¼ˆæ¥è¿‘å¹³å‡è¿çº¦ç‡8.2%ï¼‰\n",
    "    dataframe, _ = risk_groupanizer(dataframe, column_names=cat_cols, upper_limit_ratio=8.1, lower_limit_ratio=8.1)\n",
    "    print(f'---=> {dataframe.shape[1] - start_feats_num} features are generated with the risk_groupanizer')\n",
    "\n",
    "\n",
    "    # ==================== å¤„ç†å®Œæˆ ====================\n",
    "    print(f'---=> the DATA POST-PROCESSING is ended!, now the dataset has a total {dataframe.shape[1]} features')\n",
    "\n",
    "    # æ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜\n",
    "    gc.collect()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-graduation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.518255Z",
     "iopub.status.busy": "2021-05-04T15:38:40.517479Z",
     "iopub.status.idle": "2021-05-04T15:38:40.520435Z",
     "shell.execute_reply": "2021-05-04T15:38:40.519963Z"
    },
    "papermill": {
     "duration": 0.03368,
     "end_time": "2021-05-04T15:38:40.520598",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.486918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ã€KæŠ˜äº¤å‰éªŒè¯ + LightGBMè®­ç»ƒå‡½æ•°ã€‘\n",
    "æœ¬é¡¹ç›®çš„æ ¸å¿ƒï¼šä½¿ç”¨ä¼ªæ ‡ç­¾æŠ€æœ¯å’ŒKæŠ˜äº¤å‰éªŒè¯è®­ç»ƒæœ€ç»ˆæ¨¡å‹\n",
    "\"\"\"\n",
    "def Kfold_LightGBM(df):\n",
    "    \"\"\"\n",
    "    æ ¸å¿ƒåˆ›æ–°ï¼šä¼ªæ ‡ç­¾ï¼ˆPseudo-Labelingï¼‰æŠ€æœ¯\n",
    "    \n",
    "    ä»€ä¹ˆæ˜¯ä¼ªæ ‡ç­¾ï¼Ÿ\n",
    "    1. å…ˆç”¨è®­ç»ƒé›†è®­ç»ƒä¸€ä¸ªåˆæ­¥æ¨¡å‹\n",
    "    2. ç”¨è¿™ä¸ªæ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹\n",
    "    3. å°†æµ‹è¯•é›†çš„é¢„æµ‹ç»“æœä½œä¸º\"ä¼ªæ ‡ç­¾\"\n",
    "    4. æŠŠå¸¦ä¼ªæ ‡ç­¾çš„æµ‹è¯•é›†åŠ å…¥è®­ç»ƒé›†\n",
    "    5. ç”¨æ‰©å¤§åçš„è®­ç»ƒé›†é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ\n",
    "    - å¢åŠ äº†è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆä»30ä¸‡å¢åŠ åˆ°50ä¸‡+ï¼‰\n",
    "    - æµ‹è¯•é›†çš„åˆ†å¸ƒä¿¡æ¯è¢«åˆ©ç”¨ï¼ˆåŠç›‘ç£å­¦ä¹ ï¼‰\n",
    "    - é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼ˆ>0.75ï¼‰æ¥è¿‘çœŸå®æ ‡ç­¾\n",
    "    \n",
    "    é£é™©ä¸ç¼“è§£ï¼š\n",
    "    - é£é™©ï¼šé”™è¯¯çš„ä¼ªæ ‡ç­¾ä¼šè¯¯å¯¼æ¨¡å‹\n",
    "    - ç¼“è§£ï¼šåªä½¿ç”¨é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆ>0.75ï¼‰\n",
    "    - ç¼“è§£ï¼šé‡å¤æ·»åŠ 3æ¬¡ä»¥å¢å¼ºä¿¡å·\n",
    "    \"\"\"\n",
    "    print('===============================================', '\\n', '##### the ML in processing...')\n",
    "\n",
    "    # ==================== åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„é¢„æµ‹ç»“æœ ====================\n",
    "    # è¿™äº›æ˜¯ç”¨å…¶ä»–æ¨¡å‹å¯¹æµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ\n",
    "    # æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾\n",
    "    df_subx = pd.read_csv(r'../input/homecredit-best-subs/df_subs_3.csv')\n",
    "    df_sub = df_subx[['SK_ID_CURR', '23']]\n",
    "    df_sub.columns = ['SK_ID_CURR', 'TARGET']\n",
    "\n",
    "    # ==================== åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›† ====================\n",
    "    # è®­ç»ƒé›†ï¼šæœ‰çœŸå®TARGETæ ‡ç­¾\n",
    "    # æµ‹è¯•é›†ï¼šTARGETä¸ºNaNï¼Œéœ€è¦é¢„æµ‹\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    \n",
    "    # åˆ é™¤åŸå§‹DataFrameé‡Šæ”¾å†…å­˜\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # ==================== ä¼ªæ ‡ç­¾æŠ€æœ¯å®ç° ====================\n",
    "    # æ­¥éª¤1ï¼šå°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºä¼ªæ ‡ç­¾\n",
    "    # è§„åˆ™ï¼šé¢„æµ‹æ¦‚ç‡>0.75çš„æ ‡è®°ä¸ºè¿çº¦(1)ï¼Œå¦åˆ™ä¸ºæ­£å¸¸(0)\n",
    "    # 0.75æ˜¯ä¸€ä¸ªé«˜ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œåªæœ‰å¾ˆç¡®å®šçš„æ‰æ ‡è®°ä¸ºè¿çº¦\n",
    "    test_df.TARGET = np.where(df_sub.TARGET > 0.75, 1, 0)\n",
    "    \n",
    "    # æ­¥éª¤2ï¼šå°†å¸¦ä¼ªæ ‡ç­¾çš„æµ‹è¯•é›†åŠ å…¥è®­ç»ƒé›†\n",
    "    # é‡è¦ï¼šè¿™é‡Œé‡å¤æ·»åŠ äº†3æ¬¡ï¼\n",
    "    # ä¸ºä»€ä¹ˆé‡å¤3æ¬¡ï¼Ÿ\n",
    "    # - åŸå§‹è®­ç»ƒé›†çº¦30ä¸‡ï¼Œæµ‹è¯•é›†çº¦5ä¸‡\n",
    "    # - åŠ 3æ¬¡åæ¯”ä¾‹å˜ä¸º 30:15ï¼Œå¢å¼ºä¼ªæ ‡ç­¾çš„å½±å“\n",
    "    # - ä½†ä¸èƒ½åŠ å¤ªå¤šæ¬¡ï¼Œå¦åˆ™é”™è¯¯ä¼ªæ ‡ç­¾å½±å“è¿‡å¤§\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)  # ç¬¬1æ¬¡\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)  # ç¬¬2æ¬¡\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)  # ç¬¬3æ¬¡\n",
    "    print(f'Train shape: {train_df.shape}, test shape: {test_df.shape} are loaded.')\n",
    "    \n",
    "    print(f'âœ“ ä¼ªæ ‡ç­¾æŠ€æœ¯ï¼šè®­ç»ƒé›†ä»{train_df.shape[0] - 3*test_df.shape[0]}æ‰©å±•åˆ°{train_df.shape[0]}æ ·æœ¬')\n",
    "\n",
    "    # ==================== KæŠ˜äº¤å‰éªŒè¯è®¾ç½® ====================\n",
    "    # KæŠ˜äº¤å‰éªŒè¯ï¼ˆK-Fold Cross-Validationï¼‰\n",
    "    # \n",
    "    # ä»€ä¹ˆæ˜¯KæŠ˜äº¤å‰éªŒè¯ï¼Ÿ\n",
    "    # å°†è®­ç»ƒé›†åˆ†ä¸ºKä»½ï¼ˆè¿™é‡ŒK=5ï¼‰ï¼Œæ¯æ¬¡ï¼š\n",
    "    # - ç”¨4ä»½è®­ç»ƒæ¨¡å‹\n",
    "    # - ç”¨1ä»½éªŒè¯æ¨¡å‹\n",
    "    # - è½®æµ5æ¬¡ï¼Œç¡®ä¿æ¯ä»½æ•°æ®éƒ½è¢«éªŒè¯è¿‡\n",
    "    #\n",
    "    # ä¸ºä»€ä¹ˆä½¿ç”¨ï¼Ÿ\n",
    "    # 1. æ›´å¯é çš„æ€§èƒ½è¯„ä¼°ï¼ˆå‡å°‘è¿æ°”æˆåˆ†ï¼‰\n",
    "    # 2. å……åˆ†åˆ©ç”¨æ‰€æœ‰æ•°æ®\n",
    "    # 3. é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "    # 4. 5ä¸ªæ¨¡å‹çš„é¢„æµ‹å¯ä»¥é›†æˆï¼ˆensembleï¼‰\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "    # ==================== åˆå§‹åŒ–é¢„æµ‹ç»“æœå­˜å‚¨ ====================\n",
    "    # OOF (Out-Of-Fold) predictions: è®­ç»ƒé›†çš„é¢„æµ‹ç»“æœ\n",
    "    # æ¯ä¸ªæ ·æœ¬åœ¨ä½œä¸ºéªŒè¯é›†æ—¶çš„é¢„æµ‹ç»“æœ\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    # æµ‹è¯•é›†çš„é¢„æµ‹ç»“æœï¼ˆ5ä¸ªæ¨¡å‹çš„å¹³å‡ï¼‰\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    # ==================== ç‰¹å¾é€‰æ‹© ====================\n",
    "    # æ’é™¤ä¸å‚ä¸è®­ç»ƒçš„åˆ—ï¼ˆæ ‡ç­¾å’Œç´¢å¼•ï¼‰\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']]\n",
    "    \n",
    "    print(f'only {len(feats)} features from a total {train_df.shape[1]} features are used for ML analysis')\n",
    "    print(f'âœ“ å‡†å¤‡è¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯...')\n",
    "\n",
    "    # ==================== KæŠ˜è®­ç»ƒå¾ªç¯ ====================\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        # æ ¹æ®ç´¢å¼•åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "        \n",
    "        # ==================== LightGBMæ¨¡å‹é…ç½® ====================\n",
    "        # è¿™äº›è¶…å‚æ•°ç»è¿‡ç²¾å¿ƒè°ƒä¼˜ï¼ˆæ¥è‡ªå…¶ä»–é«˜åˆ†kernelï¼‰\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=-1,              # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ\n",
    "            #device_type='gpu',      # å¯é€‰ï¼šä½¿ç”¨GPUåŠ é€Ÿ\n",
    "            \n",
    "            # --- æ ‘ç»“æ„å‚æ•° ---\n",
    "            n_estimators=5000,       # æœ€å¤š5000æ£µæ ‘ï¼ˆæ—©åœä¼šæå‰ç»“æŸï¼‰\n",
    "            max_depth=11,            # æ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰\n",
    "            num_leaves=58,           # å¶å­èŠ‚ç‚¹æ•°ï¼ˆLightGBMæ ¸å¿ƒå‚æ•°ï¼‰\n",
    "                                     # num_leavesåº”è¯¥ < 2^max_depth\n",
    "            \n",
    "            # --- å­¦ä¹ ç‡å‚æ•° ---\n",
    "            learning_rate=0.01,      # å­¦ä¹ ç‡ï¼ˆè¾ƒå°=æ›´ç¨³å®šä½†æ›´æ…¢ï¼‰\n",
    "            \n",
    "            # --- é‡‡æ ·å‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰---\n",
    "            colsample_bytree=0.613,  # æ¯æ£µæ ‘ä½¿ç”¨61.3%çš„ç‰¹å¾\n",
    "            subsample=0.708,         # æ¯æ£µæ ‘ä½¿ç”¨70.8%çš„æ ·æœ¬\n",
    "            \n",
    "            # --- æ­£åˆ™åŒ–å‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰---\n",
    "            reg_alpha=3.564,         # L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰\n",
    "            reg_lambda=4.930,        # L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰\n",
    "            \n",
    "            # --- å¶å­èŠ‚ç‚¹å‚æ•° ---\n",
    "            max_bin=407,             # ç‰¹å¾åˆ†æ¡¶æ•°ï¼ˆè¶Šå¤§è¶Šç²¾ç»†ä½†è¶Šæ…¢ï¼‰\n",
    "            min_child_weight=6,      # å¶å­èŠ‚ç‚¹æœ€å°æƒé‡\n",
    "            min_child_samples=165,   # å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°\n",
    "            \n",
    "            # --- å…¶ä»–å‚æ•° ---\n",
    "            #keep_training_booster=True,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "\n",
    "        # ==================== æ¨¡å‹è®­ç»ƒ ====================\n",
    "        # early_stopping_rounds=500: å¦‚æœ500è½®éªŒè¯é›†AUCä¸æå‡ï¼Œåˆ™åœæ­¢\n",
    "        # eval_metric='auc': ä½¿ç”¨AUCä½œä¸ºè¯„ä»·æŒ‡æ ‡\n",
    "        # verbose=500: æ¯500è½®æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric='auc', \n",
    "                verbose=500, \n",
    "                early_stopping_rounds=500)\n",
    "\n",
    "        # ==================== é¢„æµ‹ ====================\n",
    "        # 1. å¯¹éªŒè¯é›†é¢„æµ‹ï¼ˆç”¨äºè®¡ç®—OOF AUCï¼‰\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        # 2. å¯¹æµ‹è¯•é›†é¢„æµ‹ï¼ˆç´¯åŠ åå¹³å‡ï¼‰\n",
    "        # [:, 1]è¡¨ç¤ºå–è¿çº¦æ¦‚ç‡ï¼ˆç¬¬1åˆ—æ˜¯ä¸è¿çº¦æ¦‚ç‡ï¼‰\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        # æ‰“å°æœ¬æŠ˜çš„AUCåˆ†æ•°\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        \n",
    "        # é‡Šæ”¾å†…å­˜\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # ==================== äº¤å‰éªŒè¯æ€»ä½“æ€§èƒ½ ====================\n",
    "    # OOF AUC: æ‰€æœ‰æ ·æœ¬ä½œä¸ºéªŒè¯é›†æ—¶çš„é¢„æµ‹æ±‡æ€»\n",
    "    # è¿™æ˜¯æ¨¡å‹çœŸå®æ€§èƒ½çš„æœ€ä½³ä¼°è®¡\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "\n",
    "    # ==================== ç”Ÿæˆæäº¤æ–‡ä»¶ ====================\n",
    "    # å°†5ä¸ªæ¨¡å‹çš„å¹³å‡é¢„æµ‹ç»“æœä¿å­˜ä¸ºæäº¤æ–‡ä»¶\n",
    "    test_df['TARGET'] = sub_preds\n",
    "    test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index=False)\n",
    "    print('a submission file is created')\n",
    "    print(f'âœ“ é¢„æµ‹å®Œæˆï¼æäº¤æ–‡ä»¶å·²ä¿å­˜ä¸º submission.csv')\n",
    "    print(f'âœ“ é¢„æµ‹çš„è¿çº¦æ¦‚ç‡èŒƒå›´ï¼š[{sub_preds.min():.4f}, {sub_preds.max():.4f}]')\n",
    "    print(f'âœ“ é¢„æµ‹çš„å¹³å‡è¿çº¦ç‡ï¼š{sub_preds.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-lawsuit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.555978Z",
     "iopub.status.busy": "2021-05-04T15:38:40.555282Z",
     "iopub.status.idle": "2021-05-04T20:54:03.343722Z",
     "shell.execute_reply": "2021-05-04T20:54:03.343008Z"
    },
    "papermill": {
     "duration": 18922.809903,
     "end_time": "2021-05-04T20:54:03.343997",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.534094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Application_Train_Test\" final shape: (356250, 309)\n",
      "\"Bureau/Bureau Balance\" final shape: (305811, 200)\n",
      "--=> df after merge with bureau: (356250, 509)\n",
      "\"Previous Applications\" final shape: (338857, 321)\n",
      "--=> df after merge with previous application: (356250, 830)\n",
      "\"Pos-Cash\" balance final shape: (337252, 46)\n",
      "--=> df after merge with pos cash : (356250, 875)\n",
      "\"Installments Payments\" final shape: (339587, 85)\n",
      "--=> df after merge with installments: (356250, 960)\n",
      "\"Credit Card Balance\" final shape: (103558, 284)\n",
      "--=> df after merge with credit card: (356250, 1243)\n",
      "---=> the DATA POST-PROCESSING is beginning, the dataset has 1243 features\n",
      "names of feature are renamed\n",
      "---=> pandas data types of features in the dataset are converted for a reduced memory usage\n",
      "---=> 1199 features are remained after removing non-informative features\n",
      "164 features are eliminated by LightGBM classifier with an 0.7 auc score limit in step I\n",
      "---=> 1035 features are remained after removing features not interesting for LightGBM classifier\n",
      "---=> 44 features are generated with the risk_groupanizer\n",
      "---=> the DATA POST-PROCESSING is ended!, now the dataset has a total 1079 features\n",
      "================================================== \n",
      "\n",
      "---=> df final shape: (356250, 1079)  <=--- \n",
      "\n",
      "==================================================\n",
      "=============================================== \n",
      " ##### the ML in processing...\n",
      "Train shape: (453738, 1079), test shape: (48744, 1079) are loaded.\n",
      "only 1077 features from a total 1079 features are used for ML analysis\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905142\ttraining's binary_logloss: 0.232344\tvalid_1's auc: 0.897325\tvalid_1's binary_logloss: 0.240314\n",
      "[1000]\ttraining's auc: 0.92274\ttraining's binary_logloss: 0.209022\tvalid_1's auc: 0.909934\tvalid_1's binary_logloss: 0.223098\n",
      "[1500]\ttraining's auc: 0.933288\ttraining's binary_logloss: 0.195351\tvalid_1's auc: 0.915287\tvalid_1's binary_logloss: 0.215273\n",
      "[2000]\ttraining's auc: 0.94142\ttraining's binary_logloss: 0.184865\tvalid_1's auc: 0.918268\tvalid_1's binary_logloss: 0.210412\n",
      "[2500]\ttraining's auc: 0.948161\ttraining's binary_logloss: 0.176246\tvalid_1's auc: 0.920058\tvalid_1's binary_logloss: 0.207175\n",
      "[3000]\ttraining's auc: 0.95405\ttraining's binary_logloss: 0.1688\tvalid_1's auc: 0.921394\tvalid_1's binary_logloss: 0.204692\n",
      "[3500]\ttraining's auc: 0.959189\ttraining's binary_logloss: 0.16212\tvalid_1's auc: 0.922375\tvalid_1's binary_logloss: 0.202641\n",
      "[4000]\ttraining's auc: 0.963802\ttraining's binary_logloss: 0.155943\tvalid_1's auc: 0.923128\tvalid_1's binary_logloss: 0.20097\n",
      "[4500]\ttraining's auc: 0.967975\ttraining's binary_logloss: 0.150169\tvalid_1's auc: 0.923844\tvalid_1's binary_logloss: 0.199428\n",
      "[5000]\ttraining's auc: 0.97173\ttraining's binary_logloss: 0.14468\tvalid_1's auc: 0.924443\tvalid_1's binary_logloss: 0.198038\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.97173\ttraining's binary_logloss: 0.14468\tvalid_1's auc: 0.924443\tvalid_1's binary_logloss: 0.198038\n",
      "Fold  1 AUC : 0.924443\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905837\ttraining's binary_logloss: 0.232124\tvalid_1's auc: 0.893077\tvalid_1's binary_logloss: 0.242347\n",
      "[1000]\ttraining's auc: 0.923283\ttraining's binary_logloss: 0.208853\tvalid_1's auc: 0.905964\tvalid_1's binary_logloss: 0.225129\n",
      "[1500]\ttraining's auc: 0.933657\ttraining's binary_logloss: 0.195249\tvalid_1's auc: 0.911449\tvalid_1's binary_logloss: 0.217228\n",
      "[2000]\ttraining's auc: 0.941663\ttraining's binary_logloss: 0.184845\tvalid_1's auc: 0.914518\tvalid_1's binary_logloss: 0.212335\n",
      "[2500]\ttraining's auc: 0.948587\ttraining's binary_logloss: 0.176195\tvalid_1's auc: 0.916421\tvalid_1's binary_logloss: 0.209059\n",
      "[3000]\ttraining's auc: 0.954301\ttraining's binary_logloss: 0.168843\tvalid_1's auc: 0.917626\tvalid_1's binary_logloss: 0.206689\n",
      "[3500]\ttraining's auc: 0.959491\ttraining's binary_logloss: 0.162116\tvalid_1's auc: 0.918646\tvalid_1's binary_logloss: 0.204682\n",
      "[4000]\ttraining's auc: 0.964053\ttraining's binary_logloss: 0.155929\tvalid_1's auc: 0.919422\tvalid_1's binary_logloss: 0.202973\n",
      "[4500]\ttraining's auc: 0.968328\ttraining's binary_logloss: 0.150049\tvalid_1's auc: 0.920112\tvalid_1's binary_logloss: 0.201462\n",
      "[5000]\ttraining's auc: 0.971997\ttraining's binary_logloss: 0.144647\tvalid_1's auc: 0.92063\tvalid_1's binary_logloss: 0.20017\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971997\ttraining's binary_logloss: 0.144647\tvalid_1's auc: 0.92063\tvalid_1's binary_logloss: 0.20017\n",
      "Fold  2 AUC : 0.920630\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905265\ttraining's binary_logloss: 0.232508\tvalid_1's auc: 0.895704\tvalid_1's binary_logloss: 0.240696\n",
      "[1000]\ttraining's auc: 0.922893\ttraining's binary_logloss: 0.209156\tvalid_1's auc: 0.908594\tvalid_1's binary_logloss: 0.223264\n",
      "[1500]\ttraining's auc: 0.933243\ttraining's binary_logloss: 0.195477\tvalid_1's auc: 0.913964\tvalid_1's binary_logloss: 0.21529\n",
      "[2000]\ttraining's auc: 0.941352\ttraining's binary_logloss: 0.185012\tvalid_1's auc: 0.917136\tvalid_1's binary_logloss: 0.210231\n",
      "[2500]\ttraining's auc: 0.948175\ttraining's binary_logloss: 0.176409\tvalid_1's auc: 0.919099\tvalid_1's binary_logloss: 0.206873\n",
      "[3000]\ttraining's auc: 0.954081\ttraining's binary_logloss: 0.168909\tvalid_1's auc: 0.920431\tvalid_1's binary_logloss: 0.204375\n",
      "[3500]\ttraining's auc: 0.959207\ttraining's binary_logloss: 0.162224\tvalid_1's auc: 0.92146\tvalid_1's binary_logloss: 0.202281\n",
      "[4000]\ttraining's auc: 0.963864\ttraining's binary_logloss: 0.156028\tvalid_1's auc: 0.922299\tvalid_1's binary_logloss: 0.20052\n",
      "[4500]\ttraining's auc: 0.968119\ttraining's binary_logloss: 0.150156\tvalid_1's auc: 0.923087\tvalid_1's binary_logloss: 0.198884\n",
      "[5000]\ttraining's auc: 0.971935\ttraining's binary_logloss: 0.144681\tvalid_1's auc: 0.923727\tvalid_1's binary_logloss: 0.197462\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971935\ttraining's binary_logloss: 0.144681\tvalid_1's auc: 0.923727\tvalid_1's binary_logloss: 0.197462\n",
      "Fold  3 AUC : 0.923727\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905623\ttraining's binary_logloss: 0.231929\tvalid_1's auc: 0.89495\tvalid_1's binary_logloss: 0.242589\n",
      "[1000]\ttraining's auc: 0.923033\ttraining's binary_logloss: 0.208651\tvalid_1's auc: 0.907343\tvalid_1's binary_logloss: 0.225561\n",
      "[1500]\ttraining's auc: 0.933459\ttraining's binary_logloss: 0.194959\tvalid_1's auc: 0.912764\tvalid_1's binary_logloss: 0.217628\n",
      "[2000]\ttraining's auc: 0.941646\ttraining's binary_logloss: 0.184566\tvalid_1's auc: 0.915911\tvalid_1's binary_logloss: 0.21271\n",
      "[2500]\ttraining's auc: 0.948288\ttraining's binary_logloss: 0.176119\tvalid_1's auc: 0.917755\tvalid_1's binary_logloss: 0.209501\n",
      "[3000]\ttraining's auc: 0.954057\ttraining's binary_logloss: 0.16871\tvalid_1's auc: 0.919095\tvalid_1's binary_logloss: 0.207\n",
      "[3500]\ttraining's auc: 0.959343\ttraining's binary_logloss: 0.161893\tvalid_1's auc: 0.920165\tvalid_1's binary_logloss: 0.204888\n",
      "[4000]\ttraining's auc: 0.963977\ttraining's binary_logloss: 0.155739\tvalid_1's auc: 0.920979\tvalid_1's binary_logloss: 0.203163\n",
      "[4500]\ttraining's auc: 0.968121\ttraining's binary_logloss: 0.149998\tvalid_1's auc: 0.921693\tvalid_1's binary_logloss: 0.201623\n",
      "[5000]\ttraining's auc: 0.971896\ttraining's binary_logloss: 0.144552\tvalid_1's auc: 0.922338\tvalid_1's binary_logloss: 0.20022\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971896\ttraining's binary_logloss: 0.144552\tvalid_1's auc: 0.922338\tvalid_1's binary_logloss: 0.20022\n",
      "Fold  4 AUC : 0.922338\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905187\ttraining's binary_logloss: 0.233112\tvalid_1's auc: 0.898082\tvalid_1's binary_logloss: 0.237528\n",
      "[1000]\ttraining's auc: 0.922842\ttraining's binary_logloss: 0.209511\tvalid_1's auc: 0.909799\tvalid_1's binary_logloss: 0.220898\n",
      "[1500]\ttraining's auc: 0.933275\ttraining's binary_logloss: 0.195734\tvalid_1's auc: 0.914849\tvalid_1's binary_logloss: 0.213236\n",
      "[2000]\ttraining's auc: 0.941469\ttraining's binary_logloss: 0.185277\tvalid_1's auc: 0.917758\tvalid_1's binary_logloss: 0.208512\n",
      "[2500]\ttraining's auc: 0.948207\ttraining's binary_logloss: 0.17669\tvalid_1's auc: 0.919569\tvalid_1's binary_logloss: 0.205261\n",
      "[3000]\ttraining's auc: 0.953965\ttraining's binary_logloss: 0.169298\tvalid_1's auc: 0.920767\tvalid_1's binary_logloss: 0.202877\n",
      "[3500]\ttraining's auc: 0.959161\ttraining's binary_logloss: 0.16256\tvalid_1's auc: 0.921822\tvalid_1's binary_logloss: 0.200809\n",
      "[4000]\ttraining's auc: 0.963824\ttraining's binary_logloss: 0.156315\tvalid_1's auc: 0.922631\tvalid_1's binary_logloss: 0.199063\n",
      "[4500]\ttraining's auc: 0.968079\ttraining's binary_logloss: 0.15044\tvalid_1's auc: 0.923325\tvalid_1's binary_logloss: 0.197527\n",
      "[5000]\ttraining's auc: 0.971873\ttraining's binary_logloss: 0.144925\tvalid_1's auc: 0.923917\tvalid_1's binary_logloss: 0.196092\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971873\ttraining's binary_logloss: 0.144925\tvalid_1's auc: 0.923917\tvalid_1's binary_logloss: 0.196092\n",
      "Fold  5 AUC : 0.923917\n",
      "Full AUC score 0.922996\n",
      "a submission file is created\n",
      "--=> all calculations are done!! <=--\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==================== ä¸»æ‰§è¡Œæµç¨‹ ====================\n",
    "å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ -> æ•°æ®åå¤„ç† -> æ¨¡å‹è®­ç»ƒæµæ°´çº¿\n",
    "\n",
    "æ•°æ®åˆå¹¶é¡ºåºè¯´æ˜ï¼š\n",
    "1. application: ä¸»è¡¨ï¼ˆå®¢æˆ·åŸºæœ¬ä¿¡æ¯ï¼‰ - 309åˆ—\n",
    "2. bureau: ä¿¡ç”¨å±€å†å² - å¢åŠ 200åˆ—\n",
    "3. previous_application: å†å²ç”³è¯· - å¢åŠ 321åˆ—\n",
    "4. pos_cash: åˆ†æœŸä»˜æ¬¾ä½™é¢ - å¢åŠ 46åˆ—\n",
    "5. installment: åˆ†æœŸè¿˜æ¬¾è®°å½• - å¢åŠ 85åˆ—\n",
    "6. credit_card: ä¿¡ç”¨å¡ä½™é¢ - å¢åŠ 284åˆ—\n",
    "\n",
    "æœ€ç»ˆç‰¹å¾æ•°ï¼š1243åˆ— -> åå¤„ç†åçº¦1079åˆ—\n",
    "\"\"\"\n",
    "\n",
    "# ==================== æ­¥éª¤1: å¤„ç†ä¸»ç”³è¯·è¡¨ ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ­¥éª¤ 1/7: å¤„ç†ä¸»ç”³è¯·è¡¨ (Application)')\n",
    "print('='*60)\n",
    "df = application()\n",
    "\n",
    "# ==================== æ­¥éª¤2: åˆå¹¶ä¿¡ç”¨å±€æ•°æ® ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ­¥éª¤ 2/7: åˆå¹¶ä¿¡ç”¨å±€æ•°æ® (Bureau)')\n",
    "print('='*60)\n",
    "df = df.merge(bureau_bb(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with bureau:', df.shape)\n",
    "\n",
    "# ==================== æ­¥éª¤3: åˆå¹¶å†å²ç”³è¯·æ•°æ® ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ­¥éª¤ 3/7: åˆå¹¶å†å²ç”³è¯·æ•°æ® (Previous Application)')\n",
    "print('='*60)\n",
    "df = df.merge(previous_application(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with previous application:', df.shape)\n",
    "\n",
    "# ==================== æ­¥éª¤4: åˆå¹¶POSåˆ†æœŸæ•°æ® ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ­¥éª¤ 4/7: åˆå¹¶POSåˆ†æœŸæ•°æ® (POS Cash)')\n",
    "print('='*60)\n",
    "df = df.merge(pos_cash(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with pos cash :', df.shape)\n",
    "\n",
    "# ==================== æ­¥éª¤5: åˆå¹¶åˆ†æœŸè¿˜æ¬¾æ•°æ® ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ­¥éª¤ 5/7: åˆå¹¶åˆ†æœŸè¿˜æ¬¾æ•°æ® (Installments)')\n",
    "print('='*60)\n",
    "df = df.merge(installment(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with installments:', df.shape)\n",
    "\n",
    "# ==================== æ­¥éª¤6: åˆå¹¶ä¿¡ç”¨å¡æ•°æ® ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ­¥éª¤ 6/7: åˆå¹¶ä¿¡ç”¨å¡æ•°æ® (Credit Card)')\n",
    "print('='*60)\n",
    "df = df.merge(credit_card(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with credit card:', df.shape)\n",
    "\n",
    "# ==================== æ­¥éª¤7: æ•°æ®åå¤„ç† ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ­¥éª¤ 7/7: æ•°æ®åå¤„ç† (ç‰¹å¾é€‰æ‹©ã€å†…å­˜ä¼˜åŒ–ã€é£é™©ç¼–ç )')\n",
    "print('='*60)\n",
    "df = data_post_processing(df)\n",
    "print('='*50, '\\n')\n",
    "print('---=> df final shape:', df.shape, ' <=---', '\\n')\n",
    "print('=' * 50)\n",
    "\n",
    "# ==================== æ­¥éª¤8: æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ ====================\n",
    "print('\\n' + '='*60)\n",
    "print('æ¨¡å‹è®­ç»ƒ: 5æŠ˜äº¤å‰éªŒè¯ + ä¼ªæ ‡ç­¾æŠ€æœ¯')\n",
    "print('='*60)\n",
    "Kfold_LightGBM(df)\n",
    "print('\\n' + '='*60)\n",
    "print('--=> all calculations are done!! <=--')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18935.297056,
   "end_time": "2021-05-04T20:54:04.803426",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-04T15:38:29.506370",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
