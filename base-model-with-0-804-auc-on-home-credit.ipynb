{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09432d52",
   "metadata": {},
   "source": [
    "# 📊 Home Credit违约风险预测 - 完整技术文档\n",
    "\n",
    "## 🎯 项目概览\n",
    "\n",
    "本项目是Kaggle竞赛\"Home Credit Default Risk\"的完整解决方案，目标是预测客户是否会违约贷款。\n",
    "\n",
    "**核心成果**：\n",
    "- ✅ AUC分数：**0.804+**（Top 10%水平）\n",
    "- ✅ 特征数量：**1000+** 精心构造的特征\n",
    "- ✅ 训练样本：**30万+** 扩展至 **50万+**（伪标签技术）\n",
    "- ✅ 模型：LightGBM + 5折交叉验证\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 核心技术亮点\n",
    "\n",
    "### 1️⃣ **伪标签技术（Pseudo-Labeling）**\n",
    "**最重要的创新点！**\n",
    "- 📌 **原理**：使用预训练模型对测试集预测，将高置信度预测作为\"伪标签\"，扩充训练集\n",
    "- 📌 **效果**：训练样本从30万增加到50万+，模型性能显著提升\n",
    "- 📌 **实现**：重复添加3次带伪标签的测试集（阈值>0.75）\n",
    "\n",
    "### 2️⃣ **风险分组编码（Risk Grouping）**\n",
    "**比One-Hot更强大的类别编码！**\n",
    "- 📌 **原理**：根据每个类别的违约率，标记为高/中/低风险\n",
    "- 📌 **优势**：特征数量减少90%，但包含更多预测信息\n",
    "- 📌 **举例**：职业=教师，违约率8.5% → 标记为\"职业_高风险\"\n",
    "\n",
    "### 3️⃣ **时间窗口特征**\n",
    "**捕捉行为变化趋势！**\n",
    "- 📌 **窗口**：最近12月、24月、48月\n",
    "- 📌 **应用**：如果最近12月额度使用率远高于48月，说明财务恶化\n",
    "\n",
    "### 4️⃣ **深度特征工程**\n",
    "- 📌 **7张数据表**全面聚合（application、bureau、previous等）\n",
    "- 📌 **比率特征**：收入/信贷、还款/收入等（比绝对值更有信息量）\n",
    "- 📌 **交互特征**：EXT_SOURCE_1 × EXT_SOURCE_2（捕捉非线性关系）\n",
    "- 📌 **时间特征**：就业天数/年龄、车龄/就业时长等\n",
    "\n",
    "### 5️⃣ **内存优化**\n",
    "- 📌 **技术**：自动降低数据类型精度（int64→int8、float64→float16）\n",
    "- 📌 **效果**：内存占用减少75%（从16GB→4GB）\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 数据源说明\n",
    "\n",
    "| 数据表 | 记录内容 | 关键特征 | 重要性 |\n",
    "|--------|----------|----------|--------|\n",
    "| **application** | 主申请信息 | 收入、年龄、EXT_SOURCE评分 | ⭐⭐⭐⭐⭐ |\n",
    "| **bureau** | 信用局历史 | 历史信贷、逾期记录 | ⭐⭐⭐⭐⭐ |\n",
    "| **installments** | 分期还款 | DPD（逾期天数）、还款比率 | ⭐⭐⭐⭐⭐ |\n",
    "| **credit_card** | 信用卡余额 | 额度使用率、取现行为 | ⭐⭐⭐⭐ |\n",
    "| **previous_app** | 历史申请 | 批准率、首付比例 | ⭐⭐⭐⭐ |\n",
    "| **pos_cash** | POS分期 | 小额分期的逾期情况 | ⭐⭐⭐ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 完整流程\n",
    "\n",
    "```\n",
    "数据加载 → 特征工程（7表合并）→ 后处理（选择+编码）→ 伪标签 → K折训练 → 预测\n",
    "```\n",
    "\n",
    "### 详细步骤：\n",
    "\n",
    "1. **application()**：处理主申请表，创建300+特征\n",
    "2. **bureau_bb()**：信用局数据，创建200+特征\n",
    "3. **previous_application()**：历史申请，创建300+特征\n",
    "4. **pos_cash()**：POS分期，创建45+特征\n",
    "5. **installment()**：分期还款，创建85+特征\n",
    "6. **credit_card()**：信用卡，创建280+特征\n",
    "7. **data_post_processing()**：特征选择、内存优化、风险编码\n",
    "8. **Kfold_LightGBM()**：伪标签+5折交叉验证\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 关键概念解释\n",
    "\n",
    "### DPD (Days Past Due)\n",
    "- **含义**：逾期天数，信用评分中最重要的指标\n",
    "- **分类**：0天（按时）、1-30天（轻微）、30-90天（中度）、90-120天（严重）、120+天（极严重）\n",
    "\n",
    "### AUC (Area Under Curve)\n",
    "- **含义**：ROC曲线下面积，评估分类模型的性能\n",
    "- **范围**：0.5（随机猜测）到 1.0（完美预测）\n",
    "- **本项目**：0.804（优秀水平）\n",
    "\n",
    "### 特征重要性排名\n",
    "1. **EXT_SOURCE_2/3**：外部信用评分（最强特征）\n",
    "2. **DAYS_BIRTH**：年龄\n",
    "3. **INSTAL_DPD_MAX**：历史最大逾期天数\n",
    "4. **BUREAU_CREDIT_ACTIVE**：活跃信贷数\n",
    "5. **AMT_ANNUITY**：年金（还款金额）\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 实用技巧\n",
    "\n",
    "### 特征工程原则：\n",
    "✅ **比率 > 绝对值**：收入/信贷比 比单独的收入更有意义  \n",
    "✅ **最近 > 历史**：最近12月行为比48月平均更重要  \n",
    "✅ **聚合统计**：min/max/mean/std 都有独特信息  \n",
    "✅ **交叉特征**：捕捉非线性关系  \n",
    "\n",
    "### LightGBM调参：\n",
    "- `num_leaves=58`：控制模型复杂度\n",
    "- `learning_rate=0.01`：小学习率+早停\n",
    "- `reg_alpha=3.564, reg_lambda=4.930`：防止过拟合\n",
    "- `colsample_bytree=0.613`：随机特征采样\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 性能基准\n",
    "\n",
    "| 模型版本 | 特征数 | AUC | 说明 |\n",
    "|----------|--------|-----|------|\n",
    "| Baseline | 100 | 0.75 | 仅使用主表 |\n",
    "| +Bureau | 300 | 0.78 | 加入信用局数据 |\n",
    "| +All Tables | 1000+ | 0.80 | 全部数据表 |\n",
    "| +Pseudo-Label | 1000+ | **0.804** | 伪标签技术 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 如何使用本Notebook\n",
    "\n",
    "1. **学习特征工程**：查看每个函数的详细注释\n",
    "2. **理解伪标签**：重点阅读`Kfold_LightGBM()`函数\n",
    "3. **复用代码**：工具函数（如`risk_groupanizer`）可用于其他项目\n",
    "4. **调优实验**：修改超参数或添加新特征\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 参考资源\n",
    "\n",
    "- [LightGBM官方文档](https://lightgbm.readthedocs.io/)\n",
    "- [Kaggle竞赛页面](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "- [特征工程指南](https://www.kaggle.com/learn/feature-engineering)\n",
    "\n",
    "---\n",
    "\n",
    "**开始阅读代码吧！每个函数都有详细的中文注释和实例讲解。** 🎉\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-manchester",
   "metadata": {
    "papermill": {
     "duration": 0.012532,
     "end_time": "2021-05-04T15:38:37.825385",
     "exception": false,
     "start_time": "2021-05-04T15:38:37.812853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Home Credit违约风险预测 - 基础模型研究\n",
    "\n",
    "### 项目背景\n",
    "这是一个Kaggle竞赛项目：Home Credit Default Risk（房贷违约风险预测）\n",
    "\n",
    "### 项目目标\n",
    "预测客户是否会违约贷款（TARGET=1表示违约，TARGET=0表示正常还款）\n",
    "\n",
    "### 模型性能\n",
    "- 最高AUC分数：0.804+\n",
    "- 使用特征数：900-1800个特征\n",
    "- 交叉验证：5折交叉验证\n",
    "- 计算平台：Google Colab Pro (GPU加速)\n",
    "\n",
    "### 技术亮点\n",
    "1. **特征工程**：从7个相关数据表中构建大量特征\n",
    "2. **内存优化**：通过数据类型转换将内存压缩至原来的1/4\n",
    "3. **伪标签技术**：使用测试集的预测结果扩充训练集\n",
    "4. **特征选择**：使用LightGBM进行特征筛选\n",
    "5. **集成学习**：blend boosting方法达到0.81128 AUC\n",
    "\n",
    "### 数据源\n",
    "- application_train.csv / application_test.csv：主申请表\n",
    "- bureau.csv：信用局数据\n",
    "- bureau_balance.csv：信用局月度余额\n",
    "- previous_application.csv：历史申请记录\n",
    "- POS_CASH_balance.csv：销售点分期付款余额\n",
    "- installments_payments.csv：分期付款历史\n",
    "- credit_card_balance.csv：信用卡余额\n",
    "\n",
    "### 参考资源\n",
    "本项目整合了多个优秀Kaggle kernel的思路和特征工程方法：\n",
    "* https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features <=-- 模型基础框架\n",
    "* https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution <=-- 第7名解决方案\n",
    "* https://www.kaggle.com/sangseoseo/oof-all-home-credit-default-risk <=-- 超参数来源\n",
    "* https://www.kaggle.com/ashishpatel26/different-basic-blends-possible <=-- 集成学习思路\n",
    "* https://www.kaggle.com/mathchi/home-credit-risk-with-detailed-feature-engineering\n",
    "* https://www.kaggle.com/windofdl/kernelf68f763785\n",
    "* https://www.kaggle.com/meraxes10/lgbm-credit-default-prediction\n",
    "* https://www.kaggle.com/luudactam/hc-v500\n",
    "* https://www.kaggle.com/aantonova/aggregating-all-tables-in-one-dataset\n",
    "* https://www.kaggle.com/wanakon/kernel24647bb75c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-brand",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:37.855153Z",
     "iopub.status.busy": "2021-05-04T15:38:37.854522Z",
     "iopub.status.idle": "2021-05-04T15:38:37.857381Z",
     "shell.execute_reply": "2021-05-04T15:38:37.856630Z"
    },
    "papermill": {
     "duration": 0.020288,
     "end_time": "2021-05-04T15:38:37.857548",
     "exception": false,
     "start_time": "2021-05-04T15:38:37.837260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【依赖库安装】\n",
    "安装特定版本的LightGBM库（2.3.1版本）\n",
    "LightGBM是微软开发的高效梯度提升决策树框架，特点：\n",
    "1. 训练速度快，效率高\n",
    "2. 内存占用低\n",
    "3. 准确率高\n",
    "4. 支持并行和GPU加速\n",
    "5. 能处理大规模数据\n",
    "\n",
    "注：在Kaggle环境中通常已预装，此处注释掉\n",
    "\"\"\"\n",
    "# !pip install lightgbm==2.3.1\n",
    "# import lightgbm\n",
    "# lightgbm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-volunteer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:37.891724Z",
     "iopub.status.busy": "2021-05-04T15:38:37.890994Z",
     "iopub.status.idle": "2021-05-04T15:38:40.065420Z",
     "shell.execute_reply": "2021-05-04T15:38:40.064431Z"
    },
    "papermill": {
     "duration": 2.196139,
     "end_time": "2021-05-04T15:38:40.065571",
     "exception": false,
     "start_time": "2021-05-04T15:38:37.869432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "【导入必要的库】\n",
    "项目所需的核心库及其用途说明\n",
    "\"\"\"\n",
    "\n",
    "# gc: 垃圾回收库，用于手动释放内存，在处理大数据集时非常重要\n",
    "import gc\n",
    "\n",
    "# re: 正则表达式库，用于特征名称的清洗和标准化\n",
    "import re\n",
    "\n",
    "# time: 时间库，用于记录程序运行时间\n",
    "import time\n",
    "\n",
    "# numpy: 数值计算库，提供高效的数组操作\n",
    "import numpy as np\n",
    "\n",
    "# pandas: 数据处理库，提供DataFrame等数据结构\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib & seaborn: 数据可视化库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LGBMClassifier: LightGBM的分类器，本项目的核心模型\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# roc_auc_score: ROC-AUC评分函数，本竞赛的评价指标\n",
    "# AUC (Area Under Curve) 值越接近1表示模型效果越好\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# KFold: K折交叉验证，用于划分训练集和验证集\n",
    "# 本项目使用5折交叉验证来评估模型稳定性\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# warnings: 警告控制库\n",
    "import warnings\n",
    "\n",
    "# 忽略警告信息，使输出更清晰\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-mediterranean",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.126788Z",
     "iopub.status.busy": "2021-05-04T15:38:40.126060Z",
     "iopub.status.idle": "2021-05-04T15:38:40.129287Z",
     "shell.execute_reply": "2021-05-04T15:38:40.128719Z"
    },
    "papermill": {
     "duration": 0.051056,
     "end_time": "2021-05-04T15:38:40.129440",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.078384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【核心工具函数定义】\n",
    "本模块包含特征工程中使用的核心函数\n",
    "\"\"\"\n",
    "\n",
    "# ==================== 1. One-Hot编码函数 ====================\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    【独热编码（One-Hot Encoding）】\n",
    "    \n",
    "    功能：将类别特征转换为数值特征\n",
    "    \n",
    "    原理说明：\n",
    "    将每个类别值转换为一个二进制列（0或1）\n",
    "    \n",
    "    举例：\n",
    "    原始数据：\n",
    "        性别列 = ['男', '女', '男', '女']\n",
    "    \n",
    "    转换后：\n",
    "        性别_男 = [1, 0, 1, 0]\n",
    "        性别_女 = [0, 1, 0, 1]\n",
    "    \n",
    "    参数：\n",
    "        df: 输入的DataFrame\n",
    "        nan_as_category: 是否将缺失值(NaN)也作为一个类别\n",
    "                        True表示为缺失值单独创建一列\n",
    "    \n",
    "    返回：\n",
    "        df: 编码后的DataFrame\n",
    "        new_columns: 新增的列名列表\n",
    "    \n",
    "    为什么需要One-Hot编码？\n",
    "    - 机器学习模型（如LightGBM）可以更好地理解类别特征\n",
    "    - 避免模型误认为类别之间存在大小关系\n",
    "    \"\"\"\n",
    "    # 保存原始列名\n",
    "    original_columns = list(df.columns)\n",
    "    \n",
    "    # 找出所有的类别列（数据类型为'object'的列）\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    \n",
    "    # 使用pandas的get_dummies函数进行独热编码\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    \n",
    "    # 记录新增的列名\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    \n",
    "    return df, new_columns\n",
    "\n",
    "# ==================== 2. 分组聚合函数 ====================\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    \"\"\"\n",
    "    【分组聚合统计函数】\n",
    "    \n",
    "    功能：对数据按指定列分组，并进行多种统计计算\n",
    "    \n",
    "    举例说明：\n",
    "    假设我们有一个客户的信用卡交易记录表：\n",
    "        SK_ID_CURR | 交易金额 | 交易次数\n",
    "        100001     | 500      | 1\n",
    "        100001     | 800      | 1\n",
    "        100002     | 200      | 1\n",
    "    \n",
    "    使用聚合：aggregations = {'交易金额': ['mean', 'max']}\n",
    "    \n",
    "    结果：\n",
    "        SK_ID_CURR | PREFIX_交易金额_MEAN | PREFIX_交易金额_MAX\n",
    "        100001     | 650                | 800\n",
    "        100002     | 200                | 200\n",
    "    \n",
    "    参数：\n",
    "        df_to_agg: 要进行聚合的DataFrame\n",
    "        prefix: 新列名的前缀（用于区分来自不同数据源的特征）\n",
    "        aggregations: 聚合方式字典，如 {'列名': ['mean', 'max', 'sum']}\n",
    "        aggregate_by: 分组依据列，默认按客户ID (SK_ID_CURR) 分组\n",
    "    \n",
    "    返回：\n",
    "        聚合后的DataFrame\n",
    "    \n",
    "    为什么需要聚合？\n",
    "    - 将客户的多条历史记录压缩为单行统计特征\n",
    "    - 提取历史行为的统计规律（平均值、最大值、标准差等）\n",
    "    \"\"\"\n",
    "    # 按指定列分组并进行聚合计算\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    \n",
    "    # 重命名列：格式为 \"前缀+原列名+聚合方式\"\n",
    "    # 例如：BURO_AMT_CREDIT_MAX 表示来自Bureau表的信贷金额的最大值\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    \n",
    "    # 重置索引，将分组列变回普通列\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "# ==================== 3. 分组聚合并合并函数 ====================\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    \"\"\"\n",
    "    【分组聚合后合并到主表】\n",
    "    \n",
    "    功能：先进行分组聚合，然后将结果合并到主表\n",
    "    \n",
    "    这是特征工程中非常常用的操作模式：\n",
    "    1. 对辅助表进行聚合统计\n",
    "    2. 将统计结果作为新特征添加到主表\n",
    "    \n",
    "    举例：\n",
    "    主表（application）：\n",
    "        SK_ID_CURR | 收入\n",
    "        100001     | 50000\n",
    "        100002     | 30000\n",
    "    \n",
    "    辅助表（credit_card）：\n",
    "        SK_ID_CURR | 信用卡余额\n",
    "        100001     | 1000\n",
    "        100001     | 2000\n",
    "        100002     | 500\n",
    "    \n",
    "    聚合并合并后：\n",
    "        SK_ID_CURR | 收入  | CC_信用卡余额_MEAN\n",
    "        100001     | 50000 | 1500\n",
    "        100002     | 30000 | 500\n",
    "    \n",
    "    参数：\n",
    "        df_to_agg: 需要聚合的辅助表\n",
    "        df_to_merge: 主表（将聚合结果合并到这个表）\n",
    "        prefix: 特征前缀\n",
    "        aggregations: 聚合方式\n",
    "        aggregate_by: 分组列\n",
    "    \n",
    "    返回：\n",
    "        合并后的主表（包含新的聚合特征）\n",
    "    \"\"\"\n",
    "    # 调用group函数进行聚合\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
    "    \n",
    "    # 使用左连接将聚合结果合并到主表\n",
    "    # left join确保主表的所有记录都保留\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
    "\n",
    "# ==================== 4. 求和聚合函数 ====================\n",
    "def do_sum(dataframe, group_cols, counted, agg_name):\n",
    "    \"\"\"\n",
    "    【分组求和并添加为新列】\n",
    "    \n",
    "    功能：按指定列分组，对某一列求和，并将结果作为新列添加回原表\n",
    "    \n",
    "    举例：\n",
    "    原始数据：\n",
    "        客户ID | 订单ID | 逾期次数\n",
    "        1001   | A      | 1\n",
    "        1001   | B      | 0\n",
    "        1001   | C      | 2\n",
    "        1002   | D      | 0\n",
    "    \n",
    "    调用：do_sum(df, ['客户ID'], '逾期次数', '总逾期次数')\n",
    "    \n",
    "    结果：\n",
    "        客户ID | 订单ID | 逾期次数 | 总逾期次数\n",
    "        1001   | A      | 1        | 3\n",
    "        1001   | B      | 0        | 3\n",
    "        1001   | C      | 2        | 3\n",
    "        1002   | D      | 0        | 0\n",
    "    \n",
    "    参数：\n",
    "        dataframe: 输入数据\n",
    "        group_cols: 分组列（列表）\n",
    "        counted: 需要求和的列名\n",
    "        agg_name: 新列的名称\n",
    "    \n",
    "    应用场景：\n",
    "    - 计算客户的总逾期次数\n",
    "    - 计算客户的总还款金额\n",
    "    - 统计客户的历史订单总数\n",
    "    \"\"\"\n",
    "    # 按指定列分组并求和\n",
    "    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n",
    "    \n",
    "    # 将求和结果合并回原表\n",
    "    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# ==================== 5. 内存优化函数 ====================\n",
    "def reduce_mem_usage(dataframe):\n",
    "    \"\"\"\n",
    "    【内存使用优化 - 核心技术】\n",
    "    \n",
    "    功能：通过降低数据类型精度来减少内存占用\n",
    "    \n",
    "    原理：\n",
    "    pandas默认使用int64和float64，但很多时候数据的实际范围不需要这么大的存储空间\n",
    "    \n",
    "    数据类型及其范围：\n",
    "    整数类型：\n",
    "    - int8:   -128 到 127\n",
    "    - int16:  -32,768 到 32,767\n",
    "    - int32:  -2,147,483,648 到 2,147,483,647\n",
    "    - int64:  -9,223,372,036,854,775,808 到 9,223,372,036,854,775,807\n",
    "    \n",
    "    浮点数类型：\n",
    "    - float16: 半精度浮点数\n",
    "    - float32: 单精度浮点数\n",
    "    - float64: 双精度浮点数\n",
    "    \n",
    "    举例：\n",
    "    假设某列的值范围是 [0, 100]\n",
    "    - 默认使用int64：每个值占用8字节\n",
    "    - 优化为int8：每个值只占用1字节\n",
    "    - 内存减少：87.5%\n",
    "    \n",
    "    实际效果：\n",
    "    本项目中，内存占用从原来的4倍压缩到现在的大小\n",
    "    这对于Kaggle的16GB内存限制非常重要\n",
    "    \n",
    "    注意事项：\n",
    "    - float16精度较低，可能影响某些计算\n",
    "    - 需要确保数值范围在类型限制内\n",
    "    \"\"\"\n",
    "    # 记录优化前的内存使用（单位：MB）\n",
    "    m_start = dataframe.memory_usage().sum() / 1024 ** 2\n",
    "    \n",
    "    # 遍历每一列\n",
    "    for col in dataframe.columns:\n",
    "        col_type = dataframe[col].dtype\n",
    "        \n",
    "        # 只处理数值类型，跳过object类型（字符串）\n",
    "        if col_type != object:\n",
    "            c_min = dataframe[col].min()  # 列的最小值\n",
    "            c_max = dataframe[col].max()  # 列的最大值\n",
    "            \n",
    "            # 处理整数类型\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # 根据数值范围选择最小的整数类型\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int64)\n",
    "            \n",
    "            # 处理浮点数类型\n",
    "            elif str(col_type)[:5] == 'float':\n",
    "                # 根据数值范围选择合适的浮点数类型\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float32)\n",
    "                else:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float64)\n",
    "    \n",
    "    # 记录优化后的内存使用\n",
    "    m_end = dataframe.memory_usage().sum() / 1024 ** 2\n",
    "    \n",
    "    # 可以打印内存减少的百分比\n",
    "    # print(f'内存使用从 {m_start:.2f} MB 减少到 {m_end:.2f} MB ({100 * (m_start - m_end) / m_start:.1f}% 减少)')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# 全局设置：将缺失值作为一个独立的类别\n",
    "nan_as_category = True\n",
    "\n",
    "\n",
    "# ==================== 6. 风险分组函数 ====================\n",
    "def risk_groupanizer(dataframe, column_names, target_val=1, upper_limit_ratio=8.2, lower_limit_ratio=8.2):\n",
    "    \"\"\"\n",
    "    【风险分组编码器 - 高级特征工程技术】\n",
    "    \n",
    "    功能：根据违约率将类别特征的每个类别标记为高/中/低风险\n",
    "    \n",
    "    核心思想：\n",
    "    不是简单地进行One-Hot编码，而是提取每个类别的\"风险程度\"信息\n",
    "    这种方法比传统One-Hot编码更有信息量，且大幅减少特征数量\n",
    "    \n",
    "    详细举例：\n",
    "    假设有\"职业\"这个类别特征，数据如下：\n",
    "    \n",
    "    职业         | 客户数 | 违约数 | 违约率\n",
    "    工程师       | 1000   | 50     | 5%    <- 低风险\n",
    "    教师         | 800    | 65     | 8.1%  <- 高风险（>=8.2%可调）\n",
    "    医生         | 500    | 40     | 8%    <- 中等风险\n",
    "    无业         | 200    | 30     | 15%   <- 高风险\n",
    "    \n",
    "    传统One-Hot编码会创建4列（职业_工程师, 职业_教师, 职业_医生, 职业_无业）\n",
    "    \n",
    "    风险分组编码只创建2-3列：\n",
    "    - 职业_high_risk: 教师和无业标记为1，其他为0\n",
    "    - 职业_low_risk:  工程师标记为1，其他为0\n",
    "    - 职业_medium_risk: 医生标记为1，其他为0（如果设置了中等风险阈值）\n",
    "    \n",
    "    优势：\n",
    "    1. 特征数量大幅减少（从N个类别减少到2-3个）\n",
    "    2. 包含了违约率信息，更有预测力\n",
    "    3. 避免了高基数类别特征的维度爆炸问题\n",
    "    4. 对模型来说，高/低风险标记比具体类别值更有意义\n",
    "    \n",
    "    参数：\n",
    "        dataframe: 输入数据（必须包含TARGET列）\n",
    "        column_names: 需要进行风险分组的列名列表\n",
    "        target_val: 目标值（1表示违约）\n",
    "        upper_limit_ratio: 高风险阈值（默认8.2%，即违约率>=8.2%为高风险）\n",
    "        lower_limit_ratio: 低风险阈值（默认8.2%，即违约率<=8.2%为低风险）\n",
    "    \n",
    "    返回：\n",
    "        dataframe: 添加了风险标记列的数据\n",
    "        new_columns: 新增的列名列表\n",
    "    \n",
    "    实际应用：\n",
    "    在信用评分中，某些职业、地区、收入类型确实有显著不同的违约倾向\n",
    "    这个函数能自动识别并标记这些高风险群体\n",
    "    \"\"\"\n",
    "    # one-hot encoder killer :-) \n",
    "    # 注释：这个方法是One-Hot编码的\"杀手\"，因为它能用更少的特征表达更多的信息\n",
    "    \n",
    "    # 保存原始列名\n",
    "    all_cols = dataframe.columns\n",
    "    \n",
    "    # 遍历每个需要处理的类别列\n",
    "    for col in column_names:\n",
    "        \n",
    "        # 步骤1：计算每个类别的违约率\n",
    "        # 按类别和TARGET分组，统计客户数量\n",
    "        temp_df = dataframe.groupby([col] + ['TARGET'])[['SK_ID_CURR']].count().reset_index()\n",
    "        \n",
    "        # 计算违约率百分比\n",
    "        # 公式：(该类别违约数 / 该类别总数) * 100\n",
    "        temp_df['ratio%'] = round(temp_df['SK_ID_CURR']*100/temp_df.groupby([col])['SK_ID_CURR'].transform('sum'), 1)\n",
    "        \n",
    "        # 步骤2：识别高风险类别\n",
    "        # 筛选出违约率 >= upper_limit_ratio 的类别\n",
    "        col_groups_high_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                                       (temp_df['ratio%'] >= upper_limit_ratio)][col].tolist()\n",
    "        \n",
    "        # 步骤3：识别低风险类别\n",
    "        # 筛选出违约率 <= lower_limit_ratio 的类别\n",
    "        col_groups_low_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                                      (lower_limit_ratio >= temp_df['ratio%'])][col].tolist()\n",
    "        \n",
    "        # 步骤4：如果设置了不同的上下限阈值，还可以识别中等风险类别\n",
    "        if upper_limit_ratio != lower_limit_ratio:\n",
    "            col_groups_medium_risk = temp_df[(temp_df['TARGET'] == target_val) &\n",
    "                (upper_limit_ratio > temp_df['ratio%']) & (temp_df['ratio%'] > lower_limit_ratio)][col].tolist()\n",
    "            \n",
    "            # 创建三个风险标记列\n",
    "            for risk, col_groups in zip(['_high_risk', '_medium_risk', '_low_risk'],\n",
    "                                        [col_groups_high_risk, col_groups_medium_risk, col_groups_low_risk]):\n",
    "                # 如果该样本的类别值在对应风险组中，标记为1，否则为0\n",
    "                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n",
    "        else:\n",
    "            # 如果上下限相同，只创建高风险和低风险两列\n",
    "            for risk, col_groups in zip(['_high_risk', '_low_risk'], [col_groups_high_risk, col_groups_low_risk]):\n",
    "                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n",
    "        \n",
    "        # 步骤5：删除原始的类别列（因为已经转换为风险标记列）\n",
    "        if dataframe[col].dtype == 'O' or dataframe[col].dtype == 'object':\n",
    "            dataframe.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # 返回处理后的数据和新增的列名列表\n",
    "    return dataframe, list(set(dataframe.columns).difference(set(all_cols)))\n",
    "\n",
    "\n",
    "# ==================== 7. LightGBM特征选择函数 ====================\n",
    "def ligthgbm_feature_selection(dataframe, index_cols, auc_limit=0.7):\n",
    "    \"\"\"\n",
    "    【基于LightGBM的特征选择 - 迭代式特征筛选】\n",
    "    \n",
    "    功能：使用LightGBM模型自动筛选重要特征，删除对预测无贡献的特征\n",
    "    \n",
    "    核心原理：\n",
    "    LightGBM在训练过程中会计算每个特征的重要性（feature importance）\n",
    "    - 重要性 > 0: 该特征对模型有贡献\n",
    "    - 重要性 = 0: 该特征从未被模型使用，可以删除\n",
    "    \n",
    "    算法流程：\n",
    "    1. 使用所有特征训练LightGBM模型\n",
    "    2. 计算每个特征的重要性\n",
    "    3. 删除重要性为0的特征\n",
    "    4. 重复步骤1-3，直到：\n",
    "       - 所有特征都有贡献（重要性>0），或\n",
    "       - 模型AUC分数下降到阈值以下（说明删除太多了）\n",
    "    \n",
    "    举例说明：\n",
    "    假设有100个特征：\n",
    "    \n",
    "    第1轮：\n",
    "    - 训练模型，AUC=0.85\n",
    "    - 发现20个特征重要性=0，删除\n",
    "    - 剩余80个特征\n",
    "    \n",
    "    第2轮：\n",
    "    - 使用80个特征训练，AUC=0.84\n",
    "    - 发现10个特征重要性=0，删除\n",
    "    - 剩余70个特征\n",
    "    \n",
    "    第3轮：\n",
    "    - 使用70个特征训练，AUC=0.68 < 0.7（达到阈值）\n",
    "    - 停止删除，保留80个特征\n",
    "    \n",
    "    优势：\n",
    "    1. 自动化特征选择，无需人工判断\n",
    "    2. 删除冗余特征，减少过拟合风险\n",
    "    3. 减少特征数量，提高训练速度\n",
    "    4. 基于模型本身判断，更加可靠\n",
    "    \n",
    "    参数：\n",
    "        dataframe: 包含所有特征的数据集\n",
    "        index_cols: 索引列（如SK_ID_CURR），不参与特征选择\n",
    "        auc_limit: AUC阈值，低于此值时停止删除特征\n",
    "    \n",
    "    返回：\n",
    "        筛选后的dataframe（删除了无贡献特征）\n",
    "    \n",
    "    注意：\n",
    "    本项目中由于内存限制，实际使用时读取了预先计算好的特征列表\n",
    "    \"\"\"\n",
    "    # 清理特征名称，确保只包含字母、数字和下划线\n",
    "    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n",
    "    \n",
    "    # 初始化LightGBM分类器\n",
    "    clf = LGBMClassifier(random_state=0)\n",
    "    \n",
    "    # 分离训练集（有TARGET标签的数据）\n",
    "    train_df = dataframe[dataframe['TARGET'].notnull()]\n",
    "    train_df_X = train_df.drop('TARGET', axis=1)  # 特征\n",
    "    train_df_y = train_df['TARGET']  # 标签\n",
    "    \n",
    "    # 获取所有参与训练的列（排除索引列）\n",
    "    train_columns = [col for col in train_df_X.columns if col not in index_cols]\n",
    "    \n",
    "    # 初始化：假设当前AUC为1，最优特征集为空\n",
    "    max_auc_score = 1\n",
    "    best_cols = []\n",
    "    \n",
    "    # 迭代删除无用特征，直到AUC低于阈值\n",
    "    while max_auc_score > auc_limit:\n",
    "        # 排除已经确定要保留的特征\n",
    "        train_columns = [col for col in train_columns if col not in best_cols]\n",
    "        \n",
    "        # 使用当前特征集训练模型\n",
    "        clf.fit(train_df_X[train_columns], train_df_y)\n",
    "        \n",
    "        # 获取每个特征的重要性\n",
    "        feats_imp = pd.Series(clf.feature_importances_, index=train_columns)\n",
    "        \n",
    "        # 计算当前模型的AUC分数\n",
    "        max_auc_score = roc_auc_score(train_df_y, clf.predict_proba(train_df_X[train_columns])[:, 1])\n",
    "        \n",
    "        # 保留重要性大于0的特征（有贡献的特征）\n",
    "        best_cols = feats_imp[feats_imp > 0].index.tolist()\n",
    "    \n",
    "    # 删除被筛选掉的特征（无贡献的特征）\n",
    "    dataframe.drop(train_columns, axis=1, inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-trigger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.186033Z",
     "iopub.status.busy": "2021-05-04T15:38:40.184341Z",
     "iopub.status.idle": "2021-05-04T15:38:40.186723Z",
     "shell.execute_reply": "2021-05-04T15:38:40.187221Z"
    },
    "papermill": {
     "duration": 0.045598,
     "end_time": "2021-05-04T15:38:40.187389",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.141791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【主申请表特征工程函数】\n",
    "处理application_train.csv和application_test.csv\n",
    "这是主数据表，包含客户的基本信息和申请信息\n",
    "\"\"\"\n",
    "def application():\n",
    "    \"\"\"\n",
    "    处理主申请表数据，创建大量衍生特征\n",
    "    \n",
    "    数据表说明：\n",
    "    - application_train.csv: 训练集，包含TARGET（是否违约）\n",
    "    - application_test.csv: 测试集，需要预测TARGET\n",
    "    \n",
    "    主要特征类型：\n",
    "    1. 基本信息：性别、年龄、家庭成员数等\n",
    "    2. 财务信息：收入、信贷金额、年金等\n",
    "    3. 外部评分：EXT_SOURCE_1/2/3（来自外部数据源的信用评分）\n",
    "    4. 时间信息：就业天数、出生日期等\n",
    "    \"\"\"\n",
    "    # ==================== 数据加载和合并 ====================\n",
    "    # 读取训练集和测试集\n",
    "    df = pd.read_csv(r'../input/home-credit-default-risk/application_train.csv')\n",
    "    test_df = pd.read_csv(r'../input/home-credit-default-risk/application_test.csv')\n",
    "    \n",
    "    # 合并训练集和测试集，便于统一处理特征工程\n",
    "    # 注意：测试集的TARGET列为NaN\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # ==================== 数据清洗 ====================\n",
    "    # 1. 删除性别异常值\n",
    "    # CODE_GENDER='XNA'表示性别未知，这类样本数量极少且可能有问题\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # 2. 删除收入异常值\n",
    "    # 有一个离群点收入为117M（1.17亿），明显是错误数据\n",
    "    df = df[df['AMT_INCOME_TOTAL'] < 20000000]\n",
    "    \n",
    "    # 3. 处理DAYS_EMPLOYED的异常值\n",
    "    # 365243天（约1000年）是一个占位符，表示缺失值\n",
    "    # 将其转换为NaN，让模型能正确处理\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # 4. 处理DAYS_LAST_PHONE_CHANGE的异常值\n",
    "    # 0表示从未更换过电话，在这个数据集中被视为异常\n",
    "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # ==================== 类别特征编码 ====================\n",
    "    # 1. 二元类别特征的标签编码\n",
    "    # 对于只有两个类别的特征，使用简单的0/1编码即可\n",
    "    # 例如：性别（男/女）、是否有车（是/否）、是否有房产（是/否）\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    \n",
    "    # 2. 多类别特征的One-Hot编码\n",
    "    # 对于有多个类别的特征（如职业类型、教育程度等），使用One-Hot编码\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # ==================== 文档特征聚合 ====================\n",
    "    # FLAG_DOC_X 系列特征表示客户是否提供了某种文档\n",
    "    # 例如：FLAG_DOC_2=1表示提供了2号文档，=0表示未提供\n",
    "    \n",
    "    # 统计客户提供的文档总数\n",
    "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "    \n",
    "    # 计算文档提交的峰度（kurtosis）\n",
    "    # 峰度衡量分布的尖锐程度，可以反映客户提供文档的集中程度\n",
    "    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
    "\n",
    "    # ==================== 年龄分组特征 ====================\n",
    "    def get_age_label(days_birth):\n",
    "        \"\"\"\n",
    "        将年龄转换为离散的年龄组标签\n",
    "        \n",
    "        年龄分组策略基于违约率分析：\n",
    "        - 不同年龄段的违约率差异显著\n",
    "        - 将连续的年龄变量转换为类别变量\n",
    "        - 有助于模型捕捉非线性的年龄效应\n",
    "        \"\"\"\n",
    "        # DAYS_BIRTH是负数（表示多少天前出生），转换为实际年龄\n",
    "        age_years = -days_birth / 365\n",
    "        \n",
    "        # 根据违约率分布将年龄分为5组\n",
    "        if age_years < 27: return 1      # 年轻群体（高风险）\n",
    "        elif age_years < 40: return 2    # 青年群体\n",
    "        elif age_years < 50: return 3    # 中年群体\n",
    "        elif age_years < 65: return 4    # 中老年群体\n",
    "        elif age_years < 99: return 5    # 老年群体（低风险）\n",
    "        else: return 0                   # 异常值\n",
    "    \n",
    "    # 应用年龄分组函数\n",
    "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "\n",
    "    # ==================== 外部信用评分特征工程 ====================\n",
    "    # EXT_SOURCE_1/2/3 是来自外部数据源的标准化信用评分\n",
    "    # 这些是最重要的特征之一，需要充分挖掘它们的信息\n",
    "    \n",
    "    # 1. 三个评分的乘积\n",
    "    # 如果客户在所有评分源都得高分，乘积会很大\n",
    "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    \n",
    "    # 2. 加权平均分\n",
    "    # 根据经验，EXT_SOURCE_3的权重最高（权重=3）\n",
    "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "    \n",
    "    # 忽略全NaN切片的警告\n",
    "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    \n",
    "    # 3. 计算三个评分的统计特征\n",
    "    # 这些统计量能反映评分的一致性和稳定性\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "        # min: 最低评分（衡量最差情况）\n",
    "        # max: 最高评分（衡量最好情况）\n",
    "        # mean: 平均评分（综合评价）\n",
    "        # nanmedian: 中位数（对异常值更稳健）\n",
    "        # var: 方差（评分的一致性，方差大说明评分差异大）\n",
    "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
    "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "    # ==================== 基础比率特征 ====================\n",
    "    # 比率特征（Ratio Features）在信用评分中非常重要\n",
    "    # 它们能揭示不同变量之间的相对关系，比绝对值更有信息量\n",
    "    \n",
    "    # 1. 就业占生命比例\n",
    "    # 衡量客户工作经历占年龄的比例\n",
    "    # 例如：30岁工作了10年，比例=10/30=0.33\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 2. 收入与信贷比例\n",
    "    # 衡量还款能力：收入越高、贷款额越少，比例越大，风险越低\n",
    "    # 例如：年收入100万，贷款50万，比例=2.0（还款能力强）\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # 3. 人均收入\n",
    "    # 家庭总收入除以家庭成员数\n",
    "    # 反映实际可支配收入水平\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # 4. 年金占收入比例（负债收入比）\n",
    "    # 每年还款金额占年收入的比例\n",
    "    # 比例越高，还款压力越大，违约风险越高\n",
    "    # 例如：年收入50万，年还款10万，比例=0.2（20%，压力适中）\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # 5. 还款率\n",
    "    # 年金占贷款总额的比例，反映还款计划的松紧程度\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "    # ==================== 信贷相关比率 ====================\n",
    "    # 信贷金额与商品价格的比率\n",
    "    # 如果比率>1，说明贷款额超过商品价值（可能包含其他费用）\n",
    "    # 如果比率<1，说明客户支付了首付\n",
    "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    # ==================== 收入相关比率 ====================\n",
    "    # 1. 收入与就业时长比率\n",
    "    # 每工作一天的平均收入，反映收入增长速度\n",
    "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 2. 收入与年龄比率\n",
    "    # 衡量收入增长率，年轻人高收入说明发展潜力大\n",
    "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # ==================== 时间相关比率 ====================\n",
    "    # 这些比率反映客户生活稳定性\n",
    "    \n",
    "    # 1. 身份证发布时间占年龄比例\n",
    "    # 比例越小，说明身份证是最近才办的\n",
    "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 2. 车龄占年龄比例\n",
    "    # 反映车辆新旧程度相对于年龄的关系\n",
    "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 3. 车龄占就业时长比例\n",
    "    # 车龄接近就业时长说明刚工作就买车，可能负债较重\n",
    "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 4. 手机更换时间占年龄比例\n",
    "    # 经常换手机可能反映生活不稳定\n",
    "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "\n",
    "    # ==================== 外部评分的高级组合特征 ====================\n",
    "    # 深度挖掘EXT_SOURCE系列特征，这些特征是预测力最强的\n",
    "    \n",
    "    # 1. 外部评分的均值和标准差\n",
    "    df['APPS_EXT_SOURCE_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['APPS_EXT_SOURCE_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    # 标准差的缺失值用均值填充（当只有1-2个评分时会出现NaN）\n",
    "    df['APPS_EXT_SOURCE_STD'] = df['APPS_EXT_SOURCE_STD'].fillna(df['APPS_EXT_SOURCE_STD'].mean())\n",
    "    \n",
    "    # 2. 评分与年龄的比率\n",
    "    # 衡量：在相同年龄下的信用评分水平\n",
    "    df['APP_SCORE1_TO_BIRTH_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    df['APP_SCORE2_TO_BIRTH_RATIO'] = df['EXT_SOURCE_2'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    df['APP_SCORE3_TO_BIRTH_RATIO'] = df['EXT_SOURCE_3'] / (df['DAYS_BIRTH'] / 365.25)\n",
    "    \n",
    "    # 3. 评分与就业时长的比率\n",
    "    # 工作时间短但评分高，说明客户潜力大\n",
    "    df['APP_SCORE1_TO_EMPLOY_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_EMPLOYED'] / 365.25)\n",
    "    \n",
    "    # 4. 评分的三元交互特征\n",
    "    # 捕捉评分与时间的复杂交互关系\n",
    "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['DAYS_BIRTH']\n",
    "    \n",
    "    # 5. 评分1与其他变量的比率\n",
    "    df['APP_SCORE1_TO_FAM_CNT_RATIO'] = df['EXT_SOURCE_1'] / df['CNT_FAM_MEMBERS']\n",
    "    df['APP_SCORE1_TO_GOODS_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_GOODS_PRICE']\n",
    "    df['APP_SCORE1_TO_CREDIT_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # 6. 评分之间的比率\n",
    "    # 如果各评分差异大，可能反映信息不一致\n",
    "    df['APP_SCORE1_TO_SCORE2_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_2']\n",
    "    df['APP_SCORE1_TO_SCORE3_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_3']\n",
    "    \n",
    "    # 7. 评分2与其他变量的比率\n",
    "    df['APP_SCORE2_TO_CREDIT_RATIO'] = df['EXT_SOURCE_2'] / df['AMT_CREDIT']\n",
    "    df['APP_SCORE2_TO_REGION_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT']\n",
    "    df['APP_SCORE2_TO_CITY_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT_W_CITY']\n",
    "    df['APP_SCORE2_TO_POP_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_POPULATION_RELATIVE']\n",
    "    df['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = df['EXT_SOURCE_2'] / df['DAYS_LAST_PHONE_CHANGE']\n",
    "    \n",
    "    # 8. 评分之间的两两乘积（交互特征）\n",
    "    # 捕捉不同评分源之间的协同效应\n",
    "    df['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n",
    "    df['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n",
    "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    \n",
    "    # 9. 评分与就业天数的交互\n",
    "    # 高评分+长工作时间 = 更稳定\n",
    "    df['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n",
    "    df['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n",
    "    df['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n",
    "\n",
    "    # ==================== 收入与家庭相关特征 ====================\n",
    "    # 1. 商品价格占收入比例\n",
    "    # 反映购买力：比例越低，说明商品相对便宜，还款压力小\n",
    "    df['APPS_GOODS_INCOME_RATIO'] = df['AMT_GOODS_PRICE'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # 2. 家庭人均收入（重复特征，与前面INCOME_PER_PERSON相同）\n",
    "    df['APPS_CNT_FAM_INCOME_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # 3. 收入与就业时长比率\n",
    "    # 平均每工作一天获得的收入，反映收入增长速度\n",
    "    df['APPS_INCOME_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "\n",
    "    # ==================== 从高分模型借鉴的额外特征 ====================\n",
    "    # 以下特征来自AUC>0.8的模型，证明有较强预测力\n",
    "    \n",
    "    # 1. 信贷商品比率（重复特征，增强该特征的重要性）\n",
    "    df['CREDIT_TO_GOODS_RATIO_2'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    # 2. 月收入减去年金（可支配收入）\n",
    "    # 计算每月还款后的剩余收入\n",
    "    # 例如：月收入5万，年还款12万（月均1万），剩余收入=5-1=4万\n",
    "    df['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']\n",
    "    \n",
    "    # 3. 收入就业比（重复特征）\n",
    "    df['APP_INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 4. 手机更换频率与就业时长比\n",
    "    # 手机更换频繁可能反映生活不稳定\n",
    "    df['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    # 5. 就业时长与年龄差\n",
    "    # 差值大说明很晚才开始工作，可能影响收入积累\n",
    "    df['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n",
    "\n",
    "    # 打印最终数据形状\n",
    "    print('\"Application_Train_Test\" final shape:', df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-atlas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.236779Z",
     "iopub.status.busy": "2021-05-04T15:38:40.235180Z",
     "iopub.status.idle": "2021-05-04T15:38:40.237499Z",
     "shell.execute_reply": "2021-05-04T15:38:40.237984Z"
    },
    "papermill": {
     "duration": 0.038152,
     "end_time": "2021-05-04T15:38:40.238153",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.200001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【信用局数据特征工程函数】\n",
    "处理bureau.csv和bureau_balance.csv\n",
    "包含客户在其他金融机构的信贷历史记录\n",
    "\"\"\"\n",
    "def bureau_bb():\n",
    "    \"\"\"\n",
    "    信用局(Bureau)数据说明：\n",
    "    - bureau.csv: 客户在其他金融机构的所有历史信贷记录\n",
    "    - bureau_balance.csv: 这些信贷的月度余额变化\n",
    "    \n",
    "    为什么信用局数据重要？\n",
    "    1. 反映客户的整体信用状况（不仅是本公司的）\n",
    "    2. 历史还款行为是违约风险的最佳预测因子\n",
    "    3. 多头借贷情况（在多家机构借款）\n",
    "    \"\"\"\n",
    "    # 读取两个相关表\n",
    "    bureau = pd.read_csv(r'../input/home-credit-default-risk/bureau.csv')\n",
    "    bb = pd.read_csv(r'../input/home-credit-default-risk/bureau_balance.csv')\n",
    "\n",
    "    # ==================== 时间相关特征 ====================\n",
    "    # 1. 信贷持续时间\n",
    "    # DAYS_CREDIT: 该笔信贷开始前多少天\n",
    "    # DAYS_CREDIT_ENDDATE: 该笔信贷预计结束前多少天\n",
    "    # 持续时间 = 结束日期 - 开始日期\n",
    "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "    \n",
    "    # 2. 预计结束日期与实际结束日期差异\n",
    "    # 正值：提前还清（信用好）\n",
    "    # 负值：延期还款（信用差）\n",
    "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    \n",
    "    # ==================== 债务相关特征 ====================\n",
    "    # 1. 债务比例\n",
    "    # 总信贷额度 / 当前债务，比值越大说明还款越多\n",
    "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    \n",
    "    # 2. 已还款金额\n",
    "    # 总信贷 - 当前债务 = 已还金额\n",
    "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    \n",
    "    # 3. 信贷与年金比率\n",
    "    # 反映还款计划的合理性\n",
    "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']\n",
    "    \n",
    "    # 4. 信贷时间差异特征\n",
    "    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "    \n",
    "    # 5. 当前债务占总信贷比例\n",
    "    # 比例越低说明还得越多，信用越好\n",
    "    bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / bureau['AMT_CREDIT_SUM']\n",
    "\n",
    "    # ==================== 逾期特征（DPD = Days Past Due）====================\n",
    "    # DPD是信用评分中最重要的指标之一\n",
    "    \n",
    "    # 1. 是否有过逾期\n",
    "    # CREDIT_DAY_OVERDUE: 最大逾期天数\n",
    "    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # 2. 是否有严重逾期（超过120天）\n",
    "    # 120天是一个关键阈值，通常被认为是严重违约\n",
    "    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n",
    "\n",
    "    # ==================== 类别特征编码 ====================\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # ==================== Bureau Balance聚合 ====================\n",
    "    # bureau_balance表记录了每笔信贷的月度状态\n",
    "    # 需要将多个月的记录聚合到每笔信贷上\n",
    "    \n",
    "    # 定义聚合方式\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size', 'mean']}\n",
    "    # min: 最早的月份记录（信贷开始时间）\n",
    "    # max: 最近的月份记录（当前状态）\n",
    "    # size: 记录总数（信贷持续月数）\n",
    "    # mean: 平均月份（时间跨度中点）\n",
    "    \n",
    "    # 对所有类别特征计算平均值\n",
    "    # 例如：STATUS_C (Current/当前)的均值反映了还款状态的稳定性\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "\n",
    "    # 按信贷ID聚合bureau_balance数据\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    \n",
    "    # 将聚合后的月度统计信息合并到bureau表\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "    # ==================== 按客户汇总所有信贷记录 ====================\n",
    "    # 每个客户可能在多家机构有多笔信贷\n",
    "    # 需要将所有信贷记录聚合到客户层面\n",
    "    \n",
    "    # 数值特征的聚合方式\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],  # 信贷开始时间的分布\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],  # 信贷结束时间\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],  # 信贷信息更新时间\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean', 'min'],  # 逾期天数（最严重、平均、最轻）\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],  # 最大逾期金额\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],  # 信贷总额（最大单笔、平均、累计）\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],  # 当前债务\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean', 'max', 'sum'],  # 逾期金额\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],  # 信用额度\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],  # 年金\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],  # 展期次数（延长还款期限）\n",
    "        'MONTHS_BALANCE_MIN': ['min'],  # 最早记录月份\n",
    "        'MONTHS_BALANCE_MAX': ['max'],  # 最近记录月份\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],  # 记录月数\n",
    "        'SK_ID_BUREAU': ['count'],  # 信贷总笔数\n",
    "        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],  # 实际结束日期\n",
    "        'ENDDATE_DIF': ['min', 'max', 'mean'],  # 日期差异\n",
    "        'BUREAU_CREDIT_FACT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean'],  # 债务比率\n",
    "        'DEBT_CREDIT_DIFF': ['min', 'max', 'mean'],  # 已还金额\n",
    "        'BUREAU_IS_DPD': ['mean', 'sum'],  # 有逾期的信贷占比和数量\n",
    "        'BUREAU_IS_DPD_OVER120': ['mean', 'sum']  # 严重逾期的占比和数量\n",
    "        }\n",
    "\n",
    "    # 类别特征的聚合方式（计算平均值）\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    # 执行聚合，将所有信贷记录汇总到客户层面\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    # ==================== 活跃信贷单独聚合 ====================\n",
    "    # 活跃的信贷(Active)与历史信贷的特征可能很不同\n",
    "    # 分别统计可以提供更精细的信息\n",
    "    \n",
    "    # 筛选活跃状态的信贷\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # ==================== 已关闭信贷单独聚合 ====================\n",
    "    # 已结清的信贷能反映客户的历史还款能力\n",
    "    \n",
    "    # 筛选已关闭状态的信贷\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print('\"Bureau/Bureau Balance\" final shape:', bureau_agg.shape)\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-zealand",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.285317Z",
     "iopub.status.busy": "2021-05-04T15:38:40.283608Z",
     "iopub.status.idle": "2021-05-04T15:38:40.286025Z",
     "shell.execute_reply": "2021-05-04T15:38:40.286499Z"
    },
    "papermill": {
     "duration": 0.035905,
     "end_time": "2021-05-04T15:38:40.286660",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.250755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【历史申请数据处理函数】\n",
    "处理客户在Home Credit的历史申请记录\n",
    "\"\"\"\n",
    "def previous_application():\n",
    "    \"\"\"\n",
    "    历史申请数据说明：\n",
    "    - 包含客户过去所有的贷款申请记录\n",
    "    - 可能包括：已批准、已拒绝、已取消、未使用的申请\n",
    "    \n",
    "    关键特征：\n",
    "    1. 申请金额 vs 实际批准金额的比率（议价能力）\n",
    "    2. 历史拒绝率（风险信号）\n",
    "    3. 首付比例（财务实力）\n",
    "    4. 简单利率（还款成本）\n",
    "    \"\"\"\n",
    "    prev = pd.read_csv(r'../input/home-credit-default-risk/previous_application.csv')\n",
    "\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Feature engineering: ratios and difference\n",
    "    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\n",
    "    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "\n",
    "    # Interest ratio on previous application (simplified)\n",
    "    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    prev['SIMPLE_INTERESTS'] = (total_payment / prev['AMT_CREDIT'] - 1) / prev['CNT_PAYMENT']\n",
    "\n",
    "    # Days last due difference (scheduled x done)\n",
    "    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "\n",
    "    # from off\n",
    "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
    "    prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
    "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE'] / prev['AMT_APPLICATION']\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'DAYS_TERMINATION': ['max'],\n",
    "        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n",
    "        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n",
    "        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n",
    "        'SIMPLE_INTERESTS': ['mean', 'max']\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print('\"Previous Applications\" final shape:', prev_agg.shape)\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-somerset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.321393Z",
     "iopub.status.busy": "2021-05-04T15:38:40.320660Z",
     "iopub.status.idle": "2021-05-04T15:38:40.333793Z",
     "shell.execute_reply": "2021-05-04T15:38:40.334289Z"
    },
    "papermill": {
     "duration": 0.035205,
     "end_time": "2021-05-04T15:38:40.334464",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.299259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【POS分期付款数据处理函数】\n",
    "处理销售点(Point of Sale)分期付款记录\n",
    "\"\"\"\n",
    "def pos_cash():\n",
    "    \"\"\"\n",
    "    POS_CASH数据说明：\n",
    "    - 记录客户在商店的分期付款记录（如购买手机、家电等）\n",
    "    - 包含月度余额和还款状态\n",
    "    \n",
    "    关键特征：\n",
    "    1. DPD (Days Past Due): 逾期天数统计\n",
    "    2. 是否提前还清（信用好的信号）\n",
    "    3. 剩余分期数和比例（当前负债情况）\n",
    "    4. 最近3次申请的逾期情况（最新行为模式）\n",
    "    \n",
    "    为什么重要？\n",
    "    - 小额分期也能反映还款习惯\n",
    "    - 逾期行为是违约的强预测因子\n",
    "    \"\"\"\n",
    "    pos = pd.read_csv(r'../input/home-credit-default-risk/POS_CASH_balance.csv')\n",
    "\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    # Flag months with late payment\n",
    "    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) # <-- same with ['LATE_PAYMENT']\n",
    "    pos['POS_IS_DPD_UNDER_120'] = pos['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    pos['POS_IS_DPD_OVER_120'] = pos['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n",
    "        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n",
    "        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'LATE_PAYMENT': ['mean'],\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n",
    "        'POS_IS_DPD': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "\n",
    "\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.groupby('SK_ID_PREV')\n",
    "    df_pos = pd.DataFrame()\n",
    "    df_pos['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n",
    "    df_pos['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n",
    "\n",
    "    # Percentage of previous loans completed and completed before initial term\n",
    "    df_pos['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = df_pos.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 \\\n",
    "                                                                      and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n",
    "    # Number of remaining installments (future installments) and percentage from total\n",
    "    df_pos['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n",
    "    df_pos['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n",
    "\n",
    "    # Group by SK_ID_CURR and merge\n",
    "    df_gp = df_pos.groupby('SK_ID_CURR').sum().reset_index()\n",
    "    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n",
    "    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n",
    "\n",
    "    # Percentage of late payments for the 3 most recent applications\n",
    "    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n",
    "\n",
    "    # Last month of each application\n",
    "    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "\n",
    "    # Most recent applications (last 3)\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Pos-Cash\" balance final shape:', pos_agg.shape) \n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-reference",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.363616Z",
     "iopub.status.busy": "2021-05-04T15:38:40.362967Z",
     "iopub.status.idle": "2021-05-04T15:38:40.386396Z",
     "shell.execute_reply": "2021-05-04T15:38:40.386982Z"
    },
    "papermill": {
     "duration": 0.039716,
     "end_time": "2021-05-04T15:38:40.387171",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.347455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【分期还款历史处理函数】\n",
    "处理客户历史贷款的每期还款记录\n",
    "\"\"\"\n",
    "def installment():\n",
    "    \"\"\"\n",
    "    Installments数据说明：\n",
    "    - 记录每笔贷款的每期还款详情\n",
    "    - 包含应还金额、实际还款金额、还款日期等\n",
    "    \n",
    "    核心特征工程：\n",
    "    1. DPD (Days Past Due): 逾期天数\n",
    "       - DPD=0: 按时还款\n",
    "       - DPD>0: 逾期天数\n",
    "       - DPD>120: 严重逾期\n",
    "    \n",
    "    2. DBD (Days Before Due): 提前还款天数\n",
    "       - 提前还款是信用好的信号\n",
    "    \n",
    "    3. Payment Ratio: 实际还款/应还金额\n",
    "       - >1: 还多了（信用好）\n",
    "       - <1: 还少了（逾期）\n",
    "    \n",
    "    4. 最近365天的行为模式\n",
    "       - 最近行为比历史行为更重要\n",
    "    \n",
    "    为什么最重要？\n",
    "    - 直接反映还款行为，是最强的违约预测因子\n",
    "    - 逾期频率、严重程度都有区分度\n",
    "    \"\"\"\n",
    "    ins = pd.read_csv(r'../input/home-credit-default-risk/installments_payments.csv')\n",
    "\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    # Group payments and get Payment difference\n",
    "    ins = do_sum(ins, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n",
    "    ins['PAYMENT_DIFFERENCE'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAYMENT_RATIO'] = ins['AMT_INSTALMENT'] / ins['AMT_PAYMENT_GROUPED']\n",
    "    ins['PAID_OVER_AMOUNT'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT']\n",
    "    ins['PAID_OVER'] = (ins['PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD_diff'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD_diff'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Flag late payment\n",
    "    ins['LATE_PAYMENT'] = ins['DBD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    ins['INSTALMENT_PAYMENT_RATIO'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['LATE_PAYMENT_RATIO'] = ins.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "\n",
    "    # Flag late payments that have a significant amount\n",
    "    ins['SIGNIFICANT_LATE_PAYMENT'] = ins['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    \n",
    "    # Flag k threshold late payments\n",
    "    ins['DPD_7'] = ins['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "    ins['DPD_15'] = ins['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n",
    "\n",
    "    ins['INS_IS_DPD_UNDER_120'] = ins['DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    ins['INS_IS_DPD_OVER_120'] = ins['DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum', 'var'],\n",
    "        'DBD': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'SK_ID_PREV': ['size', 'nunique'],\n",
    "        'PAYMENT_DIFFERENCE': ['mean'],\n",
    "        'PAYMENT_RATIO': ['mean', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'LATE_PAYMENT_RATIO': ['mean'],\n",
    "        'DPD_7': ['mean'],\n",
    "        'DPD_15': ['mean'],\n",
    "        'PAID_OVER': ['mean'],\n",
    "        'DPD_diff':['mean', 'min', 'max'],\n",
    "        'DBD_diff':['mean', 'min', 'max'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # from oof (DAYS_ENTRY_PAYMENT)\n",
    "    cond_day = ins['DAYS_ENTRY_PAYMENT'] >= -365\n",
    "    ins_d365_grp = ins[cond_day].groupby('SK_ID_CURR')\n",
    "    ins_d365_agg_dict = {\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'PAYMENT_DIFF': ['mean', 'min', 'max', 'sum'],\n",
    "        'PAYMENT_PERC': ['mean', 'max'],\n",
    "        'DPD_diff': ['mean', 'min', 'max'],\n",
    "        'DPD': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']}\n",
    "\n",
    "    ins_d365_agg = ins_d365_grp.agg(ins_d365_agg_dict)\n",
    "    ins_d365_agg.columns = ['INS_D365' + ('_').join(column).upper() for column in ins_d365_agg.columns.ravel()]\n",
    "\n",
    "    ins_agg = ins_agg.merge(ins_d365_agg, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Installments Payments\" final shape:', ins_agg.shape)\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-colleague",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.417741Z",
     "iopub.status.busy": "2021-05-04T15:38:40.417044Z",
     "iopub.status.idle": "2021-05-04T15:38:40.433026Z",
     "shell.execute_reply": "2021-05-04T15:38:40.432335Z"
    },
    "papermill": {
     "duration": 0.032819,
     "end_time": "2021-05-04T15:38:40.433176",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.400357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【信用卡余额数据处理函数】\n",
    "处理客户信用卡的月度余额和使用情况\n",
    "\"\"\"\n",
    "def credit_card():\n",
    "    \"\"\"\n",
    "    信用卡数据说明：\n",
    "    - 记录客户信用卡的月度使用和还款情况\n",
    "    - 包含余额、取现、最低还款额等信息\n",
    "    \n",
    "    核心特征工程：\n",
    "    1. 信用额度使用率 (LIMIT_USE)\n",
    "       - 使用率高表示资金紧张，违约风险高\n",
    "       - 例如：额度1万，用了9千，使用率=90%（高风险）\n",
    "    \n",
    "    2. 还款充足性 (PAYMENT_DIV_MIN)\n",
    "       - 实际还款/最低还款\n",
    "       - >1: 还得比最低多（良好）\n",
    "       - ≈1: 只还最低还款（警告信号）\n",
    "       - <1: 连最低都没还够（高风险）\n",
    "    \n",
    "    3. 取现行为 (DRAWING_LIMIT_RATIO)\n",
    "       - 信用卡取现通常利息很高\n",
    "       - 频繁取现说明现金流紧张\n",
    "    \n",
    "    4. 逾期标记\n",
    "       - 一般逾期 (0-120天)\n",
    "       - 严重逾期 (>120天)\n",
    "    \n",
    "    5. 时间窗口特征 (12/24/48个月)\n",
    "       - 最近行为比历史行为更有预测力\n",
    "       - 分别统计不同时间段的行为模式\n",
    "    \"\"\"\n",
    "    cc = pd.read_csv(r'../input/home-credit-default-risk/credit_card_balance.csv')\n",
    "\n",
    "    # 类别特征编码\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    # ==================== 核心衍生特征 ====================\n",
    "    # 1. 信用额度使用率（关键指标）\n",
    "    # 当前余额/信用额度，反映信用卡使用程度\n",
    "    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    \n",
    "    # 2. 还款充足性\n",
    "    # 实际还款金额/最低还款额，>1表示还得比最低多\n",
    "    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n",
    "    \n",
    "    # 3. 是否逾期标记\n",
    "    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # 4. 取现占额度比例\n",
    "    # ATM取现/信用额度，取现多说明缺现金\n",
    "    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "    # 5. 逾期程度分类\n",
    "    # 轻度逾期（1-119天）\n",
    "    cc['CARD_IS_DPD_UNDER_120'] = cc['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    # 严重逾期（≥120天）\n",
    "    cc['CARD_IS_DPD_OVER_120'] = cc['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # ==================== 按客户聚合所有信用卡记录 ====================\n",
    "    # 对所有数值特征进行多种统计聚合\n",
    "    # min/max: 极值（最好/最差情况）\n",
    "    # mean: 平均水平\n",
    "    # sum: 累计值\n",
    "    # var: 波动性（方差大说明使用不稳定）\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    # 统计每个客户的信用卡总数\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # ==================== 最近一个月的信用卡状态 ====================\n",
    "    # 原理：最近的行为比历史平均更重要\n",
    "    # 找到每张信用卡最近一个月的记录\n",
    "    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    last_months_df = cc[cc.index.isin(last_ids)]\n",
    "    \n",
    "    # 计算最近一个月的余额统计\n",
    "    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n",
    "\n",
    "    # ==================== 时间窗口特征（重要技术）====================\n",
    "    # 核心思想：客户行为会随时间变化\n",
    "    # - 最近12个月：反映当前状态\n",
    "    # - 最近24个月：反映中期趋势\n",
    "    # - 最近48个月：反映长期行为\n",
    "    # \n",
    "    # 举例说明：\n",
    "    # 如果客户：\n",
    "    # - 48个月平均额度使用率: 30%（历史良好）\n",
    "    # - 12个月平均额度使用率: 80%（最近恶化）\n",
    "    # -> 说明财务状况在恶化，违约风险上升！\n",
    "    \n",
    "    # 定义需要按时间窗口统计的特征\n",
    "    CREDIT_CARD_TIME_AGG = {\n",
    "        'AMT_BALANCE': ['mean', 'max'],                    # 余额统计\n",
    "        'LIMIT_USE': ['max', 'mean'],                      # 额度使用率\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':['max'],                 # 信用额度\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],        # ATM取现\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],            # 总取现\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],        # POS取现\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],        # 最低还款额\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],        # 总还款额\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],           # 应收总额\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum', 'mean'], # ATM取现次数\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],    # 总取现次数\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['mean'],              # POS取现次数\n",
    "        'SK_DPD': ['mean', 'max', 'sum'],                  # 逾期天数\n",
    "        'LIMIT_USE': ['min', 'max'],                       # 额度使用率（重复用于强调）\n",
    "        'DRAWING_LIMIT_RATIO': ['min', 'max'],             # 取现比例\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],                   # 逾期标记\n",
    "        'CARD_IS_DPD_UNDER_120': ['mean', 'sum'],          # 轻度逾期\n",
    "        'CARD_IS_DPD_OVER_120': ['mean', 'sum']            # 严重逾期\n",
    "    }\n",
    "\n",
    "    # 循环创建12个月、24个月、48个月的时间窗口特征\n",
    "    for months in [12, 24, 48]:\n",
    "        # 筛选最近N个月有记录的信用卡\n",
    "        # MONTHS_BALANCE是负数，-12表示最近12个月\n",
    "        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n",
    "        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n",
    "        \n",
    "        # 创建特征前缀，例如：INS_12M_（Installment 12 Months）\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        \n",
    "        # 聚合并合并到主表\n",
    "        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n",
    "\n",
    "\n",
    "    print('\"Credit Card Balance\" final shape:', cc_agg.shape)\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-conservative",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.471173Z",
     "iopub.status.busy": "2021-05-04T15:38:40.470451Z",
     "iopub.status.idle": "2021-05-04T15:38:40.473500Z",
     "shell.execute_reply": "2021-05-04T15:38:40.472821Z"
    },
    "papermill": {
     "duration": 0.02737,
     "end_time": "2021-05-04T15:38:40.473642",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.446272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【数据后处理函数】\n",
    "对合并后的完整数据集进行最后的处理和优化\n",
    "\"\"\"\n",
    "def data_post_processing(dataframe):\n",
    "    \"\"\"\n",
    "    数据后处理流程：\n",
    "    1. 特征名称标准化\n",
    "    2. 内存优化\n",
    "    3. 删除无信息特征\n",
    "    4. LightGBM特征选择\n",
    "    5. 风险分组编码\n",
    "    \n",
    "    这是特征工程的最后一步，确保数据集高质量且高效\n",
    "    \"\"\"\n",
    "    print(f'---=> the DATA POST-PROCESSING is beginning, the dataset has {dataframe.shape[1]} features')\n",
    "    \n",
    "    # 保存索引相关列名（这些列不参与模型训练）\n",
    "    index_cols = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']\n",
    "\n",
    "    # ==================== 步骤1: 特征名称标准化 ====================\n",
    "    # 将所有特殊字符替换为下划线，确保特征名符合规范\n",
    "    # 例如：AMT_CREDIT-SUM -> AMT_CREDIT_SUM\n",
    "    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n",
    "    print('names of feature are renamed')\n",
    "\n",
    "    # ==================== 步骤2: 内存优化 ====================\n",
    "    # 通过降低数据类型精度来减少内存占用\n",
    "    # 这对于Kaggle的16GB内存限制至关重要\n",
    "    dataframe = reduce_mem_usage(dataframe)\n",
    "    print(f'---=> pandas data types of features in the dataset are converted for a reduced memory usage')\n",
    "\n",
    "    # ==================== 步骤3: 删除无信息特征 ====================\n",
    "    # 如果一个特征只有一个取值（或全是缺失值），它对模型没有任何帮助\n",
    "    # 例如：某列全是1，或全是NaN\n",
    "    noninformative_cols = []\n",
    "    for col in dataframe.columns:\n",
    "        # 统计该列的不同取值数量\n",
    "        if len(dataframe[col].value_counts()) < 2:\n",
    "            noninformative_cols.append(col)\n",
    "\n",
    "    dataframe.drop(noninformative_cols, axis=1, inplace=True)\n",
    "    print(f'---=> {dataframe.shape[1]} features are remained after removing non-informative features')\n",
    "\n",
    "    # ==================== 步骤4: LightGBM特征选择 ====================\n",
    "    # 使用预训练的LightGBM模型筛选出的重要特征\n",
    "    feature_num = dataframe.shape[1]\n",
    "    \n",
    "    # 注意：原本应该调用ligthgbm_feature_selection函数\n",
    "    # 但由于内存限制，这里读取预先计算好的结果\n",
    "    auc_limit = 0.7\n",
    "    # dataframe = ligthgbm_feature_selection(dataframe, index_cols, auc_limit=auc_limit)\n",
    "    \n",
    "    # 读取需要删除的特征列表\n",
    "    all_features = dataframe.columns.tolist()\n",
    "    selected_feature_df = pd.read_csv('../input/homecredit-best-subs/removed_cols_lgbm.csv')\n",
    "    selected_features = selected_feature_df.removed_cols.tolist()\n",
    "    \n",
    "    # 保留有用的特征\n",
    "    remained_features = set(all_features).difference(set(selected_features))\n",
    "    dataframe = dataframe[remained_features]\n",
    "    print(f'{feature_num - dataframe.shape[1]} features are eliminated by LightGBM classifier with an {auc_limit} auc score limit in step I')\n",
    "    print(f'---=> {dataframe.shape[1]} features are remained after removing features not interesting for LightGBM classifier')\n",
    "\n",
    "\n",
    "    # ==================== 步骤5: 风险分组编码 ====================\n",
    "    # 对剩余的类别特征应用风险分组技术\n",
    "    # 这是最后一次特征工程，将类别转换为风险标记\n",
    "    start_feats_num = dataframe.shape[1]\n",
    "    \n",
    "    # 选择合适的类别特征：\n",
    "    # - 类别数在3-20之间（太少无意义，太多会爆炸）\n",
    "    # - 不是索引列\n",
    "    cat_cols = [col for col in dataframe.columns if 3 < len(dataframe[col].value_counts()) < 20 and col not in index_cols]\n",
    "    \n",
    "    # 应用风险分组，阈值设为8.1%（接近平均违约率8.2%）\n",
    "    dataframe, _ = risk_groupanizer(dataframe, column_names=cat_cols, upper_limit_ratio=8.1, lower_limit_ratio=8.1)\n",
    "    print(f'---=> {dataframe.shape[1] - start_feats_num} features are generated with the risk_groupanizer')\n",
    "\n",
    "\n",
    "    # ==================== 处理完成 ====================\n",
    "    print(f'---=> the DATA POST-PROCESSING is ended!, now the dataset has a total {dataframe.shape[1]} features')\n",
    "\n",
    "    # 手动触发垃圾回收，释放内存\n",
    "    gc.collect()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-graduation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.518255Z",
     "iopub.status.busy": "2021-05-04T15:38:40.517479Z",
     "iopub.status.idle": "2021-05-04T15:38:40.520435Z",
     "shell.execute_reply": "2021-05-04T15:38:40.519963Z"
    },
    "papermill": {
     "duration": 0.03368,
     "end_time": "2021-05-04T15:38:40.520598",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.486918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【K折交叉验证 + LightGBM训练函数】\n",
    "本项目的核心：使用伪标签技术和K折交叉验证训练最终模型\n",
    "\"\"\"\n",
    "def Kfold_LightGBM(df):\n",
    "    \"\"\"\n",
    "    核心创新：伪标签（Pseudo-Labeling）技术\n",
    "    \n",
    "    什么是伪标签？\n",
    "    1. 先用训练集训练一个初步模型\n",
    "    2. 用这个模型对测试集进行预测\n",
    "    3. 将测试集的预测结果作为\"伪标签\"\n",
    "    4. 把带伪标签的测试集加入训练集\n",
    "    5. 用扩大后的训练集重新训练最终模型\n",
    "    \n",
    "    为什么有效？\n",
    "    - 增加了训练样本数量（从30万增加到50万+）\n",
    "    - 测试集的分布信息被利用（半监督学习）\n",
    "    - 高置信度的预测（>0.75）接近真实标签\n",
    "    \n",
    "    风险与缓解：\n",
    "    - 风险：错误的伪标签会误导模型\n",
    "    - 缓解：只使用高置信度样本（>0.75）\n",
    "    - 缓解：重复添加3次以增强信号\n",
    "    \"\"\"\n",
    "    print('===============================================', '\\n', '##### the ML in processing...')\n",
    "\n",
    "    # ==================== 加载预训练模型的预测结果 ====================\n",
    "    # 这些是用其他模型对测试集的预测结果\n",
    "    # 我们将使用这些预测作为伪标签\n",
    "    df_subx = pd.read_csv(r'../input/homecredit-best-subs/df_subs_3.csv')\n",
    "    df_sub = df_subx[['SK_ID_CURR', '23']]\n",
    "    df_sub.columns = ['SK_ID_CURR', 'TARGET']\n",
    "\n",
    "    # ==================== 分离训练集和测试集 ====================\n",
    "    # 训练集：有真实TARGET标签\n",
    "    # 测试集：TARGET为NaN，需要预测\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    \n",
    "    # 删除原始DataFrame释放内存\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # ==================== 伪标签技术实现 ====================\n",
    "    # 步骤1：将预测结果转换为伪标签\n",
    "    # 规则：预测概率>0.75的标记为违约(1)，否则为正常(0)\n",
    "    # 0.75是一个高置信度阈值，只有很确定的才标记为违约\n",
    "    test_df.TARGET = np.where(df_sub.TARGET > 0.75, 1, 0)\n",
    "    \n",
    "    # 步骤2：将带伪标签的测试集加入训练集\n",
    "    # 重要：这里重复添加了3次！\n",
    "    # 为什么重复3次？\n",
    "    # - 原始训练集约30万，测试集约5万\n",
    "    # - 加3次后比例变为 30:15，增强伪标签的影响\n",
    "    # - 但不能加太多次，否则错误伪标签影响过大\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)  # 第1次\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)  # 第2次\n",
    "    train_df = pd.concat([train_df, test_df], axis=0)  # 第3次\n",
    "    print(f'Train shape: {train_df.shape}, test shape: {test_df.shape} are loaded.')\n",
    "    \n",
    "    print(f'✓ 伪标签技术：训练集从{train_df.shape[0] - 3*test_df.shape[0]}扩展到{train_df.shape[0]}样本')\n",
    "\n",
    "    # ==================== K折交叉验证设置 ====================\n",
    "    # K折交叉验证（K-Fold Cross-Validation）\n",
    "    # \n",
    "    # 什么是K折交叉验证？\n",
    "    # 将训练集分为K份（这里K=5），每次：\n",
    "    # - 用4份训练模型\n",
    "    # - 用1份验证模型\n",
    "    # - 轮流5次，确保每份数据都被验证过\n",
    "    #\n",
    "    # 为什么使用？\n",
    "    # 1. 更可靠的性能评估（减少运气成分）\n",
    "    # 2. 充分利用所有数据\n",
    "    # 3. 防止过拟合\n",
    "    # 4. 5个模型的预测可以集成（ensemble）\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "    # ==================== 初始化预测结果存储 ====================\n",
    "    # OOF (Out-Of-Fold) predictions: 训练集的预测结果\n",
    "    # 每个样本在作为验证集时的预测结果\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    # 测试集的预测结果（5个模型的平均）\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    # ==================== 特征选择 ====================\n",
    "    # 排除不参与训练的列（标签和索引）\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']]\n",
    "    \n",
    "    print(f'only {len(feats)} features from a total {train_df.shape[1]} features are used for ML analysis')\n",
    "    print(f'✓ 准备进行5折交叉验证...')\n",
    "\n",
    "    # ==================== K折训练循环 ====================\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        # 根据索引分割训练集和验证集\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "        \n",
    "        # ==================== LightGBM模型配置 ====================\n",
    "        # 这些超参数经过精心调优（来自其他高分kernel）\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=-1,              # 使用所有CPU核心\n",
    "            #device_type='gpu',      # 可选：使用GPU加速\n",
    "            \n",
    "            # --- 树结构参数 ---\n",
    "            n_estimators=5000,       # 最多5000棵树（早停会提前结束）\n",
    "            max_depth=11,            # 树的最大深度（防止过拟合）\n",
    "            num_leaves=58,           # 叶子节点数（LightGBM核心参数）\n",
    "                                     # num_leaves应该 < 2^max_depth\n",
    "            \n",
    "            # --- 学习率参数 ---\n",
    "            learning_rate=0.01,      # 学习率（较小=更稳定但更慢）\n",
    "            \n",
    "            # --- 采样参数（防止过拟合）---\n",
    "            colsample_bytree=0.613,  # 每棵树使用61.3%的特征\n",
    "            subsample=0.708,         # 每棵树使用70.8%的样本\n",
    "            \n",
    "            # --- 正则化参数（防止过拟合）---\n",
    "            reg_alpha=3.564,         # L1正则化（Lasso）\n",
    "            reg_lambda=4.930,        # L2正则化（Ridge）\n",
    "            \n",
    "            # --- 叶子节点参数 ---\n",
    "            max_bin=407,             # 特征分桶数（越大越精细但越慢）\n",
    "            min_child_weight=6,      # 叶子节点最小权重\n",
    "            min_child_samples=165,   # 叶子节点最小样本数\n",
    "            \n",
    "            # --- 其他参数 ---\n",
    "            #keep_training_booster=True,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "\n",
    "        # ==================== 模型训练 ====================\n",
    "        # early_stopping_rounds=500: 如果500轮验证集AUC不提升，则停止\n",
    "        # eval_metric='auc': 使用AUC作为评价指标\n",
    "        # verbose=500: 每500轮打印一次进度\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric='auc', \n",
    "                verbose=500, \n",
    "                early_stopping_rounds=500)\n",
    "\n",
    "        # ==================== 预测 ====================\n",
    "        # 1. 对验证集预测（用于计算OOF AUC）\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        # 2. 对测试集预测（累加后平均）\n",
    "        # [:, 1]表示取违约概率（第1列是不违约概率）\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        # 打印本折的AUC分数\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        \n",
    "        # 释放内存\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # ==================== 交叉验证总体性能 ====================\n",
    "    # OOF AUC: 所有样本作为验证集时的预测汇总\n",
    "    # 这是模型真实性能的最佳估计\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "\n",
    "    # ==================== 生成提交文件 ====================\n",
    "    # 将5个模型的平均预测结果保存为提交文件\n",
    "    test_df['TARGET'] = sub_preds\n",
    "    test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index=False)\n",
    "    print('a submission file is created')\n",
    "    print(f'✓ 预测完成！提交文件已保存为 submission.csv')\n",
    "    print(f'✓ 预测的违约概率范围：[{sub_preds.min():.4f}, {sub_preds.max():.4f}]')\n",
    "    print(f'✓ 预测的平均违约率：{sub_preds.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-lawsuit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-04T15:38:40.555978Z",
     "iopub.status.busy": "2021-05-04T15:38:40.555282Z",
     "iopub.status.idle": "2021-05-04T20:54:03.343722Z",
     "shell.execute_reply": "2021-05-04T20:54:03.343008Z"
    },
    "papermill": {
     "duration": 18922.809903,
     "end_time": "2021-05-04T20:54:03.343997",
     "exception": false,
     "start_time": "2021-05-04T15:38:40.534094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Application_Train_Test\" final shape: (356250, 309)\n",
      "\"Bureau/Bureau Balance\" final shape: (305811, 200)\n",
      "--=> df after merge with bureau: (356250, 509)\n",
      "\"Previous Applications\" final shape: (338857, 321)\n",
      "--=> df after merge with previous application: (356250, 830)\n",
      "\"Pos-Cash\" balance final shape: (337252, 46)\n",
      "--=> df after merge with pos cash : (356250, 875)\n",
      "\"Installments Payments\" final shape: (339587, 85)\n",
      "--=> df after merge with installments: (356250, 960)\n",
      "\"Credit Card Balance\" final shape: (103558, 284)\n",
      "--=> df after merge with credit card: (356250, 1243)\n",
      "---=> the DATA POST-PROCESSING is beginning, the dataset has 1243 features\n",
      "names of feature are renamed\n",
      "---=> pandas data types of features in the dataset are converted for a reduced memory usage\n",
      "---=> 1199 features are remained after removing non-informative features\n",
      "164 features are eliminated by LightGBM classifier with an 0.7 auc score limit in step I\n",
      "---=> 1035 features are remained after removing features not interesting for LightGBM classifier\n",
      "---=> 44 features are generated with the risk_groupanizer\n",
      "---=> the DATA POST-PROCESSING is ended!, now the dataset has a total 1079 features\n",
      "================================================== \n",
      "\n",
      "---=> df final shape: (356250, 1079)  <=--- \n",
      "\n",
      "==================================================\n",
      "=============================================== \n",
      " ##### the ML in processing...\n",
      "Train shape: (453738, 1079), test shape: (48744, 1079) are loaded.\n",
      "only 1077 features from a total 1079 features are used for ML analysis\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905142\ttraining's binary_logloss: 0.232344\tvalid_1's auc: 0.897325\tvalid_1's binary_logloss: 0.240314\n",
      "[1000]\ttraining's auc: 0.92274\ttraining's binary_logloss: 0.209022\tvalid_1's auc: 0.909934\tvalid_1's binary_logloss: 0.223098\n",
      "[1500]\ttraining's auc: 0.933288\ttraining's binary_logloss: 0.195351\tvalid_1's auc: 0.915287\tvalid_1's binary_logloss: 0.215273\n",
      "[2000]\ttraining's auc: 0.94142\ttraining's binary_logloss: 0.184865\tvalid_1's auc: 0.918268\tvalid_1's binary_logloss: 0.210412\n",
      "[2500]\ttraining's auc: 0.948161\ttraining's binary_logloss: 0.176246\tvalid_1's auc: 0.920058\tvalid_1's binary_logloss: 0.207175\n",
      "[3000]\ttraining's auc: 0.95405\ttraining's binary_logloss: 0.1688\tvalid_1's auc: 0.921394\tvalid_1's binary_logloss: 0.204692\n",
      "[3500]\ttraining's auc: 0.959189\ttraining's binary_logloss: 0.16212\tvalid_1's auc: 0.922375\tvalid_1's binary_logloss: 0.202641\n",
      "[4000]\ttraining's auc: 0.963802\ttraining's binary_logloss: 0.155943\tvalid_1's auc: 0.923128\tvalid_1's binary_logloss: 0.20097\n",
      "[4500]\ttraining's auc: 0.967975\ttraining's binary_logloss: 0.150169\tvalid_1's auc: 0.923844\tvalid_1's binary_logloss: 0.199428\n",
      "[5000]\ttraining's auc: 0.97173\ttraining's binary_logloss: 0.14468\tvalid_1's auc: 0.924443\tvalid_1's binary_logloss: 0.198038\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.97173\ttraining's binary_logloss: 0.14468\tvalid_1's auc: 0.924443\tvalid_1's binary_logloss: 0.198038\n",
      "Fold  1 AUC : 0.924443\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905837\ttraining's binary_logloss: 0.232124\tvalid_1's auc: 0.893077\tvalid_1's binary_logloss: 0.242347\n",
      "[1000]\ttraining's auc: 0.923283\ttraining's binary_logloss: 0.208853\tvalid_1's auc: 0.905964\tvalid_1's binary_logloss: 0.225129\n",
      "[1500]\ttraining's auc: 0.933657\ttraining's binary_logloss: 0.195249\tvalid_1's auc: 0.911449\tvalid_1's binary_logloss: 0.217228\n",
      "[2000]\ttraining's auc: 0.941663\ttraining's binary_logloss: 0.184845\tvalid_1's auc: 0.914518\tvalid_1's binary_logloss: 0.212335\n",
      "[2500]\ttraining's auc: 0.948587\ttraining's binary_logloss: 0.176195\tvalid_1's auc: 0.916421\tvalid_1's binary_logloss: 0.209059\n",
      "[3000]\ttraining's auc: 0.954301\ttraining's binary_logloss: 0.168843\tvalid_1's auc: 0.917626\tvalid_1's binary_logloss: 0.206689\n",
      "[3500]\ttraining's auc: 0.959491\ttraining's binary_logloss: 0.162116\tvalid_1's auc: 0.918646\tvalid_1's binary_logloss: 0.204682\n",
      "[4000]\ttraining's auc: 0.964053\ttraining's binary_logloss: 0.155929\tvalid_1's auc: 0.919422\tvalid_1's binary_logloss: 0.202973\n",
      "[4500]\ttraining's auc: 0.968328\ttraining's binary_logloss: 0.150049\tvalid_1's auc: 0.920112\tvalid_1's binary_logloss: 0.201462\n",
      "[5000]\ttraining's auc: 0.971997\ttraining's binary_logloss: 0.144647\tvalid_1's auc: 0.92063\tvalid_1's binary_logloss: 0.20017\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971997\ttraining's binary_logloss: 0.144647\tvalid_1's auc: 0.92063\tvalid_1's binary_logloss: 0.20017\n",
      "Fold  2 AUC : 0.920630\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905265\ttraining's binary_logloss: 0.232508\tvalid_1's auc: 0.895704\tvalid_1's binary_logloss: 0.240696\n",
      "[1000]\ttraining's auc: 0.922893\ttraining's binary_logloss: 0.209156\tvalid_1's auc: 0.908594\tvalid_1's binary_logloss: 0.223264\n",
      "[1500]\ttraining's auc: 0.933243\ttraining's binary_logloss: 0.195477\tvalid_1's auc: 0.913964\tvalid_1's binary_logloss: 0.21529\n",
      "[2000]\ttraining's auc: 0.941352\ttraining's binary_logloss: 0.185012\tvalid_1's auc: 0.917136\tvalid_1's binary_logloss: 0.210231\n",
      "[2500]\ttraining's auc: 0.948175\ttraining's binary_logloss: 0.176409\tvalid_1's auc: 0.919099\tvalid_1's binary_logloss: 0.206873\n",
      "[3000]\ttraining's auc: 0.954081\ttraining's binary_logloss: 0.168909\tvalid_1's auc: 0.920431\tvalid_1's binary_logloss: 0.204375\n",
      "[3500]\ttraining's auc: 0.959207\ttraining's binary_logloss: 0.162224\tvalid_1's auc: 0.92146\tvalid_1's binary_logloss: 0.202281\n",
      "[4000]\ttraining's auc: 0.963864\ttraining's binary_logloss: 0.156028\tvalid_1's auc: 0.922299\tvalid_1's binary_logloss: 0.20052\n",
      "[4500]\ttraining's auc: 0.968119\ttraining's binary_logloss: 0.150156\tvalid_1's auc: 0.923087\tvalid_1's binary_logloss: 0.198884\n",
      "[5000]\ttraining's auc: 0.971935\ttraining's binary_logloss: 0.144681\tvalid_1's auc: 0.923727\tvalid_1's binary_logloss: 0.197462\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971935\ttraining's binary_logloss: 0.144681\tvalid_1's auc: 0.923727\tvalid_1's binary_logloss: 0.197462\n",
      "Fold  3 AUC : 0.923727\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905623\ttraining's binary_logloss: 0.231929\tvalid_1's auc: 0.89495\tvalid_1's binary_logloss: 0.242589\n",
      "[1000]\ttraining's auc: 0.923033\ttraining's binary_logloss: 0.208651\tvalid_1's auc: 0.907343\tvalid_1's binary_logloss: 0.225561\n",
      "[1500]\ttraining's auc: 0.933459\ttraining's binary_logloss: 0.194959\tvalid_1's auc: 0.912764\tvalid_1's binary_logloss: 0.217628\n",
      "[2000]\ttraining's auc: 0.941646\ttraining's binary_logloss: 0.184566\tvalid_1's auc: 0.915911\tvalid_1's binary_logloss: 0.21271\n",
      "[2500]\ttraining's auc: 0.948288\ttraining's binary_logloss: 0.176119\tvalid_1's auc: 0.917755\tvalid_1's binary_logloss: 0.209501\n",
      "[3000]\ttraining's auc: 0.954057\ttraining's binary_logloss: 0.16871\tvalid_1's auc: 0.919095\tvalid_1's binary_logloss: 0.207\n",
      "[3500]\ttraining's auc: 0.959343\ttraining's binary_logloss: 0.161893\tvalid_1's auc: 0.920165\tvalid_1's binary_logloss: 0.204888\n",
      "[4000]\ttraining's auc: 0.963977\ttraining's binary_logloss: 0.155739\tvalid_1's auc: 0.920979\tvalid_1's binary_logloss: 0.203163\n",
      "[4500]\ttraining's auc: 0.968121\ttraining's binary_logloss: 0.149998\tvalid_1's auc: 0.921693\tvalid_1's binary_logloss: 0.201623\n",
      "[5000]\ttraining's auc: 0.971896\ttraining's binary_logloss: 0.144552\tvalid_1's auc: 0.922338\tvalid_1's binary_logloss: 0.20022\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971896\ttraining's binary_logloss: 0.144552\tvalid_1's auc: 0.922338\tvalid_1's binary_logloss: 0.20022\n",
      "Fold  4 AUC : 0.922338\n",
      "[LightGBM] [Warning] num_threads is set with nthread=-1, will be overridden by n_jobs=-1. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.905187\ttraining's binary_logloss: 0.233112\tvalid_1's auc: 0.898082\tvalid_1's binary_logloss: 0.237528\n",
      "[1000]\ttraining's auc: 0.922842\ttraining's binary_logloss: 0.209511\tvalid_1's auc: 0.909799\tvalid_1's binary_logloss: 0.220898\n",
      "[1500]\ttraining's auc: 0.933275\ttraining's binary_logloss: 0.195734\tvalid_1's auc: 0.914849\tvalid_1's binary_logloss: 0.213236\n",
      "[2000]\ttraining's auc: 0.941469\ttraining's binary_logloss: 0.185277\tvalid_1's auc: 0.917758\tvalid_1's binary_logloss: 0.208512\n",
      "[2500]\ttraining's auc: 0.948207\ttraining's binary_logloss: 0.17669\tvalid_1's auc: 0.919569\tvalid_1's binary_logloss: 0.205261\n",
      "[3000]\ttraining's auc: 0.953965\ttraining's binary_logloss: 0.169298\tvalid_1's auc: 0.920767\tvalid_1's binary_logloss: 0.202877\n",
      "[3500]\ttraining's auc: 0.959161\ttraining's binary_logloss: 0.16256\tvalid_1's auc: 0.921822\tvalid_1's binary_logloss: 0.200809\n",
      "[4000]\ttraining's auc: 0.963824\ttraining's binary_logloss: 0.156315\tvalid_1's auc: 0.922631\tvalid_1's binary_logloss: 0.199063\n",
      "[4500]\ttraining's auc: 0.968079\ttraining's binary_logloss: 0.15044\tvalid_1's auc: 0.923325\tvalid_1's binary_logloss: 0.197527\n",
      "[5000]\ttraining's auc: 0.971873\ttraining's binary_logloss: 0.144925\tvalid_1's auc: 0.923917\tvalid_1's binary_logloss: 0.196092\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's auc: 0.971873\ttraining's binary_logloss: 0.144925\tvalid_1's auc: 0.923917\tvalid_1's binary_logloss: 0.196092\n",
      "Fold  5 AUC : 0.923917\n",
      "Full AUC score 0.922996\n",
      "a submission file is created\n",
      "--=> all calculations are done!! <=--\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==================== 主执行流程 ====================\n",
    "完整的特征工程 -> 数据后处理 -> 模型训练流水线\n",
    "\n",
    "数据合并顺序说明：\n",
    "1. application: 主表（客户基本信息） - 309列\n",
    "2. bureau: 信用局历史 - 增加200列\n",
    "3. previous_application: 历史申请 - 增加321列\n",
    "4. pos_cash: 分期付款余额 - 增加46列\n",
    "5. installment: 分期还款记录 - 增加85列\n",
    "6. credit_card: 信用卡余额 - 增加284列\n",
    "\n",
    "最终特征数：1243列 -> 后处理后约1079列\n",
    "\"\"\"\n",
    "\n",
    "# ==================== 步骤1: 处理主申请表 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('步骤 1/7: 处理主申请表 (Application)')\n",
    "print('='*60)\n",
    "df = application()\n",
    "\n",
    "# ==================== 步骤2: 合并信用局数据 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('步骤 2/7: 合并信用局数据 (Bureau)')\n",
    "print('='*60)\n",
    "df = df.merge(bureau_bb(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with bureau:', df.shape)\n",
    "\n",
    "# ==================== 步骤3: 合并历史申请数据 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('步骤 3/7: 合并历史申请数据 (Previous Application)')\n",
    "print('='*60)\n",
    "df = df.merge(previous_application(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with previous application:', df.shape)\n",
    "\n",
    "# ==================== 步骤4: 合并POS分期数据 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('步骤 4/7: 合并POS分期数据 (POS Cash)')\n",
    "print('='*60)\n",
    "df = df.merge(pos_cash(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with pos cash :', df.shape)\n",
    "\n",
    "# ==================== 步骤5: 合并分期还款数据 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('步骤 5/7: 合并分期还款数据 (Installments)')\n",
    "print('='*60)\n",
    "df = df.merge(installment(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with installments:', df.shape)\n",
    "\n",
    "# ==================== 步骤6: 合并信用卡数据 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('步骤 6/7: 合并信用卡数据 (Credit Card)')\n",
    "print('='*60)\n",
    "df = df.merge(credit_card(), how='left', on='SK_ID_CURR')\n",
    "print('--=> df after merge with credit card:', df.shape)\n",
    "\n",
    "# ==================== 步骤7: 数据后处理 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('步骤 7/7: 数据后处理 (特征选择、内存优化、风险编码)')\n",
    "print('='*60)\n",
    "df = data_post_processing(df)\n",
    "print('='*50, '\\n')\n",
    "print('---=> df final shape:', df.shape, ' <=---', '\\n')\n",
    "print('=' * 50)\n",
    "\n",
    "# ==================== 步骤8: 模型训练和预测 ====================\n",
    "print('\\n' + '='*60)\n",
    "print('模型训练: 5折交叉验证 + 伪标签技术')\n",
    "print('='*60)\n",
    "Kfold_LightGBM(df)\n",
    "print('\\n' + '='*60)\n",
    "print('--=> all calculations are done!! <=--')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18935.297056,
   "end_time": "2021-05-04T20:54:04.803426",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-04T15:38:29.506370",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
